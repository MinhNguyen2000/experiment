{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcc152e0",
   "metadata": {},
   "source": [
    "## A2C (Advantage Actor Critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0670b9e3",
   "metadata": {},
   "source": [
    "Code to train and test an A2C agent (using the stable-baselines3 library) in the gymnasium CartPole-v1 position. The example is given on stable-baselines3 [documentation](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aa3a92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b089493c",
   "metadata": {},
   "source": [
    "In this example, four vectorized environments are used to train the A2C agent. The parallel environments are created by the `make_vec_env` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d21e2c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Create parallel environments\n",
    "vec_env = make_vec_env(\"CartPole-v1\", n_envs=4, vec_env_cls=SubprocVecEnv)\n",
    "model = A2C(\"MlpPolicy\", vec_env, verbose = 1, device = \"cpu\")\n",
    "model.learn(total_timesteps=1000)\n",
    "\n",
    "model.save(\"a2c_cartpole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f87367b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caomi\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[ True False False  True]\n",
      "[False False False False]\n",
      "[False False  True False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False  True False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False  True]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False  True False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False  True False False]\n",
      "[ True False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False  True False]\n",
      "[False False False False]\n",
      "[False  True False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False  True]\n",
      "[False False False False]\n",
      "[ True False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False  True  True False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False  True]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False  True False False]\n",
      "[ True False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "\n",
    "model = A2C.load(\"a2c_cartpole\")\n",
    "obs = vec_env.reset()\n",
    "dones_arr = []\n",
    "\n",
    "for i in range(100):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    dones_arr.append(dones)\n",
    "    vec_env.render(\"human\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b13826f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defe3c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
