{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54c818cf",
   "metadata": {},
   "source": [
    "This is a continuation of the experiment done on the gymnasium cart pole example in the classic-control environment package. Different from the cart pole environment, the inverted pendulum environment in the MuJoCo package extends to continuous action space. Therefore, the output of policy approximation is no longer the probability of each action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9a7765",
   "metadata": {},
   "source": [
    "# Explore the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f321130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal      # normal dist\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11eb1c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This environment has 4 continuous state observations and 1 continuous action\n",
      "Time  1  |  s_t [0.00157316 0.0094184  0.00464467 0.00441516]  |  a_t [-0.8494796]  |  reward 1 |  s_t+1 [-0.00391629  0.0228987  -0.27834346  0.66297798]\n",
      "Time  2  |  s_t [-0.00391629  0.0228987  -0.27834346  0.66297798]  |  a_t [2.4721649]  |  reward 1 |  s_t+1 [ 0.00143631  0.01114524  0.54330357 -1.22661579]\n",
      "Time  3  |  s_t [ 0.00143631  0.01114524  0.54330357 -1.22661579]  |  a_t [2.4582906]  |  reward 1 |  s_t+1 [ 0.03929636 -0.07372268  1.34786744 -3.00629528]\n",
      "Time  4  |  s_t [ 0.03929636 -0.07372268  1.34786744 -3.00629528]  |  a_t [-0.9403938]  |  reward 1 |  s_t+1 [ 0.08660461 -0.17715283  1.02100417 -2.19908831]\n",
      "Time  5  |  s_t [ 0.08660461 -0.17715283  1.02100417 -2.19908831]  |  a_t [2.6813865]  |  reward 0 |  s_t+1 [ 0.14492104 -0.30555082  1.88810287 -4.19371734]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"InvertedPendulum-v5\",render_mode='human')\n",
    "obs_space = env.observation_space.shape[0]\n",
    "act_space = env.action_space.shape[0]\n",
    "print(f\"This environment has {obs_space} continuous state observations \"\n",
    "      f\"and {act_space} continuous action\")\n",
    "\n",
    "# Simulate several episodes\n",
    "for i in range(1):\n",
    "    obs, _ = env.reset()\n",
    "    t = 1\n",
    "    done = False\n",
    "    while not done:\n",
    "        random_action = env.action_space.sample()\n",
    "        \n",
    "        next_obs, reward, term, trunc, _ = env.step(random_action)\n",
    "        print(f\"Time {t:2d}  |  s_t {obs}  |  a_t {random_action}  |  reward {reward} |  s_t+1 {next_obs}\")\n",
    "        obs = next_obs; t += 1\n",
    "        done = term or trunc\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845216b2",
   "metadata": {},
   "source": [
    "# Simplified PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8150692f",
   "metadata": {},
   "source": [
    "## Parameter and Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b22b5daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter definitions\n",
    "MODEL_NAME = 'PPO_MLP_v0'\n",
    "ALPHA = 1e-5\n",
    "BETA = 1e-3\n",
    "GAMMA = 0.99                    # discount factor for return calculation\n",
    "EPS = 0.1\n",
    "\n",
    "BUFFER_SIZE = 2048\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "N_ITERATION = 100            # number of episodes to train the Q-network\n",
    "N_EPOCH = 10\n",
    "N_EPISODE_TEST = 5\n",
    "CUDA_ENABLED = False\n",
    "SUCCESS_CRITERIA = 450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bd0016",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_layers=[64,64], cuda_enabled=False):\n",
    "        ''' Initialize the model and create a list of layers '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim        # (2 * action_space,) to estimate mean and stdev of each action\n",
    "        # hidden layers\n",
    "        for layer_size in hidden_layers:\n",
    "            self.layers.append(nn.Linear(input_dim, layer_size))\n",
    "            input_dim = layer_size\n",
    "        # output layers\n",
    "        self.layers.append(nn.Linear(input_dim, self.output_dim))\n",
    "        if cuda_enabled: self.cuda()\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers[:-1]:\n",
    "            input = torch.relu(layer(input))\n",
    "        output = self.layers[-1](input)\n",
    "        return output\n",
    "    \n",
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_layers=[32,32], cuda_enabled=False):\n",
    "        ''' Initialize the model and create a list of layers '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        for layer_size in hidden_layers:\n",
    "            self.layers.append(nn.Linear(input_dim, layer_size))\n",
    "            input_dim = layer_size\n",
    "\n",
    "        self.layers.append(nn.Linear(input_dim, self.output_dim))\n",
    "        if cuda_enabled: self.cuda()\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers[:-1]:\n",
    "            input = torch.relu(layer(input))\n",
    "        output = self.layers[-1](input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255f747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_registry = {\n",
    "    'PPO_MLP_v0': {\n",
    "        'class': PolicyNet,\n",
    "        'config': [64,32],\n",
    "        'value_class': ValueNet,\n",
    "        'value_config': [32]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2704a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOBuffer():\n",
    "    def __init__(self, obs_dim, act_dim, buffer_size):\n",
    "        self.obs_buf = np.zeros((buffer_size, obs_dim), dtype = np.float32)\n",
    "        self.act_buf = np.zeros(buffer_size, dtype = np.float32)\n",
    "        self.rew_buf = np.zeros(buffer_size, dtype = np.float32)\n",
    "        self.nobs_buf = np.zeros((buffer_size, obs_dim), dtype = np.float32)    # Next observation\n",
    "        self.logp_buf = np.zeros(buffer_size, dtype = np.float32)               # pi(a_t|s_t) or the prob of the current action given the current observation           \n",
    "        self.done_buf = np.zeros(buffer_size, dtype = np.bool8)\n",
    "\n",
    "        self.val_buf = np.zeros(buffer_size, dtype = np.float32)                # Estimated state value of current obs\n",
    "        self.adv_buf = np.zeros(buffer_size, dtype = np.float32)\n",
    "        self.tar_buf = np.zeros(buffer_size, dtype = np.float32)                # Buffer of critic target\n",
    "\n",
    "        self.ptr = 0\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, logp, done):            # Store the transitions\n",
    "        assert self.ptr < self.buffer_size\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.nobs_buf[self.ptr] = next_obs\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr += 1\n",
    "\n",
    "    def get_transition(self):\n",
    "        ''' Get the data to calculate state value and critic target before training'''\n",
    "        data = dict(obs=self.obs_buf,\n",
    "                    act=self.act_buf,\n",
    "                    rew=self.rew_buf,\n",
    "                    next_obs = self.nobs_buf,\n",
    "                    logp=self.logp_buf,\n",
    "                    done=self.done_buf)\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    def get_data(self):\n",
    "        data = dict(obs=self.obs_buf, act=self.act_buf,\n",
    "                    rew=self.rew_buf, next_obs = self.nobs_buf,\n",
    "                    logp=self.logp_buf, done=self.done_buf,\n",
    "                    val=self.val_buf,\n",
    "                    adv=self.adv_buf,\n",
    "                    tar=self.tar_buf)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c95da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO():\n",
    "    def __init__(self, model_name: str,\n",
    "                 model_registry,\n",
    "                 alpha: float, beta: float, gamma: float,\n",
    "                 eps: float,\n",
    "                 buffer_size: int, batch_size: int,\n",
    "                 n_iter_train = 5000, n_epoch = 10,\n",
    "                 result_folder = 'inv_pend_AC_results',\n",
    "                 seed = 42,\n",
    "                 cuda_enabled = False,\n",
    "                 verbose = True):\n",
    "        \n",
    "        self.result_folder = result_folder\n",
    "        self.SEED = seed\n",
    "        self.CUDA_ENABLED = cuda_enabled\n",
    "        self.VERBOSE = verbose\n",
    "        self.LOG_PERIOD = 1\n",
    "\n",
    "        self.alpha, self.beta, self.gamma = alpha, beta, gamma\n",
    "        self.eps = eps\n",
    "        self.buffer_size, self.batch_size = buffer_size, batch_size\n",
    "        self.N_ITER_TRAIN = n_iter_train\n",
    "        self.N_EPOCH = n_epoch\n",
    "\n",
    "        # Initialize the train and validation environments\n",
    "        self.env = gym.make(\"InvertedPendulum-v5\", render_mode=\"ansi\")\n",
    "        self.env_val = gym.make(\"InvertedPendulum-v5\", render_mode=\"ansi\")\n",
    "        self.obs_space = self.env.observation_space.shape[0]\n",
    "        self.act_space = self.env.action_space.shape[0]\n",
    "        \n",
    "        ''' Experiment hyperparameters '''  \n",
    "        # Policy model configuration\n",
    "        self.model_name = model_name\n",
    "        match = re.search(r'v\\d+', self.model_name)\n",
    "        self.model_id = match.group(0) if match else 404\n",
    "\n",
    "        self.policy_model_class = model_registry[self.model_name]['class']\n",
    "        self.policy_model_config = model_registry[self.model_name]['config']\n",
    "        self.value_model_class = model_registry[self.model_name]['value_class']\n",
    "        self.value_model_config = model_registry[self.model_name]['value_config']\n",
    "\n",
    "        # Instantiate and initialize the policy network\n",
    "        self.policy_net = self.policy_model_class(self.obs_space, 2*self.act_space, self.policy_model_config)\n",
    "        self.policy_net.apply(self.init_weights)\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr = self.alpha)\n",
    "\n",
    "        self.policy_net_old = self.policy_model_class(self.obs_space, 2*self.act_space, self.policy_model_config)\n",
    "        self.policy_net_old.load_state_dict(self.policy_net.state_dict())\n",
    "        self.policy_net_old.eval()                              # Set to only do inference\n",
    "        \n",
    "        # Instantiate and initialize the state value network\n",
    "        self.value_net = self.value_model_class(self.obs_space, 1, self.value_model_config)\n",
    "        self.value_net.apply(self.init_weights)\n",
    "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr = self.beta)\n",
    "\n",
    "        self.save_path = ''\n",
    "        self.model_path = ''\n",
    "        self.hyperparam_config = ''\n",
    "        self.reward_history = []\n",
    "        self.val_history = {}                                           # Monitor which episode had a validation run, the train reward, and the validation (test) reward \n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def create_directory(self):\n",
    "        ''' Function that creates directory to save model state_dict, architecture, training configuration, and history\n",
    "\n",
    "        Parameters: \n",
    "        ------------\n",
    "        (hyperparameters for differentiating between different directory)\n",
    "        \n",
    "        lr : float\n",
    "            the learning rate to optimize the Q network\n",
    "        gamma : float \n",
    "            the discount rate in Q learning\n",
    "        \n",
    "        Returns\n",
    "        ------------\n",
    "        name_codified : str\n",
    "            the shortened name for the current experiment \n",
    "        hyperparameters_codified : str\n",
    "            the shortened string of hyperparameter configuration\n",
    "        OUTPUT_DIR : path\n",
    "            the directory to which the training results and model (state_dict and architecture) will be saved\n",
    "        '''\n",
    "        timestamp = datetime.now().strftime(\"%y%m%d_%H%M\")\n",
    "\n",
    "        RESULT_FOLDER = self.result_folder\n",
    "        BASE_DIR = os.getcwd()\n",
    "        RESULT_DIR = os.path.join(BASE_DIR, RESULT_FOLDER)\n",
    "        os.makedirs(RESULT_DIR, exist_ok=True)      # Create the directory if one does not already exist\n",
    "\n",
    "        # Find the trial # of the latest run\n",
    "        existing_runs = [d for d in os.listdir(RESULT_DIR) if os.path.isdir(os.path.join(RESULT_DIR,d))]\n",
    "        run_numbers = [int(re.search(r'run_(\\d{5})',d).group(1)) for d in existing_runs if re.match(r'run_\\d{5}',d)]\n",
    "        trial_number = max(run_numbers,default=-1)+1\n",
    "\n",
    "        # Create a folder for the run\n",
    "        name_codified = f\"run_{trial_number:05d}\"\n",
    "        OUTPUT_DIR = os.path.join(RESULT_DIR,name_codified)\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)      # Create the directory\n",
    "\n",
    "        # Append the mapping from run # to hyperparameter configuration in a JSON file inside RESULT_DIR\n",
    "        trial_to_param_path = os.path.join(RESULT_DIR,'trial_to_param.json')\n",
    "        if os.path.exists(trial_to_param_path):\n",
    "            with open(trial_to_param_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "        else:\n",
    "            data = {name_codified: \"\"}\n",
    "\n",
    "        hyperparam_codified = \"AC_\"\n",
    "        hyperparam_codified += \"OOP_\"\n",
    "        hyperparam_codified += \"CUDA_\" if self.CUDA_ENABLED else \"nCUDA_\"\n",
    "        hyperparam_codified += f\"{self.model_id}_{self.alpha}_{self.beta}_{self.gamma}\"\n",
    "        hyperparam_codified_time = f\"{timestamp}_\" + hyperparam_codified\n",
    "\n",
    "        data[name_codified] = hyperparam_codified_time\n",
    "\n",
    "        with open(trial_to_param_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "        # Store the training configs in JSON file\n",
    "        training_params = {\n",
    "            'OOP':                  True,\n",
    "            'CUDA':                 self.CUDA_ENABLED,\n",
    "            'device':               torch.cuda.get_device_name(torch.cuda.current_device()),\n",
    "            'model_name':           self.model_name,\n",
    "            'alpha':                self.alpha,\n",
    "            'beta':                 self.beta,\n",
    "            'gamma':                self.gamma,\n",
    "        }\n",
    "\n",
    "        # Store training parameters in each run \n",
    "        param_path = os.path.join(OUTPUT_DIR, \"param_config.json\")\n",
    "        with open(param_path, \"w\") as f:\n",
    "            json.dump({\"parameters\": training_params}, f, indent=2)\n",
    "\n",
    "        return name_codified, hyperparam_codified, OUTPUT_DIR\n",
    "    \n",
    "    def get_first_return(self, eps_reward_history):\n",
    "        ''' Function to calculate the return of the first state '''\n",
    "        gamma = self.gamma\n",
    "        \n",
    "        return_val = [gamma ** i * eps_reward_history[i] for i in range(len(eps_reward_history))]\n",
    "        return sum(return_val)\n",
    "\n",
    "    def policy_eval(self, env: gym.Env, n_iter_test = 500, verbose = True):\n",
    "        ''' Assess the average reward when following the policy net in a test environment with random state initialization\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        env : gymnasium environment\n",
    "            this environment can be either the self.env_test or self.env_val environment (whether they are the same)\n",
    "        n_episode_test : int \n",
    "            the number of evaluation episodes\n",
    "        verbose : bool\n",
    "            whether to print testing information \n",
    "\n",
    "        Return:\n",
    "        ----------\n",
    "        average_reward : float\n",
    "            the average reward received from running the test\n",
    "        '''\n",
    "\n",
    "        reward_history = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(n_iter_test):\n",
    "                obs,_ = env.reset()\n",
    "                done = False\n",
    "                eps_reward = 0\n",
    "\n",
    "                while not done:                 # Step thorugh the episode deterministically (no exploration)\n",
    "                    obs_tensor = torch.tensor(obs, dtype = torch.float32, device = 'cuda' if self.CUDA_ENABLED else 'cpu')\n",
    "                    action_probs = self.policy_net(obs_tensor)\n",
    "                    action_dist = Categorical(action_probs)\n",
    "                    action = action_dist.sample()\n",
    "                    next_obs, reward, term, trunc, _ = env.step(action.item())\n",
    "\n",
    "                    # Strategy 1 - Accumulate the reward from the environment\n",
    "                    eps_reward += reward\n",
    "\n",
    "                    # TODO - Strategy 2 - evaluate the strategy based on states\n",
    "\n",
    "                    obs = next_obs\n",
    "                    done = term or trunc\n",
    "            \n",
    "                reward_history.append(eps_reward)\n",
    "                if verbose:\n",
    "                    print(f\"Validation episode {i+1:3d}/{n_iter_test}  |   Reward = {eps_reward:4.0f}\",end=\"\\r\")\n",
    "        reward_mean = mean(reward_history)\n",
    "        reward_stdev = stdev(reward_history)\n",
    "        \n",
    "        return reward_mean, reward_stdev\n",
    "    \n",
    "    def EMA_filter(self, reward: list, alpha):\n",
    "        ''' Function that runs an exponential moving average filter along a datastream '''\n",
    "        output = np.zeros(len(reward)+1)\n",
    "        output[0] = reward[0]\n",
    "        for idx, item in enumerate(reward):\n",
    "            output[idx+1] = (1 - alpha) * output[idx] + alpha * item\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def plot_reward_hist(self, alpha = 0.1):\n",
    "        ''' Function that plots the reward and filtered reward per episode, then saves the plot in a specified save directory'''\n",
    "        n_episodes= len(self.reward_history)\n",
    "        episodes = range(n_episodes)\n",
    "        filtered_reward_hist = self.EMA_filter(self.reward_history, alpha)\n",
    "        fig, axes = plt.subplots(1,1, figsize=(20,6))\n",
    "\n",
    "        axes.plot(episodes, self.reward_history[:n_episodes], label = 'Total reward', color = \"blue\")\n",
    "        axes.plot(episodes, filtered_reward_hist[:n_episodes], label = 'Filtered reward', color = \"red\")\n",
    "        axes.set_title(f'Total reward per episode - {self.hyperparam_config}')\n",
    "        axes.set_xlabel('Episode')\n",
    "        axes.set_ylabel('Reward')\n",
    "        axes.legend()\n",
    "        \n",
    "        # axes[0].plot(episodes, self.reward_history[:n_episodes], label = 'Total reward', color = \"blue\")\n",
    "        # axes[0].plot(episodes, filtered_reward_hist[:n_episodes], label = 'Filtered reward', color = \"red\")\n",
    "        # axes[0].set_title(f'Total reward per episode - {self.hyperparam_config}')\n",
    "        # axes[0].set_xlabel('Episode')\n",
    "        # axes[0].set_ylabel('Reward')\n",
    "        # axes[0].legend()\n",
    "\n",
    "        # n_episodes= len(self.value_history)\n",
    "        # episodes = range(n_episodes)\n",
    "        # axes[1].plot(episodes, self.value_history[:n_episodes], label = \"v(s_0)\", color = \"blue\")\n",
    "        # # If using baseline with a value estimation model, plot this as well\n",
    "        # axes[1].plot(episodes, self.value_est_history[:n_episodes], label = r\"$\\hat v$(s_0)\", color = \"red\")\n",
    "        # axes[1].set_title(f'Return in state s_0 - {self.hyperparam_config}')\n",
    "        # axes[1].set_xlabel('Episode')\n",
    "        # axes[1].set_ylabel('v(s_0)')\n",
    "        # axes[1].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if self.save_path:\n",
    "            plt.savefig(os.path.join(self.save_path,'reward_history.png'))\n",
    "        plt.show()\n",
    "\n",
    "    def save_model(self):\n",
    "        ''' Function to save the model and optimizer state_dict for inference or continued training '''\n",
    "        self.model_path = os.path.join(self.save_path, 'q_network_checkpoint.pth')\n",
    "        torch.save({\n",
    "            'model_state_dict': self.policy_net.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, self.model_path)\n",
    "\n",
    "    def load_model(self):\n",
    "        ''' This code overwrite the Q_net with the parameters store in the instance's save_path '''\n",
    "        checkpoint = torch.load(self.model_path)\n",
    "        self.policy_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    def prefill_buffer(self):\n",
    "        ''' Function that prefill a buffer with transitions (s,a,r,s',d) under the current policy '''\n",
    "        self.buffer = PPOBuffer(self.obs_space, self.act_space, self.buffer_size)\n",
    "        \n",
    "        buffer_filled = False\n",
    "        count = 0\n",
    "        eps_return = []     # Store v(s_0) of each episode in the buffer\n",
    "        eps_length = []\n",
    "        # Collect experience using the old (frozen) policy\n",
    "        while not buffer_filled:\n",
    "            obs, _ = self.env.reset()\n",
    "            done = False\n",
    "            eps_reward_history = []\n",
    "            \n",
    "            while not done:\n",
    "                with torch.no_grad():\n",
    "                    obs_tensor = torch.tensor(obs, dtype = torch.float32, device='cuda' if self.CUDA_ENABLED else 'cpu')\n",
    "                    action_probs = self.policy_net_old(obs_tensor)\n",
    "                    action_dist = Categorical(action_probs)\n",
    "                    action = action_dist.sample()\n",
    "                    action_logp = action_dist.log_prob(action)\n",
    "\n",
    "                next_obs, reward, term, trunc, _ = self.env.step(action.item()) \n",
    "                eps_reward_history.append(reward)\n",
    "                done = term or trunc\n",
    "\n",
    "                self.buffer.store(obs_tensor, action.item(), reward, next_obs, action_logp, done)\n",
    "                obs = next_obs\n",
    "\n",
    "                count += 1\n",
    "\n",
    "                if count >= self.buffer_size: \n",
    "                    buffer_filled = True\n",
    "                    break\n",
    "            if not buffer_filled: \n",
    "                eps_length.append(len(eps_reward_history))\n",
    "                eps_return.append(self.get_first_return(eps_reward_history))\n",
    "        self.avg_return = np.mean(eps_return)\n",
    "        self.avg_length = np.mean(eps_length)\n",
    "\n",
    "        # Calculate the estimated state value, critic target, and advantage\n",
    "        data = self.buffer.get_transition()\n",
    "        obs_tensor = torch.tensor(data['obs'], dtype = torch.float32, device='cuda' if self.CUDA_ENABLED else 'cpu')\n",
    "        rew_tensor = torch.tensor(data['rew'], dtype = torch.float32, \n",
    "                                  device='cuda' if self.CUDA_ENABLED else 'cpu').unsqueeze(dim=1)\n",
    "        nobs_tensor = torch.tensor(data['next_obs'], dtype = torch.float32, device='cuda' if self.CUDA_ENABLED else 'cpu')\n",
    "        done_tensor = torch.tensor(data['done'], dtype = torch.float32, \n",
    "                                   device='cuda' if self.CUDA_ENABLED else 'cpu').unsqueeze(dim=1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_tensor = self.value_net(obs_tensor)\n",
    "            next_val_tensor = self.value_net(nobs_tensor)\n",
    "        tar_tensor = rew_tensor + self.gamma * (1-done_tensor) * next_val_tensor\n",
    "        adv_tensor = tar_tensor - val_tensor\n",
    "\n",
    "        ## TODO - See if normalizing the advantage improvement training\n",
    "        adv_tensor = (adv_tensor - adv_tensor.mean()) / (adv_tensor.std() + 1e-8)\n",
    "\n",
    "        self.buffer.val_buf = val_tensor.cpu().numpy().squeeze()\n",
    "        self.buffer.tar_buf = tar_tensor.cpu().numpy().squeeze()\n",
    "        self.buffer.adv_buf = adv_tensor.cpu().numpy().squeeze() \n",
    "\n",
    "    def train_policy(self):\n",
    "        msg = \"Training ended with no good model found :<\"\n",
    "\n",
    "        # Create the directory to store results\n",
    "        self.run_number, self.hyperparam_config, self.save_path = self.create_directory()\n",
    "\n",
    "        # Training information\n",
    "        title = f\"PPO   |   Model {self.model_name}, alpha={self.alpha}, beta={self.beta}, gamma={self.gamma}\" \n",
    "        if self.VERBOSE: print(title)\n",
    "\n",
    "        self.reward_history = []                # Track the total reward per episode\n",
    "        self.val_history = {}                   # Reset the validation history\n",
    "        self.value_history = []                 # History of the v(s_0) calculated using G_t throughout training\n",
    "        self.value_est_history = []             # History of the \\hat v(s_0) calculated using the value net (baseline case)\n",
    "        self.val_time = 0                       # Time used for validation (s)\n",
    "\n",
    "        # Control of early stopping\n",
    "        consecutive_pass_count = 0\n",
    "        CONSECUTIVE_PASS_LIMIT = 3\n",
    "        EPISODE_REWARD_LIMIT = SUCCESS_CRITERIA\n",
    "        best_reward = 0\n",
    "        self.best_model_episode = None\n",
    "        performance_crit = False                # Whether desired performance is met consistently\n",
    "        train_terminated = False\n",
    "        \n",
    "\n",
    "        self.train_time_start = time.time()\n",
    "        \n",
    "        # while not train_terminated:             # Experiment level - iterate over training episodes\n",
    "        for iter in range(self.N_ITER_TRAIN):\n",
    "            self.policy_net_old.load_state_dict(self.policy_net.state_dict())\n",
    "            self.prefill_buffer()               # Generate the training data using the old policy\n",
    "\n",
    "            self.reward_history.append(self.avg_length)\n",
    "\n",
    "            # Periodic data logger\n",
    "            if iter % self.LOG_PERIOD == 0 and self.VERBOSE:\n",
    "                printout_msg = f\"Iteration {iter:3d} : Mean V(s_0) {self.avg_return:7.2f} | Mean Length {self.avg_length:7.2f}\"\n",
    "                print(printout_msg, end='\\r')\n",
    "\n",
    "            # Early stopping condition\n",
    "            if self.avg_length >= EPISODE_REWARD_LIMIT:\n",
    "                # Evaluate the current good policy and record the validation time\n",
    "                self.val_time_start = time.time()\n",
    "                test_reward, _ = self.policy_eval(self.env_val, 20, verbose=False)\n",
    "                self.val_time += time.time() - self.val_time_start\n",
    "\n",
    "                self.val_history[iter] = [self.avg_length, test_reward]\n",
    "\n",
    "                if test_reward >= best_reward:           # Set the new best reward\n",
    "                    best_reward = test_reward\n",
    "                    self.save_model()\n",
    "                    self.best_model_iteration = iter\n",
    "                    msg = f\"Training terminated due to iteration limit, best model saved at episode {self.best_model_iteration:5d}\"\n",
    "                if test_reward > EPISODE_REWARD_LIMIT: \n",
    "                    consecutive_pass_count += 1\n",
    "                else: consecutive_pass_count = 0\n",
    "            else:\n",
    "                consecutive_pass_count = 0\n",
    "\n",
    "            # Performance criteria - if good results for several episodes => training performance satisfied and terminate early\n",
    "            if consecutive_pass_count >= CONSECUTIVE_PASS_LIMIT:\n",
    "                self.save_model()\n",
    "                self.best_model_iteration = iter\n",
    "                performance_crit = True \n",
    "                msg = f\"Early termination at iteration {self.best_model_iteration:5d}, desired performance reached\"\n",
    "\n",
    "            # Checking for early training termination or truncation\n",
    "            train_terminated = performance_crit\n",
    "            if train_terminated:\n",
    "                break\n",
    "            \n",
    "            # Loop through epochs\n",
    "            for epoch in range(self.N_EPOCH):       # Iterate over multiple epochs of training data\n",
    "                indices = np.arange(self.buffer_size)\n",
    "                np.random.shuffle(indices)\n",
    "\n",
    "                # Iterate over mini batches\n",
    "                for start in range(0, self.buffer_size, self.batch_size):\n",
    "                    end = start + self.batch_size\n",
    "                    batch_idx = indices[start:end]\n",
    "                    \n",
    "                    batch_data = self.buffer.get_data()\n",
    "                    obs_batch = torch.tensor(batch_data['obs'][batch_idx], dtype = torch.float32, device='cuda' if self.CUDA_ENABLED else 'cpu')\n",
    "                    act_batch = torch.tensor(batch_data['act'][batch_idx], dtype = torch.float32, device='cuda' if self.CUDA_ENABLED else 'cpu')\n",
    "                    logp_old = torch.tensor(batch_data['logp'][batch_idx], dtype = torch.float32, device='cuda' if self.CUDA_ENABLED else 'cpu')\n",
    "                    adv_batch = torch.tensor(batch_data['adv'][batch_idx], dtype = torch.float32, device='cuda' if self.CUDA_ENABLED else 'cpu')\n",
    "                    tar_batch = torch.tensor(batch_data['tar'][batch_idx], dtype = torch.float32, \n",
    "                                             device='cuda' if self.CUDA_ENABLED else 'cpu').unsqueeze(dim=1)\n",
    "\n",
    "                    action_probs = self.policy_net(obs_batch)\n",
    "                    action_dist = Categorical(action_probs)\n",
    "                    logp = action_dist.log_prob(act_batch)\n",
    "\n",
    "                    ratios = torch.exp(logp - logp_old)\n",
    "                    clip_ratios = torch.clamp(ratios, 1 - self.eps, 1 + self.eps)\n",
    "                    actor_loss = -torch.mean(torch.min(ratios * adv_batch, clip_ratios * adv_batch))\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    actor_loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    # Optimize the critic network\n",
    "                    val_tensor = self.value_net(obs_batch)\n",
    "                    critic_loss = nn.MSELoss()(tar_batch, val_tensor)\n",
    "                    self.value_optimizer.zero_grad()\n",
    "                    critic_loss.backward()\n",
    "                    self.value_optimizer.step()\n",
    "\n",
    "        self.train_time = time.time() - self.train_time_start\n",
    "\n",
    "        print(f\"\\nTotal runtime - {self.train_time:5.2f}\")\n",
    "        print(msg)\n",
    "        return\n",
    "\n",
    "    def record(self):\n",
    "        # Load the best policy net parameter from the experiment\n",
    "        if self.model_path:\n",
    "            self.load_model()\n",
    "\n",
    "        # Average test reward of the resulting policy - if best policy does not exists use the last one\n",
    "        self.env_test = gym.make('CartPole-v1')\n",
    "        print('Testing the best policy network performance')\n",
    "        reward_mean, reward_stdev = self.policy_eval(self.env_test, n_iter_test=500, verbose=True)\n",
    "        print(f\"\\nValidation average reward {reward_mean:4.2f} (SD = {reward_stdev:4.2f})\")\n",
    "\n",
    "        # Store the validation history and average test reward in the param_config JSON file\n",
    "        param_path = os.path.join(self.save_path, 'param_config.json')\n",
    "        if os.path.exists(param_path):\n",
    "            with open(param_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "        else:\n",
    "            data = {}\n",
    "\n",
    "        data['runtime'] = self.train_time\n",
    "        data['valtime'] = self.val_time\n",
    "        data['best_model_at'] = self.best_model_iteration\n",
    "        data['val_history'] = self.val_history\n",
    "        data['test_result'] = [reward_mean, reward_stdev]\n",
    "\n",
    "        with open(param_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "        # Plot\n",
    "        self.plot_reward_hist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
