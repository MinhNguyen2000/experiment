{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "177b3eb5",
   "metadata": {},
   "source": [
    "# DQN Grid Search\n",
    "\n",
    "This script is an extension to the inverted pendulum DQN algorithm in inv_pend.ipynb. This scripts automates the grid search in a hyperparameter space to explore the best performance of DQN.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5928a33",
   "metadata": {},
   "source": [
    "TODO\n",
    "- [ ] Package functions related to the experiment in a `GridSearch` class\n",
    "    - **Attributes:**\n",
    "        - Hyperparameters - model arch, lr , buffer_size, min_replay_szie, target_update_freq, gamma, eps_start, eps_end, eps_decay, episode_train, and batch_size\n",
    "        - The simulation environment\n",
    "    - **Methods:** \n",
    "        - `create_directory()` - for storing the training results for each hyperparameter configuration\n",
    "        - `eps_greedy_policy()` - for picking out an action given the observation\n",
    "        - `DQN_train()` - for training a Q network from simulation\n",
    "\n",
    "- [ ] Save the model (parameters and architecture) following training in the grid searchd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d299b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random                       # To draw a random batch of samples form the replay buffer\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os                                               # For saving models and training results\n",
    "from datetime import datetime                           # For creating the directory of each training run\n",
    "import json                                             # For storing training parameters during each run\n",
    "import re                                               # For checking the latest trial #\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "from collections import deque       # replay buffer is a double ended queue that allows elements to be added either to the end or the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3ab4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc95fe8",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978d2147",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet_MLP(nn.Module):\n",
    "    ''' A QNetwork class that dynamically initialize an MLP Q Net'''\n",
    "    def __init__(self,input_dim,output_dim,hidden_layer = [64,32]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        for size in hidden_layer:\n",
    "            self.layers.append(nn.Linear(self.input_dim, size))\n",
    "            self.input_dim = size\n",
    "        \n",
    "        self.layers.append(nn.Linear(self.input_dim,self.output_dim))\n",
    "\n",
    "        self.cuda()\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        for layer in self.layers[:-1]:\n",
    "            input_data = torch.relu(layer(input_data))\n",
    "        return self.layers[-1](input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22aa2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(model_id: int,\n",
    "                     lr: float, \n",
    "                     gamma: float,\n",
    "                     epsilon_decay: int,\n",
    "                     batch_size: int, \n",
    "                     buffer_size: int,\n",
    "                     target_update_freq: int):\n",
    "    ''' Function that creates directory to save model state_dict, architecture, training configuration, and history\n",
    "\n",
    "    Parameters: \n",
    "    ------------\n",
    "    (hyperparameters for differentiating between different directory)\n",
    "    \n",
    "    lr : float\n",
    "        the learning rate to optimize the Q network\n",
    "    gamma : float \n",
    "        the discount rate in Q learning\n",
    "    epsilon_decay : integer\n",
    "        the amount of episode over which the exploratory rate (epsilon) decays\n",
    "    batch_size : integer\n",
    "        number of experience drawn from replay buffer to train the behaviour network\n",
    "    buffer_size : integer\n",
    "        the number of samples in the replay buffer at a time\n",
    "    target_udpate_freq : integer\n",
    "        the amount of step count during the training process before updating the target Q net (loading the parameters of the behaviour net onto the target Q Net)\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    ------------\n",
    "    name_codified : str\n",
    "        the shortened name for the current experiment \n",
    "    hyperparameters_codified : str\n",
    "        the shortened string of hyperparameter configuration\n",
    "    OUTPUT_DIR : path\n",
    "        the directory to which the training results and model (state_dict and architecture) will be saved\n",
    "    '''\n",
    "    timestamp = datetime.now().strftime(\"%y%m%d_%H%M\")\n",
    "\n",
    "    BASE_DIR = os.getcwd()\n",
    "    RESULT_DIR = os.path.join(BASE_DIR,\"cartpole_DQN_results\")\n",
    "    os.makedirs(RESULT_DIR, exist_ok=True)      # Create the directory if one does not already exist\n",
    "\n",
    "    # Find the trial # of the latest run\n",
    "    existing_runs = [d for d in os.listdir(RESULT_DIR) if os.path.isdir(os.path.join(RESULT_DIR,d))]\n",
    "    run_numbers = [int(re.search(r'run_(\\d{5})',d).group(1)) for d in existing_runs if re.match(r'run_\\d{5}',d)]\n",
    "    trial_number = max(run_numbers,default=-1)+1\n",
    "\n",
    "    # Create a folder for the run\n",
    "    name_codified = f\"run_{trial_number:05d}\"\n",
    "    OUTPUT_DIR = os.path.join(RESULT_DIR,name_codified)\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)      # Create the directory\n",
    "\n",
    "    \n",
    "    # Store the training configs in JSON file\n",
    "    training_params = {\n",
    "        'model_id': model_id,\n",
    "        'lr': lr,\n",
    "        'gamma': gamma,\n",
    "        'epsilon_decay': epsilon_decay,\n",
    "        'batch_size': batch_size,\n",
    "        'buffer_size': buffer_size,\n",
    "        'target_update_freq': target_update_freq\n",
    "    }\n",
    "\n",
    "    # Append the mapping from run # to hyperparameter configuration in a JSON file inside RESULT_DIR\n",
    "    trial_to_param_path = os.path.join(RESULT_DIR,'trial_to_param.json')\n",
    "    if os.path.exists(trial_to_param_path):\n",
    "        with open(trial_to_param_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        data = {name_codified: []}\n",
    "\n",
    "    hyperparam_codified = f\"{model_id}_{lr}_{buffer_size}_{target_update_freq}_{gamma}_{epsilon_decay}_{batch_size}\"\n",
    "    hyperparam_codified_time = f\"{timestamp}_{model_id}_{lr}_{buffer_size}_{target_update_freq}_{gamma}_{epsilon_decay}_{batch_size}\"\n",
    "    data[name_codified] = hyperparam_codified_time\n",
    "\n",
    "    with open(trial_to_param_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "    # Store training parameters in each run \n",
    "    param_path = os.path.join(OUTPUT_DIR, \"param_config.json\")\n",
    "    with open(param_path, \"w\") as f:\n",
    "        json.dump({\"parameters\": training_params}, f, indent=2)\n",
    "\n",
    "    return name_codified, hyperparam_codified, OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d464d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy_policy(env, obs, epsilon: float, q_network: QNet_MLP):\n",
    "    ''' Function to take an action according to an epsilon-greedy policy and a Q-network'''\n",
    "    if np.random.random() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.tensor(obs, dtype=torch.float32, device = 'cuda').unsqueeze(0)\n",
    "            q_values = q_network(state_tensor)\n",
    "            action = q_values.argmax().item()\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef2a6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(env_test, q_network, n_episode_test = 500):\n",
    "    ''' Assess the average reward when following a q_network in a test environment with random state initialization '''\n",
    "    \n",
    "    total_reward = 0\n",
    "    for i in range(n_episode_test):\n",
    "        obs,_ = env_test.reset()\n",
    "        done = False\n",
    "        eps_reward = 0\n",
    "\n",
    "        while not done:                 # Step thorugh the episode\n",
    "            action = eps_greedy_policy(env_test, obs, epsilon=0, q_network=q_network)\n",
    "            next_obs, reward, term, trunc, _ = env_test.step(action)\n",
    "\n",
    "            eps_reward += reward\n",
    "\n",
    "            obs = next_obs\n",
    "            done = term or trunc\n",
    "    \n",
    "        total_reward += eps_reward\n",
    "    average_reward = total_reward / n_episode_test\n",
    "\n",
    "    return average_reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f086ecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(q_network: nn.Module, optimizer: torch.optim.Optimizer, save_path):\n",
    "    ''' Function to save the model and optimizer state_dict for inference or continued training '''\n",
    "    torch.save({\n",
    "        'model_state_dict': q_network.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, os.path.join(save_path, 'q_network_checkpoint.pth'))\n",
    "\n",
    "def load_model(q_network: nn.Module, model_path):\n",
    "    checkpoint = torch.load(model_path)\n",
    "    q_network.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45dba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN_train(env: gym.Env, env_test: gym.Env,\n",
    "              q_network: nn.Module, target_net: nn.Module, optimizer: torch.optim.Optimizer, \n",
    "              replay_buffer,\n",
    "              target_update_freq,\n",
    "              gamma,\n",
    "              eps_start, eps_end, eps_decay,\n",
    "              episode_train,\n",
    "              batch_size, \n",
    "              save_path,\n",
    "              seed = 42):\n",
    "    ''' Function to train a policy for a set of hyperparameters '''\n",
    "\n",
    "    msg = \"Training ended, no good model found!\"\n",
    "\n",
    "    reward_history = np.zeros(episode_train)\n",
    "    epsilon = eps_start\n",
    "    step_count = 0\n",
    "    episode = 0\n",
    "    target_network_update_count = 0\n",
    "\n",
    "    # Control of early stopping\n",
    "    consecutive_pass_count = 0           # Number of consecutive episodes where performance exceeds a threshold\n",
    "    CONSECUTIVE_PASS_LIMIT = 3          # No. of consecutive episodes with higher performance than reward limit\n",
    "    EPISODE_REWARD_LIMIT = 450\n",
    "    best_reward = 0\n",
    "    performance_crit = False\n",
    "    train_terminated = False\n",
    "    val_history = {}                    # Monitor which episode had a validation run, the train reward, and the validation (test) reward \n",
    "\n",
    "    while not train_terminated:     # Experiment level - loop through episodes\n",
    "        obs, _ = env.reset(seed=seed)\n",
    "        eps_rewards = 0\n",
    "    \n",
    "        while True:                 # Episode level - loop through steps\n",
    "            action = eps_greedy_policy(env, obs, epsilon, q_network)\n",
    "\n",
    "            # Interact with the environment\n",
    "            next_obs, reward, term, trunc, _ = env.step(action)\n",
    "            done = term or trunc\n",
    "            replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "            obs = next_obs\n",
    "            eps_rewards += reward\n",
    "            step_count += 1\n",
    "\n",
    "            # Train the Q-net using a batch of samples from the experience replay\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch = random.sample(replay_buffer, batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                states =        torch.tensor(np.array(states),      dtype = torch.float32,  device='cuda')           # Shape of (BATCH_SIZE, 4). Data structure of [[p1 v1 theta1 dtheta1], [p2 v2 theta2 dtheta2], ...]\n",
    "                actions =       torch.tensor(actions,               dtype = torch.long,     device='cuda').unsqueeze(1)        # Must unsqueeze to have shape (BATCH_SIZE, 1). From [a1, a2, ...] to [[a1], [a2], ... [a_BATCH_SIZE]]\n",
    "                rewards =       torch.tensor(rewards,               dtype = torch.float32,  device='cuda').unsqueeze(1)\n",
    "                next_states =   torch.tensor(np.array(next_states), dtype = torch.float32,  device = 'cuda')\n",
    "                dones =         torch.tensor(dones,                 dtype = torch.float32,  device='cuda').unsqueeze(1)\n",
    "                \n",
    "                # Compute targets using target network Q(s',a',w_i^-)\n",
    "                with torch.no_grad():\n",
    "                    target_q_values = target_net(next_states)       # Find a batch of Q(s',a',w_i^-) from the batch of next_states\n",
    "                    max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]\n",
    "                    targets = rewards + gamma * max_target_q_values * (1 - dones)       # (1-dones) to avoid over estimating the value of terminal states. The target should only be the reward in the terminal states\n",
    "            \n",
    "                # Compute the current Q values for the actions taken Q(s,a,w_i)\n",
    "                q_values = q_network(states).gather(1, actions)         # obtain the q_values associated to the actual action taken in each sample\n",
    "\n",
    "                # Update the parameters of the behaviour q_network\n",
    "                loss = nn.MSELoss()(q_values, targets)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Periodically update the target network by loading the weights from the behavior network\n",
    "            if step_count % target_update_freq == 0:\n",
    "                target_network_update_count += 1\n",
    "                target_net.load_state_dict(q_network.state_dict())\n",
    "\n",
    "            if done:        # End of a training episode\n",
    "                break\n",
    "\n",
    "        # Decay epsilon after an episode\n",
    "        epsilon = max(eps_end, epsilon - (eps_start - eps_end)/eps_decay)\n",
    "\n",
    "        reward_history[episode] = eps_rewards\n",
    "        # print(f\"Episode {episode:5d}: Total reward = {eps_rewards:5.1f}, Epsilon = {epsilon:.3f}, Step count = {step_count:5d}, Target update count = {target_network_update_count:3d}\")\n",
    "        # print(f\"Average reward {sum(reward_history[:episode])/(episode+1):.2f}\")\n",
    "\n",
    "        \n",
    "        if episode % 10 == 0:                   # print progress periodically\n",
    "            print(f\"Episode {episode:5d}: Total reward = {eps_rewards:5.1f}, Epsilon = {epsilon:.3f}\", end = \"\\r\")\n",
    "\n",
    "        # Early stopping condition\n",
    "        if eps_rewards >= EPISODE_REWARD_LIMIT:\n",
    "            test_reward = policy_eval(env_test, q_network, 100)\n",
    "            val_history[episode] = [eps_rewards, test_reward]\n",
    "\n",
    "            if test_reward >= best_reward:           # Set the new best reward\n",
    "                best_reward = test_reward\n",
    "                save_model(q_network, optimizer, save_path)\n",
    "                msg = f\"Training terminated due to episode limit, best model saved at episode {episode:5d}\"\n",
    "            if test_reward > EPISODE_REWARD_LIMIT: \n",
    "                consecutive_pass_count += 1\n",
    "            else: consecutive_pass_count = 0\n",
    "        else:\n",
    "            consecutive_pass_count = 0\n",
    "            \n",
    "        # Performance criteria - if good results for several episodes => training performance satisfied and terminate early\n",
    "        if consecutive_pass_count >= CONSECUTIVE_PASS_LIMIT:\n",
    "            performance_crit = True \n",
    "            msg = f\"Early termination at episode {episode:5d}, desired performance reached\"\n",
    "\n",
    "\n",
    "        episode += 1\n",
    "\n",
    "        # Checking for early training termination or truncation\n",
    "        train_terminated = (episode >= episode_train) or (performance_crit)\n",
    "    print(\"\\n\")\n",
    "    return reward_history, val_history, msg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b9ed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for EMA filters and plotting data\n",
    "def EMA_filter(reward: list, alpha):\n",
    "    ''' Function that runs an exponential moving average filter along a datastream '''\n",
    "    output = np.zeros(len(reward)+1)\n",
    "    output[0] = reward[0]\n",
    "    for idx, item in enumerate(reward):\n",
    "        output[idx+1] = (1 - alpha) * output[idx] + alpha * item\n",
    "    \n",
    "    return output\n",
    "\n",
    "def plot_reward_hist(reward_history: list, hyperparam_config: str, save_path = None, alpha = 0.1):\n",
    "    ''' Function that plots the reward and filtered reward per episode, then saves the plot in a specified save directory'''\n",
    "    n_episodes= len(reward_history)\n",
    "    episodes = range(n_episodes)\n",
    "    filtered_reward_hist = EMA_filter(reward_history, alpha)\n",
    "\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.plot(episodes, reward_history[:n_episodes], color = \"blue\")\n",
    "    plt.plot(episodes, filtered_reward_hist[:n_episodes], color = \"red\")\n",
    "    plt.title(f'Total reward per episode - {hyperparam_config}')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend(['Total reward','Filtered reward'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(os.path.join(save_path,'reward_history.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58d19d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script test --no-raise-error\n",
    "# Test creating the Q network with dynamic hidden layer when specifying the list of hidden nodes\n",
    "\n",
    "Q_net = QNet_MLP(obs_space,action_space,[64,32])\n",
    "summary(Q_net, (obs_space,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a80f799",
   "metadata": {},
   "source": [
    "## Parameter Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d83409",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_registry = {\n",
    "    'DQN_MLP_v0': {\n",
    "        'class': QNet_MLP,\n",
    "        'config': [64,32]\n",
    "    },\n",
    "    'DQN_MLP_v1': {\n",
    "        'class': QNet_MLP,\n",
    "        'config': [32,32]\n",
    "    },\n",
    "    'DQN_MLP_v2': {\n",
    "        \n",
    "        'class': QNet_MLP,\n",
    "        'config': [32,16]\n",
    "    },\n",
    "    'DQN_MLP_v3': {\n",
    "        'class': QNet_MLP,\n",
    "        'config': [16,16]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e314f87",
   "metadata": {},
   "source": [
    "The block below is used for testing the model creation automation (Uncomment the first line to run the test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43edbf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script test --no-raise-error\n",
    "for model in model_registry:\n",
    "    Q_net = QNet_MLP(obs_space, action_space, model_registry[model]['config'])\n",
    "    # print(model_registry[model]['config'])\n",
    "    print(Q_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060fc8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual parameter grid\n",
    "param_grid = {\n",
    "    'MODEL': [model for model in model_registry],\n",
    "    'LR': [5e-4, 1e-3, 5e-3, 1e-2],\n",
    "    \"BUFFER_SIZE\": [1000, 5000, 10000],\n",
    "    \"MIN_REPLAY_SIZE\": [1000],\n",
    "    \"TARGET_UPDATE_FREQ\": [1000, 5000, 10000],\n",
    "\n",
    "    \"GAMMA\": [0.90, 0.95, 0.98],\n",
    "    \"EPSILON_START\": [1.0],\n",
    "    \"EPSILON_END\": [0.1],\n",
    "    \"EPSILON_DECAY\": [1000, 5000, 10000],\n",
    "\n",
    "    \"EPISODE_TRAIN\": [5000],                # training truncation criteria\n",
    "    \"BATCH_SIZE\": [32, 64, 128]\n",
    "}\n",
    "\n",
    "success_criteria = 450                      #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bf3c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparam grid - vary one at a time\n",
    "param_grid = {\n",
    "    'MODEL': ['DQN_MLP_v0'],\n",
    "    'LR': [5e-4],\n",
    "    \"BUFFER_SIZE\": [5000],\n",
    "    \"MIN_REPLAY_SIZE\": [1000],\n",
    "    \"TARGET_UPDATE_FREQ\": [1000],\n",
    "\n",
    "    \"GAMMA\": [0.90, 0.95, 0.98],\n",
    "    \"EPSILON_START\": [1.0],\n",
    "    \"EPSILON_END\": [0.1],\n",
    "    \"EPSILON_DECAY\": [5000],\n",
    "\n",
    "    \"EPISODE_TRAIN\": [5000],                # training truncation criteria\n",
    "    \"BATCH_SIZE\": [32]\n",
    "}\n",
    "\n",
    "success_criteria = 450                      #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c535ea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Simplified param grid to test functionality\n",
    "# param_grid = {\n",
    "#     # 'MODEL': [model for model in model_registry],\n",
    "#     'MODEL': ['DQN_MLP_v0'],\n",
    "#     'LR': [5e-4],\n",
    "#     \"BUFFER_SIZE\": [5000],\n",
    "#     \"MIN_REPLAY_SIZE\": [1000],\n",
    "#     \"TARGET_UPDATE_FREQ\": [1000],\n",
    "\n",
    "#     \"GAMMA\": [0.95],\n",
    "#     \"EPSILON_START\": [1.0],\n",
    "#     \"EPSILON_END\": [0.1],\n",
    "#     \"EPSILON_DECAY\": [5000],\n",
    "\n",
    "#     \"EPISODE_TRAIN\": [5000],                # training truncation criteria\n",
    "#     \"BATCH_SIZE\": [32]\n",
    "# }\n",
    "\n",
    "# success_criteria = 450                      #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bb6584",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da69956",
   "metadata": {},
   "source": [
    "Using itertools to loop through each combination of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a343ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time\n",
    "keys, values = zip(*param_grid.items())\n",
    "# keys, values = param_grid.keys(), param_grid.values()\n",
    "num_config = len(list(itertools.product(*values)))\n",
    "\n",
    "# Set fixed seed\n",
    "seed = 42\n",
    "np.random.seed(seed); random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "for idx, v in enumerate(itertools.product(*values)):\n",
    "\n",
    "    # Unpacking the hyperparameter configurations\n",
    "    config = dict(zip(keys,v))\n",
    "    MODEL_NAME = config['MODEL']\n",
    "    MODEL_CLASS = model_registry[MODEL_NAME]['class']\n",
    "    MODEL_CONFIG = model_registry[MODEL_NAME]['config']\n",
    "    match = re.search(r'v\\d+',MODEL_NAME)\n",
    "    MODEL_ID = match.group(0) if match else 404\n",
    "\n",
    "    LR = config['LR']\n",
    "    BUFFER_SIZE = config['BUFFER_SIZE']\n",
    "    MIN_REPLAY_SIZE = config['MIN_REPLAY_SIZE']\n",
    "    TARGET_UPDATE_FREQ = config['TARGET_UPDATE_FREQ']\n",
    "    GAMMA = config['GAMMA']\n",
    "    EPS_START = config['EPSILON_START']\n",
    "    EPS_END = config['EPSILON_END']\n",
    "    EPS_DECAY = config['EPSILON_DECAY']\n",
    "    EPISODE_TRAIN = config['EPISODE_TRAIN']\n",
    "    BATCH_SIZE = config['BATCH_SIZE']\n",
    "\n",
    "\n",
    "    # Re-initialize the environment\n",
    "    env = gym.make('CartPole-v1')\n",
    "    env_test = gym.make('CartPole-v1')\n",
    "    obs,_ = env.reset(seed=seed)\n",
    "\n",
    "    obs_space = env.observation_space.shape[0]\n",
    "    action_space = env.action_space.n\n",
    "\n",
    "\n",
    "    # Re-initialize the NN models\n",
    "    Q_net = MODEL_CLASS(obs_space,action_space,MODEL_CONFIG)\n",
    "    target_net = MODEL_CLASS(obs_space,action_space,MODEL_CONFIG)\n",
    "    target_net.load_state_dict(Q_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.SGD(Q_net.parameters(), lr = LR)\n",
    "    # optimizer = optim.Adam(Q_net.parameters(), lr = LR)\n",
    "\n",
    "    # Re-initialize and pre-fill the replay buffer\n",
    "    replay_buffer = deque(maxlen = BUFFER_SIZE)\n",
    "    \n",
    "    obs, _ = env.reset()\n",
    "    for _ in range(MIN_REPLAY_SIZE):\n",
    "        action = env.action_space.sample()\n",
    "        next_obs, reward, term, trunc, _ = env.step(action)\n",
    "        done = term or trunc\n",
    "\n",
    "        replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "        obs = next_obs if not done else env.reset()[0]\n",
    "\n",
    "    # Create the directory to store results\n",
    "    _, hyperparam_config, save_path = create_directory(MODEL_ID,LR,GAMMA,EPS_DECAY,BATCH_SIZE,BUFFER_SIZE,TARGET_UPDATE_FREQ)\n",
    "\n",
    "    # Training information\n",
    "    print(f'Trial {idx+1}/{num_config} - model {MODEL_NAME}, lr={LR}, buffer={BUFFER_SIZE}, target_freq={TARGET_UPDATE_FREQ}, gamma={GAMMA}, eps_decay={EPS_DECAY}, batch_size={BATCH_SIZE}')\n",
    "    \n",
    "    # Train the DQN with the given hyperparameter configuration\n",
    "    start_time = time.time()\n",
    "    reward_history, val_history, msg = DQN_train(env, env_test,\n",
    "                                    Q_net, target_net,optimizer,\n",
    "                                    replay_buffer,\n",
    "                                    TARGET_UPDATE_FREQ, GAMMA, \n",
    "                                    EPS_START, EPS_END, EPS_DECAY,\n",
    "                                    EPISODE_TRAIN,\n",
    "                                    BATCH_SIZE, \n",
    "                                    save_path,\n",
    "                                    seed=seed)\n",
    "    end_time = time.time()\n",
    "    print(f\"Runtime - {end_time - start_time:.3f}\")\n",
    "    print(msg)\n",
    "\n",
    "    # Load the best Q net parameter from the experiment\n",
    "    model_path = os.path.join(save_path,'q _network_checkpoint.pth')\n",
    "    load_model(Q_net, model_path)\n",
    "\n",
    "    # Average test reward of the resulting policy\n",
    "    env_test = gym.make('CartPole-v1')\n",
    "    average_reward = policy_eval(env_test, Q_net, n_episode_test=500)\n",
    "    print(f\"Validation average reward {average_reward:4.2f}\")\n",
    "\n",
    "    # Store the validation hisotry and average test reward in the param_config JSON file\n",
    "    param_path = os.path.join(save_path,'param_config.json')\n",
    "    if os.path.exists(param_path):\n",
    "        with open(param_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        data = {}\n",
    "    data['val_history'] = val_history\n",
    "    data['test_result'] = average_reward\n",
    "    \n",
    "    \n",
    "\n",
    "    with open(param_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    \n",
    "    # Display and save the reward history in current trial folderr \n",
    "    plot_reward_hist(reward_history, hyperparam_config, save_path)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def4e72",
   "metadata": {},
   "source": [
    "## Load and Simulate Saved Model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e22994a",
   "metadata": {},
   "source": [
    "# Double DQN (DDQN) Grid Search\n",
    "\n",
    "This script is an extension to the inverted pendulum DQN algorithm in inv_pend.ipynb. This scripts automates the grid search in a hyperparameter space to explore the best performance of DQN.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d86e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random                       # To draw a random batch of samples form the replay buffer\n",
    "import gymnasium as gym             # To create the inverted pendulum environment\n",
    "import torch                        \n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import deque       # replay buffer is a double ended queue that allows elements to be added either to the end or the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0af745b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142e2e58",
   "metadata": {},
   "source": [
    "## Parameter & Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "735f4e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Actual parameter grid\n",
    "# param_grid = {\n",
    "#     'MODEL': [model for model in model_registry],\n",
    "#     'LR': [5e-4, 1e-3, 5e-3, 1e-2],\n",
    "#     \"BUFFER_SIZE\": [1000, 5000, 10000],\n",
    "#     \"MIN_REPLAY_SIZE\": [1000],\n",
    "#     \"TARGET_UPDATE_FREQ\": [1000, 5000, 10000],\n",
    "\n",
    "#     \"GAMMA\": [0.90, 0.95, 0.98],\n",
    "#     \"EPSILON_START\": [1.0],\n",
    "#     \"EPSILON_END\": [0.1],\n",
    "#     \"EPSILON_DECAY\": [1000, 5000, 10000],\n",
    "\n",
    "#     \"EPISODE_TRAIN\": [5000],                # training truncation criteria\n",
    "#     \"BATCH_SIZE\": [32, 64, 128]\n",
    "# }\n",
    "\n",
    "# success_criteria = 450                      #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe39380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparam grid - vary one at a time\n",
    "param_grid = {\n",
    "    'MODEL': ['DQN_MLP_v0'],\n",
    "    'LR': [1e-4, 5e-4, 1e-3, 5e-3],\n",
    "    # 'LR': [1e-4],\n",
    "    \"BUFFER_SIZE\": [5000],\n",
    "    \"MIN_REPLAY_SIZE\": [1000],\n",
    "    \"TARGET_UPDATE_FREQ\": [1000],\n",
    "\n",
    "    \"GAMMA\": [0.95],\n",
    "    \"EPSILON_START\": [1.0],\n",
    "    \"EPSILON_END\": [0.1],\n",
    "    \"EPSILON_DECAY\": [5000],\n",
    "\n",
    "    \"EPISODE_TRAIN\": [5000],                # training truncation criteria\n",
    "    \"BATCH_SIZE\": [32]\n",
    "}\n",
    "\n",
    "CUDA_ENABLED = False                        # Control whether to use cuda or not\n",
    "SUCCESS_CRITERIA = 450                      # Control the performance threshold for early stopping during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30a1e910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Simplified param grid to test functionality\n",
    "# param_grid = {\n",
    "#     # 'MODEL': [model for model in model_registry],\n",
    "#     'MODEL': ['DQN_MLP_v0'],\n",
    "#     'LR': [5e-4],\n",
    "#     \"BUFFER_SIZE\": [5000],\n",
    "#     \"MIN_REPLAY_SIZE\": [1000],\n",
    "#     \"TARGET_UPDATE_FREQ\": [1000],\n",
    "\n",
    "#     \"GAMMA\": [0.95],\n",
    "#     \"EPSILON_START\": [1.0],\n",
    "#     \"EPSILON_END\": [0.1],\n",
    "#     \"EPSILON_DECAY\": [5000],\n",
    "\n",
    "#     \"EPISODE_TRAIN\": [5000],                # training truncation criteria\n",
    "#     \"BATCH_SIZE\": [32]\n",
    "# }\n",
    "\n",
    "# success_criteria = 450                      #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4848ed7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet_MLP(nn.Module):\n",
    "    ''' A QNetwork class that dynamically initialize an MLP Q Net'''\n",
    "    def __init__(self,input_dim,output_dim,hidden_layer = [64,32], cuda_enabled=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        for size in hidden_layer:\n",
    "            self.layers.append(nn.Linear(self.input_dim, size))\n",
    "            self.input_dim = size\n",
    "        \n",
    "        self.layers.append(nn.Linear(self.input_dim,self.output_dim))\n",
    "\n",
    "        if cuda_enabled: self.cuda()\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        for layer in self.layers[:-1]:\n",
    "            input_data = torch.relu(layer(input_data))\n",
    "        return self.layers[-1](input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beb8f2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_registry = {\n",
    "    'DQN_MLP_v0': {\n",
    "        'class': QNet_MLP,\n",
    "        'config': [64,32]\n",
    "    },\n",
    "    'DQN_MLP_v1': {\n",
    "        'class': QNet_MLP,\n",
    "        'config': [32,32]\n",
    "    },\n",
    "    'DQN_MLP_v2': {\n",
    "        \n",
    "        'class': QNet_MLP,\n",
    "        'config': [32,16]\n",
    "    },\n",
    "    'DQN_MLP_v3': {\n",
    "        'class': QNet_MLP,\n",
    "        'config': [16,16]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76cf7187",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN_experiment():\n",
    "    def __init__(self, model_name: str,      # \"DQN_MLP_v0\" or \"DQN_MLP_v1\"\n",
    "                 model_registry, \n",
    "                 lr: float, \n",
    "                 buffer_size: int, \n",
    "                 target_update_freq: int, \n",
    "                 gamma: float, \n",
    "                 eps_start: float, \n",
    "                 eps_decay: int,\n",
    "                 eps_end: float, \n",
    "                 batch_size: int,\n",
    "                 seed = 42,\n",
    "                 cuda_enabled = False):\n",
    "        \n",
    "        self.seed = seed\n",
    "        self.cuda_enabled = cuda_enabled\n",
    "        \n",
    "        ''' Defining hyperparameters in the experiment '''\n",
    "        self.model_name = model_name                                        # Full name of the model\n",
    "        self.model_class = model_registry[self.model_name]['class']             # The model class \"QNet_MLP\" or \"QNet_test\"\n",
    "        self.model_config = model_registry[self.model_name]['config']       # List of nodes in each hidden layer\n",
    "        match = re.search(r'v\\d+',self.model_name)\n",
    "        self.model_id = match.group(0) if match else 404                    # Extract the \"v0\" or \"v1\" out of model name for abbreviation\n",
    "\n",
    "        # Hyperparameters of the experiment\n",
    "        self.lr = lr\n",
    "        self.buffer_size = buffer_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.gamma = gamma\n",
    "        self.eps_start, self.eps, self.eps_decay, self.eps_end = eps_start, eps_start, eps_decay, eps_end\n",
    "        self.batch_size = batch_size\n",
    "        self.episode_train = 5000\n",
    "        self.min_replay_size = 1000\n",
    "\n",
    "        # Initialize the train and validation environments\n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "        self.env_val = gym.make(\"CartPole-v1\")\n",
    "        self.obs_space = self.env.observation_space.shape[0]\n",
    "        self.act_space = self.env.action_space.n\n",
    "\n",
    "        # Initialize the 2 Q networks and the optimizer for the behavior Q_net\n",
    "        self.Q_net = self.model_class(self.obs_space, self.act_space, self.model_config)\n",
    "        self.target_net = self.model_class(self.obs_space, self.act_space, self.model_config)\n",
    "        self.target_net.load_state_dict(self.Q_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.SGD(self.Q_net.parameters(), lr = self.lr)\n",
    "\n",
    "        self.save_path = \"\"                                             # Directory of the current run\n",
    "        self.model_path = \"\"                                            # Path to a model \n",
    "        self.hyperparam_config = \"\"                                     # Shortened list of importatnt hyperparameters\n",
    "        self.reward_history = np.zeros(self.episode_train)\n",
    "        self.val_history = {}                                           # Monitor which episode had a validation run, the train reward, and the validation (test) reward \n",
    "\n",
    "    def create_directory(self):\n",
    "        ''' Function that creates directory to save model state_dict, architecture, training configuration, and history\n",
    "\n",
    "        Parameters: \n",
    "        ------------\n",
    "        (hyperparameters for differentiating between different directory)\n",
    "        \n",
    "        lr : float\n",
    "            the learning rate to optimize the Q network\n",
    "        gamma : float \n",
    "            the discount rate in Q learning\n",
    "        epsilon_decay : integer\n",
    "            the amount of episode over which the exploratory rate (epsilon) decays\n",
    "        batch_size : integer\n",
    "            number of experience drawn from replay buffer to train the behaviour network\n",
    "        buffer_size : integer\n",
    "            the number of samples in the replay buffer at a time\n",
    "        target_udpate_freq : integer\n",
    "            the amount of step count during the training process before updating the target Q net (loading the parameters of the behaviour net onto the target Q Net)\n",
    "        \n",
    "\n",
    "        Returns\n",
    "        ------------\n",
    "        name_codified : str\n",
    "            the shortened name for the current experiment \n",
    "        hyperparameters_codified : str\n",
    "            the shortened string of hyperparameter configuration\n",
    "        OUTPUT_DIR : path\n",
    "            the directory to which the training results and model (state_dict and architecture) will be saved\n",
    "        '''\n",
    "        timestamp = datetime.now().strftime(\"%y%m%d_%H%M\")\n",
    "\n",
    "        BASE_DIR = os.getcwd()\n",
    "        RESULT_DIR = os.path.join(BASE_DIR, \"cartpole_DDQN_results\")\n",
    "        os.makedirs(RESULT_DIR, exist_ok=True)      # Create the directory if one does not already exist\n",
    "\n",
    "        # Find the trial # of the latest run\n",
    "        existing_runs = [d for d in os.listdir(RESULT_DIR) if os.path.isdir(os.path.join(RESULT_DIR,d))]\n",
    "        run_numbers = [int(re.search(r'run_(\\d{5})',d).group(1)) for d in existing_runs if re.match(r'run_\\d{5}',d)]\n",
    "        trial_number = max(run_numbers,default=-1)+1\n",
    "\n",
    "        # Create a folder for the run\n",
    "        name_codified = f\"run_{trial_number:05d}\"\n",
    "        OUTPUT_DIR = os.path.join(RESULT_DIR,name_codified)\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)      # Create the directory\n",
    "\n",
    "        # Append the mapping from run # to hyperparameter configuration in a JSON file inside RESULT_DIR\n",
    "        trial_to_param_path = os.path.join(RESULT_DIR,'trial_to_param.json')\n",
    "        if os.path.exists(trial_to_param_path):\n",
    "            with open(trial_to_param_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "        else:\n",
    "            data = {name_codified: []}\n",
    "\n",
    "        if self.cuda_enabled:\n",
    "            hyperparam_codified = f\"DDQN_OOP_CUDA_{self.model_id}_{self.lr}_{self.buffer_size}_{self.target_update_freq}_{self.gamma}_{self.eps_decay}_{self.batch_size}\"\n",
    "            hyperparam_codified_time = f\"{timestamp}_DDQN_OOP_CUDA_{self.model_id}_{self.lr}_{self.buffer_size}_{self.target_update_freq}_{self.gamma}_{self.eps_decay}_{self.batch_size}\"\n",
    "        else:   \n",
    "            hyperparam_codified = f\"DDQN_OOP_nCUDA_{self.model_id}_{self.lr}_{self.buffer_size}_{self.target_update_freq}_{self.gamma}_{self.eps_decay}_{self.batch_size}\"\n",
    "            hyperparam_codified_time = f\"{timestamp}_DDQN_OOP_nCUDA_{self.model_id}_{self.lr}_{self.buffer_size}_{self.target_update_freq}_{self.gamma}_{self.eps_decay}_{self.batch_size}\"\n",
    "        data[name_codified] = hyperparam_codified_time\n",
    "\n",
    "        with open(trial_to_param_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "        # Store the training configs in JSON file\n",
    "        training_params = {\n",
    "            'OOP':                  True,\n",
    "            'CUDA':                 self.cuda_enabled,\n",
    "            'model_id':             self.model_id,\n",
    "            'lr':                   self.lr,\n",
    "            'gamma':                self.gamma,\n",
    "            'epsilon_decay':        self.eps_decay,\n",
    "            'batch_size':           self.batch_size,\n",
    "            'buffer_size':          self.buffer_size,\n",
    "            'target_update_freq':   self.target_update_freq\n",
    "        }\n",
    "\n",
    "        # Store training parameters in each run \n",
    "        param_path = os.path.join(OUTPUT_DIR, \"param_config.json\")\n",
    "        with open(param_path, \"w\") as f:\n",
    "            json.dump({\"parameters\": training_params}, f, indent=2)\n",
    "\n",
    "        return name_codified, hyperparam_codified, OUTPUT_DIR\n",
    "    \n",
    "    def eps_greedy_policy(self, env, obs, epsilon):      \n",
    "        ''' Function to take an action according to an epsilon-greedy policy and a Q-network \n",
    "        \n",
    "        Parameters:\n",
    "        ------------\n",
    "\n",
    "        env : gym.Env\n",
    "            the environment that the agent is taking a step in (either train/val/test env with seed or no seed)\n",
    "        obs : \n",
    "            the current observation from the environment\n",
    "        epsilon : float\n",
    "            the current exploration rate\n",
    "            \n",
    "        Return:\n",
    "        ------------\n",
    "\n",
    "        action\n",
    "            the next action that the agent takes\n",
    "        '''\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                if self.cuda_enabled:\n",
    "                    state_tensor = torch.tensor(obs, dtype=torch.float32, device='cuda').unsqueeze(0)\n",
    "                else:\n",
    "                    state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
    "                q_values = self.Q_net(state_tensor)\n",
    "                action = q_values.argmax().item()\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def prefill_replay(self):\n",
    "        obs,_ = self.env.reset()\n",
    "        for _ in range(self.min_replay_size):\n",
    "            action = self.env.action_space.sample()\n",
    "            next_obs, reward, term, trunc, _ = self.env.step(action)\n",
    "            done = term or trunc\n",
    "\n",
    "            self.replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "            obs = next_obs if not done else self.env.reset()[0]\n",
    "\n",
    "    def policy_eval(self, env: gym.Env, n_episode_test = 500, verbose = 0):\n",
    "        ''' Assess the average reward when following a q_network in a test environment with random state initialization\n",
    "        \n",
    "        Parameters:\n",
    "        env : gymnasium environment\n",
    "            - Can be either the self.env_test or self.env_val environment\n",
    "\n",
    "        '''\n",
    "\n",
    "        total_reward = 0\n",
    "        for i in range(n_episode_test):\n",
    "            obs,_ = env.reset()\n",
    "            done = False\n",
    "            eps_reward = 0\n",
    "\n",
    "            while not done:                 # Step thorugh the episode deterministically (no exploration)\n",
    "                action = self.eps_greedy_policy(env, obs, epsilon=0)\n",
    "                next_obs, reward, term, trunc, _ = env.step(action)\n",
    "\n",
    "                eps_reward += reward\n",
    "\n",
    "                obs = next_obs\n",
    "                done = term or trunc\n",
    "        \n",
    "            total_reward += eps_reward\n",
    "            if verbose:\n",
    "                    print(f\"Validation episode {i+1:3d}/{n_episode_test}  |   Reward = {eps_reward:4.0f}\",end=\"\\r\")\n",
    "        average_reward = total_reward / n_episode_test\n",
    "        return average_reward\n",
    "    \n",
    "    def EMA_filter(self, reward: list, alpha):\n",
    "        ''' Function that runs an exponential moving average filter along a datastream '''\n",
    "        output = np.zeros(len(reward)+1)\n",
    "        output[0] = reward[0]\n",
    "        for idx, item in enumerate(reward):\n",
    "            output[idx+1] = (1 - alpha) * output[idx] + alpha * item\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def plot_reward_hist(self, alpha = 0.1):\n",
    "        ''' Function that plots the reward and filtered reward per episode, then saves the plot in a specified save directory'''\n",
    "        n_episodes= len(self.reward_history)\n",
    "        episodes = range(n_episodes)\n",
    "        filtered_reward_hist = self.EMA_filter(self.reward_history, alpha)\n",
    "\n",
    "        plt.figure(figsize=(20,6))\n",
    "        plt.plot(episodes, self.reward_history[:n_episodes], color = \"blue\")\n",
    "        plt.plot(episodes, filtered_reward_hist[:n_episodes], color = \"red\")\n",
    "        plt.title(f'Total reward per episode - {self.hyperparam_config}')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.legend(['Total reward','Filtered reward'])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if self.save_path:\n",
    "            plt.savefig(os.path.join(self.save_path,'reward_history.png'))\n",
    "        plt.show()\n",
    "    \n",
    "    def save_model(self):\n",
    "        ''' Function to save the model and optimizer state_dict for inference or continued training '''\n",
    "        self.model_path = os.path.join(self.save_path, 'q_network_checkpoint.pth')\n",
    "        torch.save({\n",
    "            'model_state_dict': self.Q_net.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, self.model_path)\n",
    "\n",
    "    def load_model(self):\n",
    "        ''' This code overwrite the Q_net with the parameters store in the instance's save_path '''\n",
    "        checkpoint = torch.load(self.model_path)\n",
    "        self.Q_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "    def DDQN_train(self):\n",
    "        ''' Function to train a policy for a set of hyperparameters '''\n",
    "\n",
    "        msg = \"Training ended, no good model found!\"\n",
    "\n",
    "        self.replay_buffer = deque(maxlen = self.buffer_size)\n",
    "        self.prefill_replay()\n",
    "        self.reward_history = np.zeros(self.episode_train)\n",
    "        self.eps = self.eps_start\n",
    "        step_count = 0\n",
    "        episode = 0\n",
    "        target_network_update_count = 0\n",
    "\n",
    "        # Control of early stopping\n",
    "        consecutive_pass_count = 0           # Number of consecutive episodes where performance exceeds a threshold\n",
    "        CONSECUTIVE_PASS_LIMIT = 3          # No. of consecutive episodes with higher performance than reward limit\n",
    "        EPISODE_REWARD_LIMIT = SUCCESS_CRITERIA\n",
    "        best_reward = 0\n",
    "        performance_crit = False\n",
    "        train_terminated = False\n",
    "\n",
    "        _, self.hyperparam_config, self.save_path = self.create_directory()\n",
    "        self.train_time_start = time.time()\n",
    "        while not train_terminated:     # Experiment level - loop through episodes\n",
    "            obs, _ = self.env.reset(seed=self.seed)\n",
    "            eps_rewards = 0\n",
    "        \n",
    "            while True:                 # Episode level - loop through steps\n",
    "                action = self.eps_greedy_policy(self.env, obs, epsilon = self.eps)\n",
    "\n",
    "                # Interact with the environment\n",
    "                next_obs, reward, term, trunc, _ = self.env.step(action)\n",
    "                done = term or trunc\n",
    "                self.replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "                obs = next_obs\n",
    "                eps_rewards += reward\n",
    "                step_count += 1\n",
    "\n",
    "                # Train the Q-net using a batch of samples from the experience replay\n",
    "                if len(self.replay_buffer) >= self.batch_size:\n",
    "                    batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "                    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                    states =        torch.tensor(np.array(states),      dtype = torch.float32,  device='cuda' if self.cuda_enabled else 'cpu')           # Shape of (BATCH_SIZE, 4). Data structure of [[p1 v1 theta1 dtheta1], [p2 v2 theta2 dtheta2], ...]\n",
    "                    actions =       torch.tensor(actions,               dtype = torch.long,     device='cuda' if self.cuda_enabled else 'cpu').unsqueeze(1)        # Must unsqueeze to have shape (BATCH_SIZE, 1). From [a1, a2, ...] to [[a1], [a2], ... [a_BATCH_SIZE]]\n",
    "                    rewards =       torch.tensor(rewards,               dtype = torch.float32,  device='cuda' if self.cuda_enabled else 'cpu').unsqueeze(1)\n",
    "                    next_states =   torch.tensor(np.array(next_states), dtype = torch.float32,  device='cuda' if self.cuda_enabled else 'cpu')\n",
    "                    dones =         torch.tensor(dones,                 dtype = torch.float32,  device='cuda' if self.cuda_enabled else 'cpu').unsqueeze(1)\n",
    "                    \n",
    "                    # Compute targets using target network Q(s',a',w_i^-)\n",
    "                    # TODO - Change this from DQN to DDQN code\n",
    "                    with torch.no_grad():\n",
    "                        # Select the maximizing action according to the online behaviour net\n",
    "                        optimal_next_actions_online = self.Q_net(next_states).argmax(dim=1,keepdim=True)\n",
    "                        \n",
    "                        # Find the target Q value of the maximizing action according to the target net\n",
    "                        target_q_values = self.target_net(next_states).gather(1,optimal_next_actions_online)\n",
    "                        \n",
    "                        targets = rewards + self.gamma * target_q_values * (1 - dones)       # (1-dones) to avoid over estimating the value of terminal states. The target should only be the reward in the terminal states   # (1-dones) to avoid over estimating the value of terminal states. The target should only be the reward in the terminal states\n",
    "                \n",
    "                    # Compute the current Q values for the actions taken Q(s,a,w_i)\n",
    "                    q_values = self.Q_net(states).gather(1, actions)         # obtain the q_values associated to the actual action taken in each sample\n",
    "\n",
    "                    # Update the parameters of the behaviour q_network\n",
    "                    loss = nn.MSELoss()(q_values, targets)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                # Periodically update the target network by loading the weights from the behavior network\n",
    "                if step_count % self.target_update_freq == 0:\n",
    "                    target_network_update_count += 1\n",
    "                    self.target_net.load_state_dict(self.Q_net.state_dict())\n",
    "\n",
    "                if done:        # End of a training episode\n",
    "                    break\n",
    "\n",
    "            # Decay epsilon after an episode\n",
    "            self.eps = max(self.eps_end, self.eps - (self.eps_start - self.eps_end)/self.eps_decay)\n",
    "\n",
    "            self.reward_history[episode] = eps_rewards\n",
    "            # print(f\"Episode {episode:5d}: Total reward = {eps_rewards:5.1f}, Epsilon = {epsilon:.3f}, Step count = {step_count:5d}, Target update count = {target_network_update_count:3d}\")\n",
    "            # print(f\"Average reward {sum(reward_history[:episode])/(episode+1):.2f}\")\n",
    "\n",
    "            \n",
    "            if episode % 10 == 0:                   # print progress periodically\n",
    "                print(f\"Episode {episode:5d}/{self.episode_train}: Total reward = {eps_rewards:5.1f}, Epsilon = {self.eps:.3f}\", end = \"\\r\")\n",
    "\n",
    "            # Early stopping condition\n",
    "            if eps_rewards >= EPISODE_REWARD_LIMIT:\n",
    "                test_reward = self.policy_eval(self.env_val, 100)       \n",
    "                self.val_history[episode] = [eps_rewards, test_reward]\n",
    "\n",
    "                if test_reward >= best_reward:           # Set the new best reward\n",
    "                    best_reward = test_reward\n",
    "                    self.save_model()\n",
    "                    self.best_model_episode = episode\n",
    "                    msg = f\"Training terminated due to episode limit, best model saved at episode {self.best_model_episode:5d}\"\n",
    "                if test_reward > EPISODE_REWARD_LIMIT: \n",
    "                    consecutive_pass_count += 1\n",
    "                else: consecutive_pass_count = 0\n",
    "            else:\n",
    "                consecutive_pass_count = 0\n",
    "                \n",
    "            # Performance criteria - if good results for several episodes => training performance satisfied and terminate early\n",
    "            if consecutive_pass_count >= CONSECUTIVE_PASS_LIMIT:\n",
    "                self.save_model()\n",
    "                self.best_model_episode = episode\n",
    "                performance_crit = True \n",
    "                msg = f\"Early termination at episode {episode:5d}, desired performance reached\"\n",
    "\n",
    "\n",
    "            episode += 1\n",
    "\n",
    "            # Checking for early training termination or truncation\n",
    "            train_terminated = (episode >= self.episode_train) or (performance_crit)\n",
    "        self.train_time = time.time() - self.train_time_start\n",
    "\n",
    "        print(f\"\\nRuntime - {self.train_time:5.2f}\")\n",
    "\n",
    "        print(msg)\n",
    "        return\n",
    "\n",
    "    def DDQN_record(self):\n",
    "        ''' Method to plot the reward history and store the data in the current run folder '''\n",
    "\n",
    "        # Load the best Q net parameter from the experiment\n",
    "        self.load_model()\n",
    "\n",
    "        # Average test reward of the resulting policy\n",
    "        self.env_test = gym.make('CartPole-v1')\n",
    "        print(\"\\nTesting the Q_net\")\n",
    "        average_reward = self.policy_eval(self.env_test, n_episode_test=500, verbose = 1)\n",
    "        print(f\"\\nTest average reward {average_reward:4.2f}\")\n",
    "\n",
    "        # Store the validation hisotry and average test reward in the param_config JSON file\n",
    "        param_path = os.path.join(self.save_path,'param_config.json')\n",
    "        if os.path.exists(param_path):\n",
    "            with open(param_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "        else:\n",
    "            data = {}\n",
    "        data['runtime'] = self.train_time\n",
    "        data['best_model_at'] = self.best_model_episode\n",
    "        data['val_history'] = self.val_history\n",
    "        data['test_result'] = average_reward\n",
    "\n",
    "        with open(param_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "        # Plot\n",
    "        self.plot_reward_hist()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70a0964",
   "metadata": {},
   "source": [
    "## DDQN Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b48396",
   "metadata": {},
   "source": [
    "Using itertools to loop through each combination of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fcce5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1/4 - model DQN_MLP_v0, lr=0.0001, buffer=5000, target_freq=1000, gamma=0.95, eps_decay=5000, batch_size=32\n",
      "Episode  1930/5000: Total reward =  97.0, Epsilon = 0.652\r"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import time\n",
    "keys, values = zip(*param_grid.items())\n",
    "# keys, values = param_grid.keys(), param_grid.values()\n",
    "num_config = len(list(itertools.product(*values)))\n",
    "\n",
    "# Set fixed seed\n",
    "seed = 42\n",
    "np.random.seed(seed); random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "for idx, v in enumerate(itertools.product(*values)):\n",
    "\n",
    "    # Unpacking the hyperparameter configurations\n",
    "    config = dict(zip(keys,v))\n",
    "    MODEL_NAME = config['MODEL']                            # Name of the model: \"DQN_MLP_v0\", \"DQN_MLP_v1\"\n",
    "    MODEL_CLASS = model_registry[MODEL_NAME]['class']       # The model class: QNet_MLP,...\n",
    "    MODEL_CONFIG = model_registry[MODEL_NAME]['config']     # The architecture of the model\n",
    "    match = re.search(r'v\\d+',MODEL_NAME)                   \n",
    "    MODEL_ID = match.group(0) if match else 404             # The model id: \"v0\", \"v1\"\n",
    "\n",
    "    LR = config['LR']\n",
    "    BUFFER_SIZE = config['BUFFER_SIZE']\n",
    "    MIN_REPLAY_SIZE = config['MIN_REPLAY_SIZE']\n",
    "    TARGET_UPDATE_FREQ = config['TARGET_UPDATE_FREQ']\n",
    "    GAMMA = config['GAMMA']\n",
    "    EPS_START = config['EPSILON_START']\n",
    "    EPS_END = config['EPSILON_END']\n",
    "    EPS_DECAY = config['EPSILON_DECAY']\n",
    "    EPISODE_TRAIN = config['EPISODE_TRAIN']\n",
    "    BATCH_SIZE = config['BATCH_SIZE']\n",
    "\n",
    "\n",
    "    experiment = DDQN_experiment(MODEL_NAME, model_registry, \n",
    "                                 LR, BUFFER_SIZE, TARGET_UPDATE_FREQ, GAMMA, \n",
    "                                 EPS_START, EPS_DECAY, EPS_END, \n",
    "                                 BATCH_SIZE, seed, cuda_enabled=CUDA_ENABLED)\n",
    "    \n",
    "    # # Training information\n",
    "    print(f'Trial {idx+1}/{num_config} - model {MODEL_NAME}, lr={LR}, buffer={BUFFER_SIZE}, target_freq={TARGET_UPDATE_FREQ}, gamma={GAMMA}, eps_decay={EPS_DECAY}, batch_size={BATCH_SIZE}')\n",
    "    experiment.DDQN_train()\n",
    "    experiment.DDQN_record()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6ccf53",
   "metadata": {},
   "source": [
    "## Load and Simulate Saved Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17800428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(q_network: nn.Module, model_path):\n",
    "    checkpoint = torch.load(model_path)\n",
    "    q_network.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c06f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy_policy(env, obs, epsilon: float, q_network: QNet_MLP):\n",
    "    ''' Function to take an action according to an epsilon-greedy policy and a Q-network'''\n",
    "    if np.random.random() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
    "            q_values = q_network(state_tensor)\n",
    "            action = q_values.argmax().item()\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1bfcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model after training\n",
    "\n",
    "# Manually select a folder/run to load\n",
    "run_number = 'run_00003'\n",
    "\n",
    "# Find the paths to the param_config and model checkpoint\n",
    "BASE_DIR = os.getcwd()\n",
    "RESULT_DIR = os.path.join(BASE_DIR,'cartpole_DDQN_results')\n",
    "RUN_DIR = os.path.join(RESULT_DIR, run_number)\n",
    "MODEL_PATH = os.path.join(RUN_DIR,'q_network_checkpoint.pth')\n",
    "\n",
    "# Find the model configuration\n",
    "with open(os.path.join(RUN_DIR,\"param_config.json\"),'r') as f:\n",
    "    data = json.load(f)\n",
    "model_id = 'DQN_MLP_' + data['parameters']['model_id']\n",
    "model_config = model_registry[model_id]['config']\n",
    "\n",
    "# Create a visual simulation of the network\n",
    "RENDER_MODE = \"human\"\n",
    "env_test_visual = gym.make(\"CartPole-v1\",render_mode=RENDER_MODE)\n",
    "obs_space = env_test_visual.observation_space.shape[0]\n",
    "act_space = env_test_visual.action_space.n\n",
    "num_test = 1\n",
    "\n",
    "# Create the model according to the model version in the model registry\n",
    "q_network_loaded = QNet_MLP(obs_space, act_space, model_config)\n",
    "load_model(q_network_loaded, MODEL_PATH)\n",
    "\n",
    "for episode in range(num_test):\n",
    "    obs, _ = env_test_visual.reset()\n",
    "    done = False\n",
    "    eps_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = eps_greedy_policy(env_test_visual, obs, epsilon = 0, q_network=q_network_loaded)\n",
    "        next_obs, reward, term, trunc, _ = env_test_visual.step(action)\n",
    "\n",
    "        done = term or trunc\n",
    "        eps_reward += reward\n",
    "        obs = next_obs\n",
    "    \n",
    "    print(f\"Reward from episode {episode:3d} is {eps_reward}\")\n",
    "\n",
    "env_test_visual.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca43fea3",
   "metadata": {},
   "source": [
    "# REINFORCE Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b418380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym             # To create the inverted pendulum environment\n",
    "import torch                        \n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import os\n",
    "import json                         # to save parameters and results\n",
    "import time                         # to monitor training & validation time\n",
    "from datetime import datetime       # to create a log of runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f7c33f",
   "metadata": {},
   "source": [
    "## Parameter & Class Definitions\n",
    "\n",
    "Define the hyperparameters of the experiment(s), the policy network and value network classes, and the REINFORCE experiment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "001541f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'MODEL': [\"Policy_MLP_v0\", \"Policy_MLP_v1\"],\n",
    "    'ALPHA' : [5e-4, 1e-3],\n",
    "    'BETA' : [1e-3, 5e-3],\n",
    "    'GAMMA': [0.95, 0.98],\n",
    "    'N_EPISODE_TRAIN': [5000]\n",
    "}\n",
    "\n",
    "N_EPISODE_TEST = 5\n",
    "CUDA_ENABLED = False\n",
    "SUCCESS_CRITERIA = 450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "415778a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_layers=[64,64]):\n",
    "        ''' Initialize the model and create a list of layers '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        for layer_size in hidden_layers:\n",
    "            self.layers.append(nn.Linear(input_dim, layer_size))\n",
    "            input_dim = layer_size\n",
    "\n",
    "        self.layers.append(nn.Linear(input_dim, self.output_dim))\n",
    "        if CUDA_ENABLED: self.cuda()\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers[:-1]:\n",
    "            input = torch.relu(layer(input))\n",
    "        logits = self.layers[-1](input)\n",
    "        probs = torch.softmax(logits,dim=-1)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa5305f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_layers=[32,32]):\n",
    "        ''' Initialize the model and create a list of layers '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        for layer_size in hidden_layers:\n",
    "            self.layers.append(nn.Linear(input_dim, layer_size))\n",
    "            input_dim = layer_size\n",
    "\n",
    "        self.layers.append(nn.Linear(input_dim, self.output_dim))\n",
    "        if CUDA_ENABLED: self.cuda()\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers[:-1]:\n",
    "            input = torch.relu(layer(input))\n",
    "        output = self.layers[-1](input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1aa8a32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_registry = {\n",
    "    \"Policy_MLP_v0\": {\n",
    "        'class': PolicyNet,\n",
    "        'config': [64,32]\n",
    "    },\n",
    "    'Policy_MLP_v1': {\n",
    "        'class': PolicyNet,\n",
    "        'config': [64,32],\n",
    "        'value_class': ValueNet,\n",
    "        'value_config': [32]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cba14a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE():\n",
    "    def __init__(self, model_name: str,\n",
    "                 model_registry,\n",
    "                 alpha: float,          # learning rate of the policy net\n",
    "                 beta: float,           # learning rate of the state value net\n",
    "                 gamma: float,          # discount rate in RL\n",
    "                 n_episode_train,\n",
    "                 result_folder = 'cartpole_REINFORCE_results',\n",
    "                 seed = 42,\n",
    "                 cuda_enabled = False,\n",
    "                 verbose = True):\n",
    "        \n",
    "        self.N_EPISODE_TRAIN = n_episode_train\n",
    "        self.result_folder = result_folder\n",
    "        self.SEED = seed\n",
    "        self.CUDA_ENABLED = cuda_enabled\n",
    "        self.VERBOSE = verbose\n",
    "        self.LOG_PERIOD = 50\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Initialize the train and validation environments\n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "        self.env_val = gym.make(\"CartPole-v1\")\n",
    "        self.obs_space = self.env.observation_space.shape[0]\n",
    "        self.act_space = self.env.action_space.n\n",
    "        \n",
    "        ''' Experiment hyperparameters '''\n",
    "        # Policy model configuration\n",
    "        self.model_name = model_name\n",
    "        self.model_class = model_registry[self.model_name]['class']\n",
    "        self.model_config = model_registry[self.model_name]['config']\n",
    "        match = re.search(r'v\\d+', self.model_name)\n",
    "        self.model_id = match.group(0) if match else 404\n",
    "\n",
    "        # Initialize the policy network\n",
    "        self.policy_net = self.model_class(self.obs_space, self.act_space, self.model_config)\n",
    "        self.policy_net.apply(self.init_weights)\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr = self.alpha)\n",
    "\n",
    "        # REINFORCE w/ baseline => Value model configuration (if value_class exists)\n",
    "        if 'value_class' in model_registry[self.model_name]:\n",
    "            if model_registry[self.model_name]['value_class']:\n",
    "                self.value_class = model_registry[self.model_name]['value_class']\n",
    "                self.value_config = model_registry[self.model_name]['value_config']\n",
    "                self.value_net = self.value_class(self.obs_space, 1, self.value_config)\n",
    "                self.value_net.apply(self.init_weights)\n",
    "                self.value_optimizer = optim.Adam(self.value_net.parameters(), lr = self.beta)\n",
    "                self.BASELINE = True\n",
    "        else: \n",
    "            self.BASELINE = False\n",
    "\n",
    "        self.save_path = ''\n",
    "        self.model_path = ''\n",
    "        self.hyperparam_config = ''\n",
    "        self.reward_history = []\n",
    "        self.val_history = {}                                           # Monitor which episode had a validation run, the train reward, and the validation (test) reward \n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def create_directory(self):\n",
    "        ''' Function that creates directory to save model state_dict, architecture, training configuration, and history\n",
    "\n",
    "        Parameters: \n",
    "        ------------\n",
    "        (hyperparameters for differentiating between different directory)\n",
    "        \n",
    "        lr : float\n",
    "            the learning rate to optimize the Q network\n",
    "        gamma : float \n",
    "            the discount rate in Q learning\n",
    "        \n",
    "        Returns\n",
    "        ------------\n",
    "        name_codified : str\n",
    "            the shortened name for the current experiment \n",
    "        hyperparameters_codified : str\n",
    "            the shortened string of hyperparameter configuration\n",
    "        OUTPUT_DIR : path\n",
    "            the directory to which the training results and model (state_dict and architecture) will be saved\n",
    "        '''\n",
    "        timestamp = datetime.now().strftime(\"%y%m%d_%H%M\")\n",
    "\n",
    "        BASE_DIR = os.getcwd()\n",
    "        RESULT_DIR = os.path.join(BASE_DIR, self.result_folder)\n",
    "        os.makedirs(RESULT_DIR, exist_ok=True)      # Create the directory if one does not already exist\n",
    "\n",
    "        # Find the trial # of the latest run\n",
    "        existing_runs = [d for d in os.listdir(RESULT_DIR) if os.path.isdir(os.path.join(RESULT_DIR,d))]\n",
    "        run_numbers = [int(re.search(r'run_(\\d{5})',d).group(1)) for d in existing_runs if re.match(r'run_\\d{5}',d)]\n",
    "        trial_number = max(run_numbers,default=-1)+1\n",
    "\n",
    "        # Create a folder for the run\n",
    "        name_codified = f\"run_{trial_number:05d}\"\n",
    "        OUTPUT_DIR = os.path.join(RESULT_DIR,name_codified)\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)      # Create the directory\n",
    "\n",
    "        # Append the mapping from run # to hyperparameter configuration in a JSON file inside RESULT_DIR\n",
    "        trial_to_param_path = os.path.join(RESULT_DIR,'trial_to_param.json')\n",
    "        if os.path.exists(trial_to_param_path):\n",
    "            with open(trial_to_param_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "        else:\n",
    "            data = {name_codified: \"\"}\n",
    "\n",
    "        hyperparam_codified = \"REINFORCE_\"\n",
    "        hyperparam_codified += \"baseline_\" if self.BASELINE else \"\"\n",
    "        hyperparam_codified += \"OOP_\"\n",
    "        hyperparam_codified += \"CUDA_\" if self.CUDA_ENABLED else \"nCUDA_\"\n",
    "        hyperparam_codified += f\"{self.model_id}_{self.alpha}_\"\n",
    "        hyperparam_codified += f\"{self.beta}_\" if self.BASELINE else \"\"\n",
    "        hyperparam_codified += f\"{self.gamma}\"\n",
    "\n",
    "        hyperparam_codified_time = f\"{timestamp}_\" + hyperparam_codified\n",
    "\n",
    "        data[name_codified] = hyperparam_codified_time\n",
    "\n",
    "        with open(trial_to_param_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "        # Store the training configs in JSON file\n",
    "        training_params = {\n",
    "            'OOP':                  True,\n",
    "            'CUDA':                 self.CUDA_ENABLED,\n",
    "            'device':               torch.cuda.get_device_name(torch.cuda.current_device()),\n",
    "            'model_id':             self.model_id,\n",
    "            'alpha':                self.alpha,\n",
    "            'gamma':                self.gamma,\n",
    "        }\n",
    "        if self.BASELINE: training_params['beta'] = self.beta\n",
    "\n",
    "        # Store training parameters in each run \n",
    "        param_path = os.path.join(OUTPUT_DIR, \"param_config.json\")\n",
    "        with open(param_path, \"w\") as f:\n",
    "            json.dump({\"parameters\": training_params}, f, indent=2)\n",
    "\n",
    "        return name_codified, hyperparam_codified, OUTPUT_DIR\n",
    "    \n",
    "    def get_returns(self, eps_reward_history):\n",
    "        ''' Function to calculate the return of each time step when given a list of rewards \n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        rewards : list\n",
    "            a list of rewards achieved throughout the agent's trajectory\n",
    "        gamma   : float\n",
    "            the discount factor\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        returns : list\n",
    "            a list of returns G_t at each step of the trajectory\n",
    "            \n",
    "        For each step of th trajectory (of length T):\n",
    "        - Extract the rewards from that step onward\n",
    "        - Each step is multiplied by the corresponding gamma ^ index \n",
    "            the first reward received from leaving the state is not discounted\n",
    "            the last reward received from the trajectory is discouned by gamma ^ (T-1)\n",
    "        - Sum these values together to obtain the return at each step\n",
    "        '''\n",
    "        returns = np.zeros(len(eps_reward_history))\n",
    "        gamma = self.gamma\n",
    "        \n",
    "        for step, _ in enumerate(eps_reward_history):          # step through the \"trajectory\" or history of reward\n",
    "            step_reward = eps_reward_history[step:]            # reward from the current step onward\n",
    "\n",
    "            # List of discounted rewards at each time step\n",
    "            return_val = [gamma ** i * step_reward[i] for i in range(len(step_reward))]\n",
    "            return_val = sum(return_val)\n",
    "            \n",
    "            returns[step] = return_val\n",
    "        return returns\n",
    "\n",
    "    def policy_eval(self, env: gym.Env, n_episode_test = 500, verbose = True):\n",
    "        ''' Assess the average reward when following the policy net in a test environment with random state initialization\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        env : gymnasium environment\n",
    "            this environment can be either the self.env_test or self.env_val environment (whether they are the same)\n",
    "        n_episode_test : int \n",
    "            the number of evaluation episodes\n",
    "        verbose : bool\n",
    "            whether to print testing information \n",
    "\n",
    "        Return:\n",
    "        ----------\n",
    "        average_reward : float\n",
    "            the average reward received from running the test\n",
    "        '''\n",
    "\n",
    "        total_reward = 0\n",
    "        with torch.no_grad():\n",
    "            for i in range(n_episode_test):\n",
    "                obs,_ = env.reset()\n",
    "                done = False\n",
    "                eps_reward = 0\n",
    "\n",
    "                while not done:                 # Step thorugh the episode deterministically (no exploration)\n",
    "                    obs_tensor = torch.tensor(obs, dtype = torch.float32, device = 'cuda' if self.CUDA_ENABLED else 'cpu')\n",
    "                    action_probs = self.policy_net(obs_tensor)\n",
    "                    action_dist = Categorical(action_probs)\n",
    "                    action = action_dist.sample()\n",
    "                    next_obs, reward, term, trunc, _ = env.step(action.item())\n",
    "\n",
    "                    # Strategy 1 - Accumulate the reward from the environment\n",
    "                    eps_reward += reward\n",
    "\n",
    "                    # TODO - Strategy 2 - evaluate the strategy based on states\n",
    "\n",
    "                    obs = next_obs\n",
    "                    done = term or trunc\n",
    "            \n",
    "                total_reward += eps_reward\n",
    "                if verbose:\n",
    "                    print(f\"Validation episode {i+1:3d}/{n_episode_test}  |   Reward = {eps_reward:4.0f}\",end=\"\\r\")\n",
    "        average_reward = total_reward / n_episode_test\n",
    "        \n",
    "        return average_reward\n",
    "    \n",
    "    def EMA_filter(self, reward: list, alpha):\n",
    "        ''' Function that runs an exponential moving average filter along a datastream '''\n",
    "        output = np.zeros(len(reward)+1)\n",
    "        output[0] = reward[0]\n",
    "        for idx, item in enumerate(reward):\n",
    "            output[idx+1] = (1 - alpha) * output[idx] + alpha * item\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def plot_reward_hist(self, alpha = 0.1):\n",
    "        ''' Function that plots the reward and filtered reward per episode, then saves the plot in a specified save directory'''\n",
    "        n_episodes= len(self.reward_history)\n",
    "        episodes = range(n_episodes)\n",
    "        filtered_reward_hist = self.EMA_filter(self.reward_history, alpha)\n",
    "        fig, axes = plt.subplots(2,1, figsize=(20,12))\n",
    "        axes[0].plot(episodes, self.reward_history[:n_episodes], label = 'Total reward', color = \"blue\")\n",
    "        axes[0].plot(episodes, filtered_reward_hist[:n_episodes], label = 'Filtered reward', color = \"red\")\n",
    "        axes[0].set_title(f'Total reward per episode - {self.hyperparam_config}')\n",
    "        axes[0].set_xlabel('Episode')\n",
    "        axes[0].set_ylabel('Reward')\n",
    "        axes[0].legend()\n",
    "\n",
    "        n_episodes= len(self.value_history)\n",
    "        episodes = range(n_episodes)\n",
    "        axes[1].plot(episodes, self.value_history[:n_episodes], label = \"v(s_0)\", color = \"blue\")\n",
    "        # If using baseline with a value estimation model, plot this as well\n",
    "        if self.BASELINE: axes[1].plot(episodes, self.value_est_history[:n_episodes], label = r\"$\\hat v$(s_0)\", color = \"red\")\n",
    "        axes[1].set_title(f'Return in state s_0 - {self.hyperparam_config}')\n",
    "        axes[1].set_xlabel('Episode')\n",
    "        axes[1].set_ylabel('v(s_0)')\n",
    "        axes[1].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if self.save_path:\n",
    "            plt.savefig(os.path.join(self.save_path,'reward_history.png'))\n",
    "        plt.show()\n",
    "\n",
    "    def save_model(self):\n",
    "        ''' Function to save the model and optimizer state_dict for inference or continued training '''\n",
    "        self.model_path = os.path.join(self.save_path, 'q_network_checkpoint.pth')\n",
    "        torch.save({\n",
    "            'model_state_dict': self.policy_net.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, self.model_path)\n",
    "\n",
    "    def load_model(self):\n",
    "        ''' This code overwrite the Q_net with the parameters store in the instance's save_path '''\n",
    "        checkpoint = torch.load(self.model_path)\n",
    "        self.policy_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    def train_policy(self):\n",
    "        msg = \"Training ended with no good model found :<\"\n",
    "\n",
    "        # Create the directory to store results\n",
    "        self.run_number, self.hyperparam_config, self.save_path = self.create_directory()\n",
    "\n",
    "        # Training information\n",
    "        if self.BASELINE: title = f\"REINFORCE baseline model {self.model_name}, alpha={self.alpha}, beta={self.beta}, gamma={self.gamma}\" \n",
    "        else: title = f\"REINFORCE w/o baseline model {self.model_name}, alpha={self.alpha}, gamma={self.gamma}\" \n",
    "        if self.VERBOSE: print(title)\n",
    "\n",
    "        self.reward_history = []                # Track the total reward per episode\n",
    "        self.val_history = {}                   # Reset the validation history\n",
    "        self.policy_loss_history = []           # History of loss throughout training\n",
    "        self.value_loss_history = []\n",
    "        self.value_history = []                 # History of the v(s_0) calculated using G_t throughout training\n",
    "        if self.BASELINE: self.value_est_history = []             # History of the \\hat v(s_0) calculated using the value net (baseline case)\n",
    "        self.val_time = 0                       # Time used for validation (s)\n",
    "        episode = 0\n",
    "\n",
    "        # Control of early stopping\n",
    "        consecutive_pass_count = 0\n",
    "        CONSECUTIVE_PASS_LIMIT = 3\n",
    "        EPISODE_REWARD_LIMIT = SUCCESS_CRITERIA\n",
    "        best_reward = 0\n",
    "        self.best_model_episode = None\n",
    "        performance_crit = False                # Whether desired performance is met consistently\n",
    "        train_terminated = False\n",
    "        \n",
    "\n",
    "        self.train_time_start = time.time()\n",
    "        while not train_terminated:             # Experiment level - loop through episodes\n",
    "            obs, _ = self.env.reset(seed=self.SEED)\n",
    "            obs_tensor = torch.tensor(obs, dtype = torch.float32, device = 'cuda' if self.CUDA_ENABLED else 'cpu')\n",
    "            done = False\n",
    "            step = 0\n",
    "            eps_reward_history = []\n",
    "            log_prob_history = []\n",
    "\n",
    "            if self.BASELINE:\n",
    "                obs_history = []\n",
    "                obs_history.append(obs_tensor)\n",
    "                \n",
    "\n",
    "            while not done:                     # Episode level - loop through steps in an episode\n",
    "                step += 1\n",
    "                action_probs = self.policy_net(obs_tensor)\n",
    "                action_dist = Categorical(action_probs)\n",
    "                action = action_dist.sample()\n",
    "\n",
    "                next_obs, reward, term, trunc, _ = self.env.step(action.item())\n",
    "\n",
    "                if self.BASELINE: obs_history.append(obs_tensor)\n",
    "                eps_reward_history.append(reward)\n",
    "                log_prob_history.append(action_dist.log_prob(action))\n",
    "                \n",
    "                obs_tensor = torch.tensor(next_obs, dtype = torch.float32, device = 'cuda' if self.CUDA_ENABLED else 'cpu')\n",
    "                done = term or trunc\n",
    "            \n",
    "            # Post episode calculations - returns, total episode reward, and loss\n",
    "            returns = self.get_returns(eps_reward_history)\n",
    "            returns = torch.tensor(returns, dtype = torch.float32, device='cuda' if self.CUDA_ENABLED else 'cpu')\n",
    "\n",
    "            if self.BASELINE: \n",
    "                obs_history_tensor = torch.stack(obs_history)\n",
    "                # print(obs_history_tensor.shape)\n",
    "                baseline_values = self.value_net(obs_history_tensor)[:-1]\n",
    "                delta = returns.unsqueeze(dim=1) - baseline_values.detach()\n",
    "                # Find loss of the value function and update parameters\n",
    "                value_loss = -torch.sum(baseline_values * delta)\n",
    "                self.value_optimizer.zero_grad()\n",
    "                value_loss.backward()\n",
    "                self.value_optimizer.step()\n",
    "\n",
    "                # Find loss of the policy network and update parameters\n",
    "                policy_loss = - torch.sum(torch.stack(log_prob_history) * delta)\n",
    "                self.optimizer.zero_grad()\n",
    "                policy_loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            else:\n",
    "                policy_loss = - torch.sum(torch.stack(log_prob_history) * returns)\n",
    "                self.optimizer.zero_grad()\n",
    "                policy_loss.backward()\n",
    "                self.optimizer.step()\n",
    "            \n",
    "            eps_reward = sum(eps_reward_history)\n",
    "        \n",
    "\n",
    "            self.reward_history.append(eps_reward)     # Total reward of episode\n",
    "            self.policy_loss_history.append(policy_loss)\n",
    "            self.value_history.append(returns[0])\n",
    "            if self.BASELINE:\n",
    "                with torch.no_grad():   # Estimate and store \\hat v(s_0)\n",
    "                    self.value_est_history.append(self.value_net(obs_history[0]).item())\n",
    "\n",
    "            # Optimize the network parameters\n",
    "            \n",
    "            # Periodic data logger\n",
    "            \n",
    "            if episode % self.LOG_PERIOD == 0 and self.VERBOSE:\n",
    "                printout_msg = f\"Episode {episode:5d}/{self.N_EPISODE_TRAIN}: Total reward = {eps_reward:5.1f}   |   G_0 = {returns[0]: 5.2f}\"\n",
    "                if self.BASELINE: printout_msg += fr\"   |   $\\hat v(s_0)$ = {self.value_est_history[-1]:6.2f}\" \n",
    "                print(printout_msg, end='\\r')\n",
    "\n",
    "            # Early stopping condition\n",
    "            if eps_reward >= EPISODE_REWARD_LIMIT:\n",
    "                # Evaluate the current good policy and record the validation time\n",
    "                self.val_time_start = time.time()\n",
    "                test_reward = self.policy_eval(self.env_val, 20,verbose=False)\n",
    "                self.val_time += time.time() - self.val_time_start\n",
    "\n",
    "                self.val_history[episode] = [eps_reward, test_reward]\n",
    "\n",
    "                if test_reward >= best_reward:           # Set the new best reward\n",
    "                    best_reward = test_reward\n",
    "                     \n",
    "                    self.best_model_episode = episode\n",
    "                    msg = f\"Training terminated due to episode limit, best model saved at episode {self.best_model_episode:5d}\"\n",
    "                if test_reward > EPISODE_REWARD_LIMIT: \n",
    "                    consecutive_pass_count += 1\n",
    "                else: consecutive_pass_count = 0\n",
    "            else:\n",
    "                consecutive_pass_count = 0\n",
    "\n",
    "            # Performance criteria - if good results for several episodes => training performance satisfied and terminate early\n",
    "            if consecutive_pass_count >= CONSECUTIVE_PASS_LIMIT:\n",
    "                self.save_model()\n",
    "                self.best_model_episode = episode\n",
    "                performance_crit = True \n",
    "                msg = f\"Early termination at episode {self.best_model_episode:5d}, desired performance reached\"\n",
    "\n",
    "\n",
    "            episode += 1\n",
    "\n",
    "            # Checking for early training termination or truncation\n",
    "            train_terminated = (episode >= self.N_EPISODE_TRAIN) or (performance_crit)\n",
    "\n",
    "        self.train_time = time.time() - self.train_time_start\n",
    "\n",
    "        print(f\"\\nTotal runtime - {self.train_time:5.2f}\")\n",
    "        print(msg)\n",
    "        return\n",
    "\n",
    "    def record(self):\n",
    "        # Load the best policy net parameter from the experiment\n",
    "        if self.model_path:\n",
    "            self.load_model()\n",
    "\n",
    "        # Average test reward of the resulting policy - if best policy does not exists use the last one\n",
    "        self.env_test = gym.make('CartPole-v1')\n",
    "        print('Testing the best policy network performance')\n",
    "        average_reward = self.policy_eval(self.env_test, n_episode_test=500, verbose=True)\n",
    "        print(f\"\\nValidation average reward {average_reward:4.2f}\")\n",
    "\n",
    "        # Store the validation history and average test reward in the param_config JSON file\n",
    "        param_path = os.path.join(self.save_path, 'param_config.json')\n",
    "        if os.path.exists(param_path):\n",
    "            with open(param_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "        else:\n",
    "            data = {}\n",
    "\n",
    "        data['runtime'] = self.train_time\n",
    "        data['valtime'] = self.val_time\n",
    "        data['best_model_at'] = self.best_model_episode\n",
    "        data['val_history'] = self.val_history\n",
    "        data['test_result'] = average_reward\n",
    "\n",
    "        with open(param_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "        # Plot\n",
    "        self.plot_reward_hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746d2792",
   "metadata": {},
   "source": [
    "## REINFORCE Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1436ee60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REINFORCE w/o baseline model Policy_MLP_v0, alpha=0.0005, gamma=0.95\n",
      "Episode   800/5000: Total reward = 285.0   |   G_0 =  20.00\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 31\u001b[0m\n\u001b[0;32m     20\u001b[0m N_EPISODE_TRAIN \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN_EPISODE_TRAIN\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     22\u001b[0m REINFORCE_experiment \u001b[38;5;241m=\u001b[39m REINFORCE(MODEL_NAME, \n\u001b[0;32m     23\u001b[0m                                 model_registry,\n\u001b[0;32m     24\u001b[0m                                 alpha \u001b[38;5;241m=\u001b[39m ALPHA,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m                                 cuda_enabled\u001b[38;5;241m=\u001b[39mCUDA_ENABLED,\n\u001b[0;32m     30\u001b[0m                                 verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 31\u001b[0m \u001b[43mREINFORCE_experiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m REINFORCE_experiment\u001b[38;5;241m.\u001b[39mrecord()\n",
      "Cell \u001b[1;32mIn[25], line 330\u001b[0m, in \u001b[0;36mREINFORCE.train_policy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBASELINE: obs_history\u001b[38;5;241m.\u001b[39mappend(obs_tensor)\n\u001b[0;32m    329\u001b[0m eps_reward_history\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[1;32m--> 330\u001b[0m log_prob_history\u001b[38;5;241m.\u001b[39mappend(\u001b[43maction_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    332\u001b[0m obs_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(next_obs, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32, device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCUDA_ENABLED \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    333\u001b[0m done \u001b[38;5;241m=\u001b[39m term \u001b[38;5;129;01mor\u001b[39;00m trunc\n",
      "File \u001b[1;32mc:\\Users\\caomi\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\distributions\\categorical.py:140\u001b[0m, in \u001b[0;36mCategorical.log_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_args:\n\u001b[1;32m--> 140\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    142\u001b[0m     value, log_pmf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits)\n",
      "File \u001b[1;32mc:\\Users\\caomi\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\distributions\\distribution.py:315\u001b[0m, in \u001b[0;36mDistribution._validate_sample\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m support \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 315\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[43msupport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_is_all_true(valid):\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected value argument \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    323\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "keys, values = zip(*param_grid.items())\n",
    "num_config = len(list(itertools.product(*values)))\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed); random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "for idx, v in enumerate(itertools.product(*values)):\n",
    "    # Unpack the hyperparameters configurations\n",
    "    config = dict(zip(keys,v))\n",
    "    MODEL_NAME = config['MODEL']\n",
    "    MODEL_CLASS = model_registry[MODEL_NAME]['class']\n",
    "    MODEL_CONFIG = model_registry[MODEL_NAME]['config']\n",
    "    match = re.search(r'v\\d+',MODEL_NAME)\n",
    "    MODEL_ID = match.group(0) if match else 404\n",
    "\n",
    "    ALPHA = config['ALPHA']\n",
    "    BETA = config['BETA']\n",
    "    GAMMA = config['GAMMA']\n",
    "    N_EPISODE_TRAIN = config['N_EPISODE_TRAIN']\n",
    "    \n",
    "    REINFORCE_experiment = REINFORCE(MODEL_NAME, \n",
    "                                    model_registry,\n",
    "                                    alpha = ALPHA,\n",
    "                                    beta = BETA,\n",
    "                                    gamma = GAMMA,\n",
    "                                    n_episode_train = N_EPISODE_TRAIN,\n",
    "                                    seed=42,\n",
    "                                    cuda_enabled=CUDA_ENABLED,\n",
    "                                    verbose=True)\n",
    "    REINFORCE_experiment.train_policy()\n",
    "    REINFORCE_experiment.record()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd5a587",
   "metadata": {},
   "source": [
    "## Load and Simulate a Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbe48fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(q_network: nn.Module, model_path):\n",
    "    checkpoint = torch.load(model_path)\n",
    "    q_network.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e0073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manually select a folder/run to load\n",
    "run_number = 'run_00012'\n",
    "\n",
    "## Find the paths to the param_config and model checkpoint\n",
    "# RESULT_DIR = os.path.dirname(REINFORCE_experiment.save_path)    # Can run this after running one experiment\n",
    "BASE_DIR = os.getcwd()\n",
    "RESULT_DIR = os.path.join(BASE_DIR,'cartpole_REINFORCE_results')\n",
    "RUN_DIR = os.path.join(RESULT_DIR, run_number)\n",
    "MODEL_PATH = os.path.join(RUN_DIR,'q_network_checkpoint.pth')\n",
    "\n",
    "## Find the model configuration\n",
    "with open(os.path.join(RUN_DIR,\"param_config.json\"),'r') as f:\n",
    "    data = json.load(f)\n",
    "model_id = 'Policy_MLP_' + data['parameters']['model_id']\n",
    "model_config = model_registry[model_id]['config']\n",
    "\n",
    "# Create a visual simulation of the network\n",
    "RENDER_MODE = \"human\"\n",
    "env_test_visual = gym.make(\"CartPole-v1\",render_mode=RENDER_MODE)\n",
    "obs_space = env_test_visual.observation_space.shape[0]\n",
    "act_space = env_test_visual.action_space.n\n",
    "num_test = 1\n",
    "\n",
    "# Create the model according to the model version in the model registry\n",
    "policy_net_loaded = PolicyNet(obs_space, act_space, model_config)\n",
    "load_model(policy_net_loaded, MODEL_PATH)\n",
    "\n",
    "for episode in range(num_test):\n",
    "    obs, _ = env_test_visual.reset()\n",
    "    done = False\n",
    "    eps_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        obs_tensor = torch.tensor(obs, dtype = torch.float32, device = 'cuda' if CUDA_ENABLED else 'cpu')\n",
    "        action_probs = policy_net_loaded(obs_tensor)\n",
    "        action_dist = Categorical(action_probs)\n",
    "        action = action_dist.sample()\n",
    "        next_obs, reward, term, trunc, _ = env_test_visual.step(action.item())\n",
    "\n",
    "        done = term or trunc\n",
    "        eps_reward += reward\n",
    "        obs = next_obs\n",
    "    \n",
    "    print(f\"Reward from episode {episode:3d} is {eps_reward}\")\n",
    "\n",
    "env_test_visual.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc35fdd",
   "metadata": {},
   "source": [
    "# Actor Critic Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296056d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym             # To create the inverted pendulum environment\n",
    "import torch                        \n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "from statistics import mean, stdev\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import os\n",
    "import json                         # to save parameters and results\n",
    "import time                         # to monitor training & validation time\n",
    "from datetime import datetime       # to create a log of runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5498b4b",
   "metadata": {},
   "source": [
    "## Parameter & Class Definition\n",
    "Define the hyperparameters of the experiment(s), the policy network and value network classes, and the REINFORCE experiment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e3ddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_layers=[64,64]):\n",
    "        ''' Initialize the model and create a list of layers '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        for layer_size in hidden_layers:\n",
    "            self.layers.append(nn.Linear(input_dim, layer_size))\n",
    "            input_dim = layer_size\n",
    "\n",
    "        self.layers.append(nn.Linear(input_dim, self.output_dim))\n",
    "        if CUDA_ENABLED: self.cuda()\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers[:-1]:\n",
    "            input = torch.relu(layer(input))\n",
    "        logits = self.layers[-1](input)\n",
    "        probs = torch.softmax(logits,dim=-1)\n",
    "        return probs\n",
    "    \n",
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_layers=[32,32]):\n",
    "        ''' Initialize the model and create a list of layers '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        for layer_size in hidden_layers:\n",
    "            self.layers.append(nn.Linear(input_dim, layer_size))\n",
    "            input_dim = layer_size\n",
    "\n",
    "        self.layers.append(nn.Linear(input_dim, self.output_dim))\n",
    "        if CUDA_ENABLED: self.cuda()\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers[:-1]:\n",
    "            input = torch.relu(layer(input))\n",
    "        output = self.layers[-1](input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29de91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_registry = {\n",
    "    'AC_MLP_v0': {\n",
    "        'class': PolicyNet,\n",
    "        'config': [64,64],\n",
    "        'value_class': ValueNet,\n",
    "        'value_config': [64,64]\n",
    "    },\n",
    "    'AC_MLP_v1': {\n",
    "        'class': PolicyNet,\n",
    "        'config': [128],\n",
    "        'value_class': ValueNet,\n",
    "        'value_config': [128]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c52fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'MODEL': [\"AC_MLP_v0\"],\n",
    "    'ALPHA' : [5e-5, 1e-4, 5e-4, 1e-3],\n",
    "    'BETA' : [1e-3],\n",
    "    'GAMMA': [0.98],\n",
    "    'N_EPISODE_TRAIN': [5000]\n",
    "}\n",
    "\n",
    "N_EPISODE_TEST = 5\n",
    "CUDA_ENABLED = False\n",
    "SUCCESS_CRITERIA = 450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3a9a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic():\n",
    "    def __init__(self, model_name: str,\n",
    "                 model_registry,\n",
    "                 alpha: float,          # learning rate of the policy net\n",
    "                 beta: float,           # learning rate of the state value net\n",
    "                 gamma: float,          # discount rate in RL\n",
    "                 n_episode_train,\n",
    "                 result_folder = 'cartpole_AC_results',\n",
    "                 seed = 42,\n",
    "                 cuda_enabled = False,\n",
    "                 verbose = True):\n",
    "        \n",
    "        self.N_EPISODE_TRAIN = n_episode_train\n",
    "        self.result_folder = result_folder\n",
    "        self.SEED = seed\n",
    "        self.CUDA_ENABLED = cuda_enabled\n",
    "        self.VERBOSE = verbose\n",
    "        self.LOG_PERIOD = 50\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Initialize the train and validation environments\n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "        self.env_val = gym.make(\"CartPole-v1\")\n",
    "        self.obs_space = self.env.observation_space.shape[0]\n",
    "        self.act_space = self.env.action_space.n\n",
    "        \n",
    "        ''' Experiment hyperparameters '''\n",
    "        # Policy model configuration\n",
    "        self.model_name = model_name\n",
    "        match = re.search(r'v\\d+', self.model_name)\n",
    "        self.model_id = match.group(0) if match else 404\n",
    "\n",
    "        self.policy_model_class = model_registry[self.model_name]['class']\n",
    "        self.policy_model_config = model_registry[self.model_name]['config']\n",
    "        self.value_model_class = model_registry[self.model_name]['value_class']\n",
    "        self.value_model_config = model_registry[self.model_name]['value_config']\n",
    "\n",
    "        # Instantiate and initialize the policy network\n",
    "        self.policy_net = self.policy_model_class(self.obs_space, self.act_space, self.policy_model_config)\n",
    "        self.policy_net.apply(self.init_weights)\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr = self.alpha)\n",
    "        \n",
    "        # Instantiate and initialize the state value network\n",
    "        self.value_net = self.value_model_class(self.obs_space, 1, self.value_model_config)\n",
    "        self.value_net.apply(self.init_weights)\n",
    "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr = self.beta)\n",
    "\n",
    "        self.save_path = ''\n",
    "        self.model_path = ''\n",
    "        self.hyperparam_config = ''\n",
    "        self.reward_history = []\n",
    "        self.val_history = {}                                           # Monitor which episode had a validation run, the train reward, and the validation (test) reward \n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def create_directory(self):\n",
    "        ''' Function that creates directory to save model state_dict, architecture, training configuration, and history\n",
    "\n",
    "        Parameters: \n",
    "        ------------\n",
    "        (hyperparameters for differentiating between different directory)\n",
    "        \n",
    "        lr : float\n",
    "            the learning rate to optimize the Q network\n",
    "        gamma : float \n",
    "            the discount rate in Q learning\n",
    "        \n",
    "        Returns\n",
    "        ------------\n",
    "        name_codified : str\n",
    "            the shortened name for the current experiment \n",
    "        hyperparameters_codified : str\n",
    "            the shortened string of hyperparameter configuration\n",
    "        OUTPUT_DIR : path\n",
    "            the directory to which the training results and model (state_dict and architecture) will be saved\n",
    "        '''\n",
    "        timestamp = datetime.now().strftime(\"%y%m%d_%H%M\")\n",
    "\n",
    "        BASE_DIR = os.getcwd()\n",
    "        RESULT_DIR = os.path.join(BASE_DIR, \"cartpole_reuslts\", self.result_folder)\n",
    "        os.makedirs(RESULT_DIR, exist_ok=True)      # Create the directory if one does not already exist\n",
    "\n",
    "        # Find the trial # of the latest run\n",
    "        existing_runs = [d for d in os.listdir(RESULT_DIR) if os.path.isdir(os.path.join(RESULT_DIR,d))]\n",
    "        run_numbers = [int(re.search(r'run_(\\d{5})',d).group(1)) for d in existing_runs if re.match(r'run_\\d{5}',d)]\n",
    "        trial_number = max(run_numbers,default=-1)+1\n",
    "\n",
    "        # Create a folder for the run\n",
    "        name_codified = f\"run_{trial_number:05d}\"\n",
    "        OUTPUT_DIR = os.path.join(RESULT_DIR,name_codified)\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)      # Create the directory\n",
    "\n",
    "        # Append the mapping from run # to hyperparameter configuration in a JSON file inside RESULT_DIR\n",
    "        trial_to_param_path = os.path.join(RESULT_DIR,'trial_to_param.json')\n",
    "        if os.path.exists(trial_to_param_path):\n",
    "            with open(trial_to_param_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "        else:\n",
    "            data = {name_codified: \"\"}\n",
    "\n",
    "        hyperparam_codified = \"AC_\"\n",
    "        hyperparam_codified += \"OOP_\"\n",
    "        hyperparam_codified += \"CUDA_\" if self.CUDA_ENABLED else \"nCUDA_\"\n",
    "        hyperparam_codified += f\"{self.model_id}_{self.alpha}_{self.beta}_{self.gamma}\"\n",
    "        hyperparam_codified_time = f\"{timestamp}_\" + hyperparam_codified\n",
    "\n",
    "        data[name_codified] = hyperparam_codified_time\n",
    "\n",
    "        with open(trial_to_param_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "        # Store the training configs in JSON file\n",
    "        training_params = {\n",
    "            'OOP':                  True,\n",
    "            'CUDA':                 self.CUDA_ENABLED,\n",
    "            'device':               torch.cuda.get_device_name(torch.cuda.current_device()),\n",
    "            'model_name':           self.model_name,\n",
    "            'alpha':                self.alpha,\n",
    "            'beta':                 self.beta,\n",
    "            'gamma':                self.gamma,\n",
    "        }\n",
    "\n",
    "        # Store training parameters in each run \n",
    "        param_path = os.path.join(OUTPUT_DIR, \"param_config.json\")\n",
    "        with open(param_path, \"w\") as f:\n",
    "            json.dump({\"parameters\": training_params}, f, indent=2)\n",
    "\n",
    "        return name_codified, hyperparam_codified, OUTPUT_DIR\n",
    "    \n",
    "    def get_returns(self, eps_reward_history):\n",
    "        ''' Function to calculate the return of each time step when given a list of rewards \n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        rewards : list\n",
    "            a list of rewards achieved throughout the agent's trajectory\n",
    "        gamma   : float\n",
    "            the discount factor\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        returns : list\n",
    "            a list of returns G_t at each step of the trajectory\n",
    "            \n",
    "        For each step of th trajectory (of length T):\n",
    "        - Extract the rewards from that step onward\n",
    "        - Each step is multiplied by the corresponding gamma ^ index \n",
    "            the first reward received from leaving the state is not discounted\n",
    "            the last reward received from the trajectory is discouned by gamma ^ (T-1)\n",
    "        - Sum these values together to obtain the return at each step\n",
    "        '''\n",
    "        returns = np.zeros(len(eps_reward_history))\n",
    "        gamma = self.gamma\n",
    "        \n",
    "        for step, _ in enumerate(eps_reward_history):          # step through the \"trajectory\" or history of reward\n",
    "            step_reward = eps_reward_history[step:]            # reward from the current step onward\n",
    "\n",
    "            # List of discounted rewards at each time step\n",
    "            return_val = [gamma ** i * step_reward[i] for i in range(len(step_reward))]\n",
    "            return_val = sum(return_val)\n",
    "            \n",
    "            returns[step] = return_val\n",
    "        return returns\n",
    "\n",
    "    def policy_eval(self, env: gym.Env, n_episode_test = 500, verbose = True):\n",
    "        ''' Assess the average reward when following the policy net in a test environment with random state initialization\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        env : gymnasium environment\n",
    "            this environment can be either the self.env_test or self.env_val environment (whether they are the same)\n",
    "        n_episode_test : int \n",
    "            the number of evaluation episodes\n",
    "        verbose : bool\n",
    "            whether to print testing information \n",
    "\n",
    "        Return:\n",
    "        ----------\n",
    "        average_reward : float\n",
    "            the average reward received from running the test\n",
    "        '''\n",
    "\n",
    "        reward_history = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(n_episode_test):\n",
    "                obs,_ = env.reset()\n",
    "                done = False\n",
    "                eps_reward = 0\n",
    "\n",
    "                while not done:                 # Step thorugh the episode deterministically (no exploration)\n",
    "                    obs_tensor = torch.tensor(obs, dtype = torch.float32, device = 'cuda' if self.CUDA_ENABLED else 'cpu')\n",
    "                    action_probs = self.policy_net(obs_tensor)\n",
    "                    action_dist = Categorical(action_probs)\n",
    "                    action = action_dist.sample()\n",
    "                    next_obs, reward, term, trunc, _ = env.step(action.item())\n",
    "\n",
    "                    # Strategy 1 - Accumulate the reward from the environment\n",
    "                    eps_reward += reward\n",
    "\n",
    "                    # TODO - Strategy 2 - evaluate the strategy based on states\n",
    "\n",
    "                    obs = next_obs\n",
    "                    done = term or trunc\n",
    "            \n",
    "                reward_history.append(eps_reward)\n",
    "                if verbose:\n",
    "                    print(f\"Validation episode {i+1:3d}/{n_episode_test}  |   Reward = {eps_reward:4.0f}\",end=\"\\r\")\n",
    "        reward_mean = mean(reward_history)\n",
    "        reward_stdev = stdev(reward_history)\n",
    "        \n",
    "        return reward_mean, reward_stdev\n",
    "    \n",
    "    def EMA_filter(self, reward: list, alpha):\n",
    "        ''' Function that runs an exponential moving average filter along a datastream '''\n",
    "        output = np.zeros(len(reward)+1)\n",
    "        output[0] = reward[0]\n",
    "        for idx, item in enumerate(reward):\n",
    "            output[idx+1] = (1 - alpha) * output[idx] + alpha * item\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def plot_reward_hist(self, alpha = 0.1):\n",
    "        ''' Function that plots the reward and filtered reward per episode, then saves the plot in a specified save directory'''\n",
    "        n_episodes= len(self.reward_history)\n",
    "        episodes = range(n_episodes)\n",
    "        filtered_reward_hist = self.EMA_filter(self.reward_history, alpha)\n",
    "        fig, axes = plt.subplots(2,1, figsize=(20,12))\n",
    "        axes[0].plot(episodes, self.reward_history[:n_episodes], label = 'Total reward', color = \"blue\")\n",
    "        axes[0].plot(episodes, filtered_reward_hist[:n_episodes], label = 'Filtered reward', color = \"red\")\n",
    "        axes[0].set_title(f'Total reward per episode - {self.hyperparam_config}')\n",
    "        axes[0].set_xlabel('Episode')\n",
    "        axes[0].set_ylabel('Reward')\n",
    "        axes[0].legend()\n",
    "\n",
    "        n_episodes= len(self.value_history)\n",
    "        episodes = range(n_episodes)\n",
    "        axes[1].plot(episodes, self.value_history[:n_episodes], label = \"v(s_0)\", color = \"blue\")\n",
    "        # If using baseline with a value estimation model, plot this as well\n",
    "        axes[1].plot(episodes, self.value_est_history[:n_episodes], label = r\"$\\hat v$(s_0)\", color = \"red\")\n",
    "        axes[1].set_title(f'Return in state s_0 - {self.hyperparam_config}')\n",
    "        axes[1].set_xlabel('Episode')\n",
    "        axes[1].set_ylabel('v(s_0)')\n",
    "        axes[1].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if self.save_path:\n",
    "            plt.savefig(os.path.join(self.save_path,'reward_history.png'))\n",
    "        plt.show()\n",
    "\n",
    "    def save_model(self):\n",
    "        ''' Function to save the model and optimizer state_dict for inference or continued training '''\n",
    "        self.model_path = os.path.join(self.save_path, 'q_network_checkpoint.pth')\n",
    "        torch.save({\n",
    "            'model_state_dict': self.policy_net.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, self.model_path)\n",
    "\n",
    "    def load_model(self):\n",
    "        ''' This code overwrite the Q_net with the parameters store in the instance's save_path '''\n",
    "        checkpoint = torch.load(self.model_path)\n",
    "        self.policy_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    def train_policy(self):\n",
    "        msg = \"Training ended with no good model found :<\"\n",
    "\n",
    "        # Create the directory to store results\n",
    "        self.run_number, self.hyperparam_config, self.save_path = self.create_directory()\n",
    "\n",
    "        # Training information\n",
    "        title = f\"Actor Critic   |   Model {self.model_name}, alpha={self.alpha}, beta={self.beta}, gamma={self.gamma}\" \n",
    "        if self.VERBOSE: print(title)\n",
    "\n",
    "        self.reward_history = []                # Track the total reward per episode\n",
    "        self.val_history = {}                   # Reset the validation history\n",
    "        self.value_history = []                 # History of the v(s_0) calculated using G_t throughout training\n",
    "        self.value_est_history = []             # History of the \\hat v(s_0) calculated using the value net (baseline case)\n",
    "        self.val_time = 0                       # Time used for validation (s)\n",
    "        episode = 0\n",
    "\n",
    "        # Control of early stopping\n",
    "        consecutive_pass_count = 0\n",
    "        CONSECUTIVE_PASS_LIMIT = 3\n",
    "        EPISODE_REWARD_LIMIT = SUCCESS_CRITERIA\n",
    "        best_reward = 0\n",
    "        self.best_model_episode = None\n",
    "        performance_crit = False                # Whether desired performance is met consistently\n",
    "        train_terminated = False\n",
    "        \n",
    "\n",
    "        self.train_time_start = time.time()\n",
    "        while not train_terminated:             # Experiment level - loop through episodes\n",
    "            obs, _ = self.env.reset(seed=self.SEED)\n",
    "            obs_tensor = torch.tensor(obs, dtype = torch.float32, device = 'cuda' if self.CUDA_ENABLED else 'cpu')\n",
    "            obs_init_tensor = obs_tensor\n",
    "            done = False\n",
    "            step = 0\n",
    "            eps_reward_history = []\n",
    "\n",
    "            while not done:                     # Episode level - loop through steps in an episode\n",
    "                action_probs = self.policy_net(obs_tensor)\n",
    "                action_dist = Categorical(action_probs)\n",
    "                action = action_dist.sample()\n",
    "\n",
    "                next_obs, reward, term, trunc, _ = self.env.step(action.item())\n",
    "                done = term or trunc\n",
    "                current_state_value = self.value_net(obs_tensor)\n",
    "                next_obs_tensor = torch.tensor(next_obs, dtype = torch.float32, device = 'cuda' if self.CUDA_ENABLED else 'cpu')\n",
    "                with torch.no_grad():\n",
    "                    if done: target_state_value = torch.tensor([reward], dtype=torch.float32, device='cuda' if self.CUDA_ENABLED else 'cpu')\n",
    "                    else: target_state_value = reward + self.gamma * self.value_net(next_obs_tensor)\n",
    "                delta = target_state_value - current_state_value\n",
    "                \n",
    "                # Find and optimize the value loss\n",
    "                value_loss = nn.MSELoss()(current_state_value, target_state_value)\n",
    "                # value_loss = delta.pow(2)\n",
    "                self.value_optimizer.zero_grad()\n",
    "                value_loss.backward()\n",
    "                self.value_optimizer.step()\n",
    "                \n",
    "                # Find and optimize the policy loss\n",
    "                policy_loss = - delta.detach() * action_dist.log_prob(action)\n",
    "                self.optimizer.zero_grad()\n",
    "                policy_loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                eps_reward_history.append(reward)\n",
    "                obs_tensor = next_obs_tensor\n",
    "                \n",
    "                step += 1\n",
    "                \n",
    "            # print(step)\n",
    "            # print(f\"\\nFinal value loss: {value_loss} \\nFinal policy loss: {policy_loss}\")\n",
    "            ## Post episode calculations\n",
    "            # Calculating return state s_0\n",
    "            returns = self.get_returns(eps_reward_history)\n",
    "            self.value_history.append(returns[0])\n",
    "\n",
    "            eps_reward = sum(eps_reward_history)\n",
    "            self.reward_history.append(eps_reward)     # Total reward of episode\n",
    "            \n",
    "            with torch.no_grad():   # Estimate and store \\hat v(s_0)\n",
    "                self.value_est_history.append(self.value_net(obs_init_tensor).item())\n",
    "            \n",
    "            # Periodic data logger\n",
    "            \n",
    "            if episode % self.LOG_PERIOD == 0 and self.VERBOSE:\n",
    "                printout_msg = f\"Episode {episode:5d}/{self.N_EPISODE_TRAIN}: Total reward = {eps_reward:5.1f}   |   G_0 = {returns[0]: 5.2f}   |   $\\hat v(s_0)$ = {self.value_est_history[-1]:6.2f}\" \n",
    "                print(printout_msg, end='\\r')\n",
    "\n",
    "            # Early stopping condition\n",
    "            if eps_reward >= EPISODE_REWARD_LIMIT:\n",
    "                # Evaluate the current good policy and record the validation time\n",
    "                self.val_time_start = time.time()\n",
    "                test_reward, _ = self.policy_eval(self.env_val, 20, verbose=False)\n",
    "                self.val_time += time.time() - self.val_time_start\n",
    "\n",
    "                self.val_history[episode] = [eps_reward, test_reward]\n",
    "\n",
    "                if test_reward >= best_reward:           # Set the new best reward\n",
    "                    best_reward = test_reward\n",
    "                    self.save_model()\n",
    "                    self.best_model_episode = episode\n",
    "                    msg = f\"Training terminated due to episode limit, best model saved at episode {self.best_model_episode:5d}\"\n",
    "                if test_reward > EPISODE_REWARD_LIMIT: \n",
    "                    consecutive_pass_count += 1\n",
    "                else: consecutive_pass_count = 0\n",
    "            else:\n",
    "                consecutive_pass_count = 0\n",
    "\n",
    "            # Performance criteria - if good results for several episodes => training performance satisfied and terminate early\n",
    "            if consecutive_pass_count >= CONSECUTIVE_PASS_LIMIT:\n",
    "                self.save_model()\n",
    "                self.best_model_episode = episode\n",
    "                performance_crit = True \n",
    "                msg = f\"Early termination at episode {self.best_model_episode:5d}, desired performance reached\"\n",
    "\n",
    "\n",
    "            episode += 1\n",
    "\n",
    "            # Checking for early training termination or truncation\n",
    "            train_terminated = (episode >= self.N_EPISODE_TRAIN) or (performance_crit)\n",
    "\n",
    "        self.train_time = time.time() - self.train_time_start\n",
    "\n",
    "        print(f\"\\nTotal runtime - {self.train_time:5.2f}\")\n",
    "        print(msg)\n",
    "        return\n",
    "\n",
    "    def record(self):\n",
    "        # Load the best policy net parameter from the experiment\n",
    "        if self.model_path:\n",
    "            self.load_model()\n",
    "\n",
    "        # Average test reward of the resulting policy - if best policy does not exists use the last one\n",
    "        self.env_test = gym.make('CartPole-v1')\n",
    "        print('Testing the best policy network performance')\n",
    "        reward_mean, reward_stdev = self.policy_eval(self.env_test, n_episode_test=500, verbose=True)\n",
    "        print(f\"\\nValidation average reward {reward_mean:4.2f} (SD = {reward_stdev:4.2f})\")\n",
    "\n",
    "        # Store the validation history and average test reward in the param_config JSON file\n",
    "        param_path = os.path.join(self.save_path, 'param_config.json')\n",
    "        if os.path.exists(param_path):\n",
    "            with open(param_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "        else:\n",
    "            data = {}\n",
    "\n",
    "        data['runtime'] = self.train_time\n",
    "        data['valtime'] = self.val_time\n",
    "        data['best_model_at'] = self.best_model_episode\n",
    "        data['val_history'] = self.val_history\n",
    "        data['test_result'] = [reward_mean, reward_stdev]\n",
    "\n",
    "        with open(param_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "        # Plot\n",
    "        self.plot_reward_hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cf1dd3",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ece2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "keys, values = zip(*param_grid.items())\n",
    "num_config = len(list(itertools.product(*values)))\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "for idx, v in enumerate(itertools.product(*values)):\n",
    "    for i in range(10):\n",
    "        # Unpack the hyperparameters configurations\n",
    "        config = dict(zip(keys,v))\n",
    "        MODEL_NAME = config['MODEL']\n",
    "        MODEL_CLASS = model_registry[MODEL_NAME]['class']\n",
    "        MODEL_CONFIG = model_registry[MODEL_NAME]['config']\n",
    "        match = re.search(r'v\\d+',MODEL_NAME)\n",
    "        MODEL_ID = match.group(0) if match else 404\n",
    "\n",
    "        ALPHA = config['ALPHA']\n",
    "        BETA = config['BETA']\n",
    "        GAMMA = config['GAMMA']\n",
    "        N_EPISODE_TRAIN = config['N_EPISODE_TRAIN']\n",
    "        \n",
    "        AC_experiment = ActorCritic(MODEL_NAME, \n",
    "                                        model_registry,\n",
    "                                        alpha = ALPHA,\n",
    "                                        beta = BETA,\n",
    "                                        gamma = GAMMA,\n",
    "                                        n_episode_train = N_EPISODE_TRAIN,\n",
    "                                        seed=42,\n",
    "                                        cuda_enabled=CUDA_ENABLED,\n",
    "                                        verbose=True)\n",
    "        AC_experiment.train_policy()\n",
    "        AC_experiment.record()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732e712e",
   "metadata": {},
   "source": [
    "## Load and Simulate a Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535cead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(q_network: nn.Module, model_path):\n",
    "    checkpoint = torch.load(model_path)\n",
    "    q_network.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64929b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6809ccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manually select a folder/run to load\n",
    "run_number = 'run_00043'\n",
    "\n",
    "## Find the paths to the param_config and model checkpoint\n",
    "# RESULT_DIR = os.path.dirname(REINFORCE_experiment.save_path)    # Can run this after running one experiment\n",
    "BASE_DIR = os.getcwd()\n",
    "RESULT_DIR = os.path.join(BASE_DIR,'cartpole_AC_results')\n",
    "RUN_DIR = os.path.join(RESULT_DIR, run_number)\n",
    "MODEL_PATH = os.path.join(RUN_DIR,'q_network_checkpoint.pth')\n",
    "\n",
    "## Find the model configuration\n",
    "with open(os.path.join(RUN_DIR,\"param_config.json\"),'r') as f:\n",
    "    data = json.load(f)\n",
    "model_name = data['parameters']['model_name']\n",
    "model_config = model_registry[model_name]['config']\n",
    "\n",
    "# Create a visual simulation of the network\n",
    "RENDER_MODE = \"human\"\n",
    "env_test_visual = gym.make(\"CartPole-v1\",render_mode=RENDER_MODE)\n",
    "obs_space = env_test_visual.observation_space.shape[0]\n",
    "act_space = env_test_visual.action_space.n\n",
    "num_test = 1\n",
    "\n",
    "# Create the model according to the model version in the model registry\n",
    "policy_net_loaded = PolicyNet(obs_space, act_space, model_config)\n",
    "load_model(policy_net_loaded, MODEL_PATH)\n",
    "\n",
    "for episode in range(num_test):\n",
    "    obs, _ = env_test_visual.reset()\n",
    "    done = False\n",
    "    eps_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        obs_tensor = torch.tensor(obs, dtype = torch.float32, device = 'cuda' if CUDA_ENABLED else 'cpu')\n",
    "        action_probs = policy_net_loaded(obs_tensor)\n",
    "        action_dist = Categorical(action_probs)\n",
    "        action = action_dist.sample()\n",
    "        next_obs, reward, term, trunc, _ = env_test_visual.step(action.item())\n",
    "\n",
    "        done = term or trunc\n",
    "        eps_reward += reward\n",
    "        obs = next_obs\n",
    "    \n",
    "    print(f\"Reward from episode {episode:3d} is {eps_reward}\")\n",
    "\n",
    "env_test_visual.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
