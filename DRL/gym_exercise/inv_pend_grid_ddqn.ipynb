{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "177b3eb5",
   "metadata": {},
   "source": [
    "# Inverted Pendulum DDQN - Grid Search\n",
    "\n",
    "This script is an extension to the inverted pendulum DQN algorithm in inv_pend.ipynb. This scripts automates the grid search in a hyperparameter space to explore the best performance of DQN.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d299b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random                       # To draw a random batch of samples form the replay buffer\n",
    "import gymnasium as gym             # To create the inverted pendulum environment\n",
    "import torch                        \n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import deque       # replay buffer is a double ended queue that allows elements to be added either to the end or the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "373024df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9b8bfc",
   "metadata": {},
   "source": [
    "# Parameter Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9744e20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Actual parameter grid\n",
    "# param_grid = {\n",
    "#     'MODEL': [model for model in model_registry],\n",
    "#     'LR': [5e-4, 1e-3, 5e-3, 1e-2],\n",
    "#     \"BUFFER_SIZE\": [1000, 5000, 10000],\n",
    "#     \"MIN_REPLAY_SIZE\": [1000],\n",
    "#     \"TARGET_UPDATE_FREQ\": [1000, 5000, 10000],\n",
    "\n",
    "#     \"GAMMA\": [0.90, 0.95, 0.98],\n",
    "#     \"EPSILON_START\": [1.0],\n",
    "#     \"EPSILON_END\": [0.1],\n",
    "#     \"EPSILON_DECAY\": [1000, 5000, 10000],\n",
    "\n",
    "#     \"EPISODE_TRAIN\": [5000],                # training truncation criteria\n",
    "#     \"BATCH_SIZE\": [32, 64, 128]\n",
    "# }\n",
    "\n",
    "# success_criteria = 450                      #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50e08e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparam grid - vary one at a time\n",
    "param_grid = {\n",
    "    'MODEL': ['DQN_MLP_v0'],\n",
    "    'LR': [1e-4, 5e-4, 1e-3, 5e-3],\n",
    "    # 'LR': [1e-4],\n",
    "    \"BUFFER_SIZE\": [5000],\n",
    "    \"MIN_REPLAY_SIZE\": [1000],\n",
    "    \"TARGET_UPDATE_FREQ\": [1000],\n",
    "\n",
    "    \"GAMMA\": [0.95],\n",
    "    \"EPSILON_START\": [1.0],\n",
    "    \"EPSILON_END\": [0.1],\n",
    "    \"EPSILON_DECAY\": [5000],\n",
    "\n",
    "    \"EPISODE_TRAIN\": [5000],                # training truncation criteria\n",
    "    \"BATCH_SIZE\": [32]\n",
    "}\n",
    "\n",
    "CUDA_ENABLED = False                        # Control whether to use cuda or not\n",
    "SUCCESS_CRITERIA = 450                      # Control the performance threshold for early stopping during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eb7d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Simplified param grid to test functionality\n",
    "# param_grid = {\n",
    "#     # 'MODEL': [model for model in model_registry],\n",
    "#     'MODEL': ['DQN_MLP_v0'],\n",
    "#     'LR': [5e-4],\n",
    "#     \"BUFFER_SIZE\": [5000],\n",
    "#     \"MIN_REPLAY_SIZE\": [1000],\n",
    "#     \"TARGET_UPDATE_FREQ\": [1000],\n",
    "\n",
    "#     \"GAMMA\": [0.95],\n",
    "#     \"EPSILON_START\": [1.0],\n",
    "#     \"EPSILON_END\": [0.1],\n",
    "#     \"EPSILON_DECAY\": [5000],\n",
    "\n",
    "#     \"EPISODE_TRAIN\": [5000],                # training truncation criteria\n",
    "#     \"BATCH_SIZE\": [32]\n",
    "# }\n",
    "\n",
    "# success_criteria = 450                      #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc95fe8",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "978d2147",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet_MLP(nn.Module):\n",
    "    ''' A QNetwork class that dynamically initialize an MLP Q Net'''\n",
    "    def __init__(self,input_dim,output_dim,hidden_layer = [64,32]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        for size in hidden_layer:\n",
    "            self.layers.append(nn.Linear(self.input_dim, size))\n",
    "            self.input_dim = size\n",
    "        \n",
    "        self.layers.append(nn.Linear(self.input_dim,self.output_dim))\n",
    "\n",
    "        if CUDA_ENABLED: self.cuda()\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        for layer in self.layers[:-1]:\n",
    "            input_data = torch.relu(layer(input_data))\n",
    "        return self.layers[-1](input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75f1aa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_registry = {\n",
    "    'DQN_MLP_v0': {\n",
    "        'class': QNet_MLP,\n",
    "        'config': [64,32]\n",
    "    },\n",
    "    'DQN_MLP_v1': {\n",
    "        'class': QNet_MLP,\n",
    "        'config': [32,32]\n",
    "    },\n",
    "    'DQN_MLP_v2': {\n",
    "        \n",
    "        'class': QNet_MLP,\n",
    "        'config': [32,16]\n",
    "    },\n",
    "    'DQN_MLP_v3': {\n",
    "        'class': QNet_MLP,\n",
    "        'config': [16,16]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e2ba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN_experiment():\n",
    "    def __init__(self, model_name: str,      # \"DQN_MLP_v0\" or \"DQN_MLP_v1\"\n",
    "                 model_registry, \n",
    "                 lr: float, \n",
    "                 buffer_size: int, \n",
    "                 target_update_freq: int, \n",
    "                 gamma: float, \n",
    "                 eps_start: float, \n",
    "                 eps_decay: int,\n",
    "                 eps_end: float, \n",
    "                 batch_size: int,\n",
    "                 seed = 42,\n",
    "                 cuda_enabled = False):\n",
    "        \n",
    "        self.seed = seed\n",
    "        self.cuda_enabled = cuda_enabled\n",
    "        \n",
    "        ''' Defining hyperparameters in the experiment '''\n",
    "        self.model_name = model_name                                        # Full name of the model\n",
    "        self.model_class = model_registry[self.model_name]['class']             # The model class \"QNet_MLP\" or \"QNet_test\"\n",
    "        self.model_config = model_registry[self.model_name]['config']       # List of nodes in each hidden layer\n",
    "        match = re.search(r'v\\d+',self.model_name)\n",
    "        self.model_id = match.group(0) if match else 404                    # Extract the \"v0\" or \"v1\" out of model name for abbreviation\n",
    "\n",
    "        # Hyperparameters of the experiment\n",
    "        self.lr = lr\n",
    "        self.buffer_size = buffer_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.gamma = gamma\n",
    "        self.eps_start, self.eps, self.eps_decay, self.eps_end = eps_start, eps_start, eps_decay, eps_end\n",
    "        self.batch_size = batch_size\n",
    "        self.episode_train = 5000\n",
    "        self.min_replay_size = 1000\n",
    "\n",
    "        # Initialize the train and validation environments\n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "        self.env_val = gym.make(\"CartPole-v1\")\n",
    "        self.obs_space = self.env.observation_space.shape[0]\n",
    "        self.act_space = self.env.action_space.n\n",
    "\n",
    "        # Initialize the 2 Q networks and the optimizer for the behavior Q_net\n",
    "        self.Q_net = self.model_class(self.obs_space, self.act_space, self.model_config)\n",
    "        self.target_net = self.model_class(self.obs_space, self.act_space, self.model_config).cuda()\n",
    "        self.target_net.load_state_dict(self.Q_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.SGD(self.Q_net.parameters(), lr = self.lr)\n",
    "\n",
    "        self.save_path = \"\"                                             # Directory of the current run\n",
    "        self.model_path = \"\"                                            # Path to a model \n",
    "        self.hyperparam_config = \"\"                                     # Shortened list of importatnt hyperparameters\n",
    "        self.reward_history = np.zeros(self.episode_train)\n",
    "        self.val_history = {}                                           # Monitor which episode had a validation run, the train reward, and the validation (test) reward \n",
    "\n",
    "    def create_directory(self):\n",
    "        ''' Function that creates directory to save model state_dict, architecture, training configuration, and history\n",
    "\n",
    "        Parameters: \n",
    "        ------------\n",
    "        (hyperparameters for differentiating between different directory)\n",
    "        \n",
    "        lr : float\n",
    "            the learning rate to optimize the Q network\n",
    "        gamma : float \n",
    "            the discount rate in Q learning\n",
    "        epsilon_decay : integer\n",
    "            the amount of episode over which the exploratory rate (epsilon) decays\n",
    "        batch_size : integer\n",
    "            number of experience drawn from replay buffer to train the behaviour network\n",
    "        buffer_size : integer\n",
    "            the number of samples in the replay buffer at a time\n",
    "        target_udpate_freq : integer\n",
    "            the amount of step count during the training process before updating the target Q net (loading the parameters of the behaviour net onto the target Q Net)\n",
    "        \n",
    "\n",
    "        Returns\n",
    "        ------------\n",
    "        name_codified : str\n",
    "            the shortened name for the current experiment \n",
    "        hyperparameters_codified : str\n",
    "            the shortened string of hyperparameter configuration\n",
    "        OUTPUT_DIR : path\n",
    "            the directory to which the training results and model (state_dict and architecture) will be saved\n",
    "        '''\n",
    "        timestamp = datetime.now().strftime(\"%y%m%d_%H%M\")\n",
    "\n",
    "        RESULT_FOLDER = \"inv_pend_DDQN_results\"\n",
    "        BASE_DIR = os.getcwd()\n",
    "        RESULT_DIR = os.path.join(BASE_DIR, RESULT_FOLDER)\n",
    "        os.makedirs(RESULT_DIR, exist_ok=True)      # Create the directory if one does not already exist\n",
    "\n",
    "        # Find the trial # of the latest run\n",
    "        existing_runs = [d for d in os.listdir(RESULT_DIR) if os.path.isdir(os.path.join(RESULT_DIR,d))]\n",
    "        run_numbers = [int(re.search(r'run_(\\d{5})',d).group(1)) for d in existing_runs if re.match(r'run_\\d{5}',d)]\n",
    "        trial_number = max(run_numbers,default=-1)+1\n",
    "\n",
    "        # Create a folder for the run\n",
    "        name_codified = f\"run_{trial_number:05d}\"\n",
    "        OUTPUT_DIR = os.path.join(RESULT_DIR,name_codified)\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)      # Create the directory\n",
    "\n",
    "        # Append the mapping from run # to hyperparameter configuration in a JSON file inside RESULT_DIR\n",
    "        trial_to_param_path = os.path.join(RESULT_DIR,'trial_to_param.json')\n",
    "        if os.path.exists(trial_to_param_path):\n",
    "            with open(trial_to_param_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "        else:\n",
    "            data = {name_codified: []}\n",
    "\n",
    "        if self.cuda_enabled:\n",
    "            hyperparam_codified = f\"DDQN_OOP_CUDA_{self.model_id}_{self.lr}_{self.buffer_size}_{self.target_update_freq}_{self.gamma}_{self.eps_decay}_{self.batch_size}\"\n",
    "            hyperparam_codified_time = f\"{timestamp}_DDQN_OOP_CUDA_{self.model_id}_{self.lr}_{self.buffer_size}_{self.target_update_freq}_{self.gamma}_{self.eps_decay}_{self.batch_size}\"\n",
    "        else:   \n",
    "            hyperparam_codified = f\"DDQN_OOP_nCUDA_{self.model_id}_{self.lr}_{self.buffer_size}_{self.target_update_freq}_{self.gamma}_{self.eps_decay}_{self.batch_size}\"\n",
    "            hyperparam_codified_time = f\"{timestamp}_DDQN_OOP_nCUDA_{self.model_id}_{self.lr}_{self.buffer_size}_{self.target_update_freq}_{self.gamma}_{self.eps_decay}_{self.batch_size}\"\n",
    "        data[name_codified] = hyperparam_codified_time\n",
    "\n",
    "        with open(trial_to_param_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "        # Store the training configs in JSON file\n",
    "        training_params = {\n",
    "            'OOP':                  True,\n",
    "            'CUDA':                 self.cuda_enabled,\n",
    "            'model_id':             self.model_id,\n",
    "            'lr':                   self.lr,\n",
    "            'gamma':                self.gamma,\n",
    "            'epsilon_decay':        self.eps_decay,\n",
    "            'batch_size':           self.batch_size,\n",
    "            'buffer_size':          self.buffer_size,\n",
    "            'target_update_freq':   self.target_update_freq\n",
    "        }\n",
    "\n",
    "        # Store training parameters in each run \n",
    "        param_path = os.path.join(OUTPUT_DIR, \"param_config.json\")\n",
    "        with open(param_path, \"w\") as f:\n",
    "            json.dump({\"parameters\": training_params}, f, indent=2)\n",
    "\n",
    "        return name_codified, hyperparam_codified, OUTPUT_DIR\n",
    "    \n",
    "    def eps_greedy_policy(self, env, obs, epsilon):      \n",
    "        ''' Function to take an action according to an epsilon-greedy policy and a Q-network \n",
    "        \n",
    "        Parameters:\n",
    "        ------------\n",
    "\n",
    "        env : gym.Env\n",
    "            the environment that the agent is taking a step in (either train/val/test env with seed or no seed)\n",
    "        obs : \n",
    "            the current observation from the environment\n",
    "        epsilon : float\n",
    "            the current exploration rate\n",
    "            \n",
    "        Return:\n",
    "        ------------\n",
    "\n",
    "        action\n",
    "            the next action that the agent takes\n",
    "        '''\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                if self.cuda_enabled:\n",
    "                    state_tensor = torch.tensor(obs, dtype=torch.float32, device='cuda').unsqueeze(0)\n",
    "                else:\n",
    "                    state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
    "                q_values = self.Q_net(state_tensor)\n",
    "                action = q_values.argmax().item()\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def prefill_replay(self):\n",
    "        obs,_ = self.env.reset()\n",
    "        for _ in range(self.min_replay_size):\n",
    "            action = self.env.action_space.sample()\n",
    "            next_obs, reward, term, trunc, _ = self.env.step(action)\n",
    "            done = term or trunc\n",
    "\n",
    "            self.replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "            obs = next_obs if not done else self.env.reset()[0]\n",
    "\n",
    "    def policy_eval(self, env: gym.Env, n_episode_test = 500, verbose = 0):\n",
    "        ''' Assess the average reward when following a q_network in a test environment with random state initialization\n",
    "        \n",
    "        Parameters:\n",
    "        env : gymnasium environment\n",
    "            - Can be either the self.env_test or self.env_val environment\n",
    "\n",
    "        '''\n",
    "\n",
    "        total_reward = 0\n",
    "        for i in range(n_episode_test):\n",
    "            obs,_ = env.reset()\n",
    "            done = False\n",
    "            eps_reward = 0\n",
    "\n",
    "            while not done:                 # Step thorugh the episode deterministically (no exploration)\n",
    "                action = self.eps_greedy_policy(env, obs, epsilon=0)\n",
    "                next_obs, reward, term, trunc, _ = env.step(action)\n",
    "\n",
    "                eps_reward += reward\n",
    "\n",
    "                obs = next_obs\n",
    "                done = term or trunc\n",
    "        \n",
    "            total_reward += eps_reward\n",
    "            if verbose:\n",
    "                    print(f\"Validation episode {i+1:3d}/{n_episode_test}  |   Reward = {eps_reward:4.0f}\",end=\"\\r\")\n",
    "        average_reward = total_reward / n_episode_test\n",
    "        return average_reward\n",
    "    \n",
    "    def EMA_filter(self, reward: list, alpha):\n",
    "        ''' Function that runs an exponential moving average filter along a datastream '''\n",
    "        output = np.zeros(len(reward)+1)\n",
    "        output[0] = reward[0]\n",
    "        for idx, item in enumerate(reward):\n",
    "            output[idx+1] = (1 - alpha) * output[idx] + alpha * item\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def plot_reward_hist(self, alpha = 0.1):\n",
    "        ''' Function that plots the reward and filtered reward per episode, then saves the plot in a specified save directory'''\n",
    "        n_episodes= len(self.reward_history)\n",
    "        episodes = range(n_episodes)\n",
    "        filtered_reward_hist = self.EMA_filter(self.reward_history, alpha)\n",
    "\n",
    "        plt.figure(figsize=(20,6))\n",
    "        plt.plot(episodes, self.reward_history[:n_episodes], color = \"blue\")\n",
    "        plt.plot(episodes, filtered_reward_hist[:n_episodes], color = \"red\")\n",
    "        plt.title(f'Total reward per episode - {self.hyperparam_config}')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.legend(['Total reward','Filtered reward'])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if self.save_path:\n",
    "            plt.savefig(os.path.join(self.save_path,'reward_history.png'))\n",
    "        plt.show()\n",
    "    \n",
    "    def save_model(self):\n",
    "        ''' Function to save the model and optimizer state_dict for inference or continued training '''\n",
    "        self.model_path = os.path.join(self.save_path, 'q_network_checkpoint.pth')\n",
    "        torch.save({\n",
    "            'model_state_dict': self.Q_net.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, self.model_path)\n",
    "\n",
    "    def load_model(self):\n",
    "        ''' This code overwrite the Q_net with the parameters store in the instance's save_path '''\n",
    "        checkpoint = torch.load(self.model_path)\n",
    "        self.Q_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "    def DDQN_train(self):\n",
    "        ''' Function to train a policy for a set of hyperparameters '''\n",
    "\n",
    "        msg = \"Training ended, no good model found!\"\n",
    "\n",
    "        self.replay_buffer = deque(maxlen = self.buffer_size)\n",
    "        self.prefill_replay()\n",
    "        self.reward_history = np.zeros(self.episode_train)\n",
    "        self.eps = self.eps_start\n",
    "        step_count = 0\n",
    "        episode = 0\n",
    "        target_network_update_count = 0\n",
    "\n",
    "        # Control of early stopping\n",
    "        consecutive_pass_count = 0           # Number of consecutive episodes where performance exceeds a threshold\n",
    "        CONSECUTIVE_PASS_LIMIT = 3          # No. of consecutive episodes with higher performance than reward limit\n",
    "        EPISODE_REWARD_LIMIT = SUCCESS_CRITERIA\n",
    "        best_reward = 0\n",
    "        performance_crit = False\n",
    "        train_terminated = False\n",
    "\n",
    "        _, self.hyperparam_config, self.save_path = self.create_directory()\n",
    "        self.train_time_start = time.time()\n",
    "        while not train_terminated:     # Experiment level - loop through episodes\n",
    "            obs, _ = self.env.reset(seed=self.seed)\n",
    "            eps_rewards = 0\n",
    "        \n",
    "            while True:                 # Episode level - loop through steps\n",
    "                action = self.eps_greedy_policy(self.env, obs, epsilon = self.eps)\n",
    "\n",
    "                # Interact with the environment\n",
    "                next_obs, reward, term, trunc, _ = self.env.step(action)\n",
    "                done = term or trunc\n",
    "                self.replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "                obs = next_obs\n",
    "                eps_rewards += reward\n",
    "                step_count += 1\n",
    "\n",
    "                # Train the Q-net using a batch of samples from the experience replay\n",
    "                if len(self.replay_buffer) >= self.batch_size:\n",
    "                    batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "                    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                    if self.cuda_enabled:\n",
    "                        states =        torch.tensor(np.array(states),      dtype = torch.float32,  device='cuda')           # Shape of (BATCH_SIZE, 4). Data structure of [[p1 v1 theta1 dtheta1], [p2 v2 theta2 dtheta2], ...]\n",
    "                        actions =       torch.tensor(actions,               dtype = torch.long,     device='cuda').unsqueeze(1)        # Must unsqueeze to have shape (BATCH_SIZE, 1). From [a1, a2, ...] to [[a1], [a2], ... [a_BATCH_SIZE]]\n",
    "                        rewards =       torch.tensor(rewards,               dtype = torch.float32,  device='cuda').unsqueeze(1)\n",
    "                        next_states =   torch.tensor(np.array(next_states), dtype = torch.float32,  device = 'cuda')\n",
    "                        dones =         torch.tensor(dones,                 dtype = torch.float32,  device='cuda').unsqueeze(1)\n",
    "                    else:\n",
    "                        states = torch.FloatTensor(states)                                          # Shape of (BATCH_SIZE, 4). Data structure of [[p1 v1 theta1 dtheta1], [p2 v2 theta2 dtheta2], ...]\n",
    "                        actions = torch.LongTensor(actions).unsqueeze(1)                            # Must unsqueeze to have shape (BATCH_SIZE, 1). From [a1, a2, ...] to [[a1], [a2], ... [a_BATCH_SIZE]]\n",
    "                        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "                        next_states = torch.FloatTensor(next_states)\n",
    "                        dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "                    \n",
    "                    # Compute targets using target network Q(s',a',w_i^-)\n",
    "                    # TODO - Change this from DQN to DDQN code\n",
    "                    with torch.no_grad():\n",
    "                        # Select the maximizing action according to the online behaviour net\n",
    "                        optimal_next_actions_online = self.Q_net(next_states).argmax(dim=1,keepdim=True)\n",
    "                        \n",
    "                        # Find the target Q value of the maximizing action according to the target net\n",
    "                        target_q_values = self.target_net(next_states).gather(1,optimal_next_actions_online)\n",
    "                        \n",
    "                        targets = rewards + self.gamma * target_q_values * (1 - dones)       # (1-dones) to avoid over estimating the value of terminal states. The target should only be the reward in the terminal states   # (1-dones) to avoid over estimating the value of terminal states. The target should only be the reward in the terminal states\n",
    "                \n",
    "                    # Compute the current Q values for the actions taken Q(s,a,w_i)\n",
    "                    q_values = self.Q_net(states).gather(1, actions)         # obtain the q_values associated to the actual action taken in each sample\n",
    "\n",
    "                    # Update the parameters of the behaviour q_network\n",
    "                    loss = nn.MSELoss()(q_values, targets)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                # Periodically update the target network by loading the weights from the behavior network\n",
    "                if step_count % self.target_update_freq == 0:\n",
    "                    target_network_update_count += 1\n",
    "                    self.target_net.load_state_dict(self.Q_net.state_dict())\n",
    "\n",
    "                if done:        # End of a training episode\n",
    "                    break\n",
    "\n",
    "            # Decay epsilon after an episode\n",
    "            self.eps = max(self.eps_end, self.eps - (self.eps_start - self.eps_end)/self.eps_decay)\n",
    "\n",
    "            self.reward_history[episode] = eps_rewards\n",
    "            # print(f\"Episode {episode:5d}: Total reward = {eps_rewards:5.1f}, Epsilon = {epsilon:.3f}, Step count = {step_count:5d}, Target update count = {target_network_update_count:3d}\")\n",
    "            # print(f\"Average reward {sum(reward_history[:episode])/(episode+1):.2f}\")\n",
    "\n",
    "            \n",
    "            if episode % 10 == 0:                   # print progress periodically\n",
    "                print(f\"Episode {episode:5d}/{self.episode_train}: Total reward = {eps_rewards:5.1f}, Epsilon = {self.eps:.3f}\", end = \"\\r\")\n",
    "\n",
    "            # Early stopping condition\n",
    "            if eps_rewards >= EPISODE_REWARD_LIMIT:\n",
    "                test_reward = self.policy_eval(self.env_val, 100)       \n",
    "                self.val_history[episode] = [eps_rewards, test_reward]\n",
    "\n",
    "                if test_reward >= best_reward:           # Set the new best reward\n",
    "                    best_reward = test_reward\n",
    "                    self.save_model()\n",
    "                    self.best_model_episode = episode\n",
    "                    msg = f\"Training terminated due to episode limit, best model saved at episode {self.best_model_episode:5d}\"\n",
    "                if test_reward > EPISODE_REWARD_LIMIT: \n",
    "                    consecutive_pass_count += 1\n",
    "                else: consecutive_pass_count = 0\n",
    "            else:\n",
    "                consecutive_pass_count = 0\n",
    "                \n",
    "            # Performance criteria - if good results for several episodes => training performance satisfied and terminate early\n",
    "            if consecutive_pass_count >= CONSECUTIVE_PASS_LIMIT:\n",
    "                self.save_model()\n",
    "                self.best_model_episode = episode\n",
    "                performance_crit = True \n",
    "                msg = f\"Early termination at episode {episode:5d}, desired performance reached\"\n",
    "\n",
    "\n",
    "            episode += 1\n",
    "\n",
    "            # Checking for early training termination or truncation\n",
    "            train_terminated = (episode >= self.episode_train) or (performance_crit)\n",
    "        self.train_time = time.time() - self.train_time_start\n",
    "\n",
    "        print(f\"\\nRuntime - {self.train_time:5.2f}\")\n",
    "\n",
    "        print(msg)\n",
    "        return\n",
    "\n",
    "    def DDQN_record(self):\n",
    "        ''' Method to plot the reward history and store the data in the current run folder '''\n",
    "\n",
    "        # Load the best Q net parameter from the experiment\n",
    "        self.load_model()\n",
    "\n",
    "        # Average test reward of the resulting policy\n",
    "        self.env_test = gym.make('CartPole-v1')\n",
    "        print(\"\\nTesting the Q_net\")\n",
    "        average_reward = self.policy_eval(self.env_test, n_episode_test=500, verbose = 1)\n",
    "        print(f\"\\nTest average reward {average_reward:4.2f}\")\n",
    "\n",
    "        # Store the validation hisotry and average test reward in the param_config JSON file\n",
    "        param_path = os.path.join(self.save_path,'param_config.json')\n",
    "        if os.path.exists(param_path):\n",
    "            with open(param_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "        else:\n",
    "            data = {}\n",
    "        data['runtime'] = self.train_time\n",
    "        data['best_model_at'] = self.best_model_episode\n",
    "        data['val_history'] = self.val_history\n",
    "        data['test_result'] = average_reward\n",
    "\n",
    "        with open(param_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "        # Plot\n",
    "        self.plot_reward_hist()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bb6584",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da69956",
   "metadata": {},
   "source": [
    "Using itertools to loop through each combination of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a343ac42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import time\n",
    "keys, values = zip(*param_grid.items())\n",
    "# keys, values = param_grid.keys(), param_grid.values()\n",
    "num_config = len(list(itertools.product(*values)))\n",
    "\n",
    "# Set fixed seed\n",
    "seed = 42\n",
    "np.random.seed(seed); random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "for idx, v in enumerate(itertools.product(*values)):\n",
    "\n",
    "    # Unpacking the hyperparameter configurations\n",
    "    config = dict(zip(keys,v))\n",
    "    MODEL_NAME = config['MODEL']                            # Name of the model: \"DQN_MLP_v0\", \"DQN_MLP_v1\"\n",
    "    MODEL_CLASS = model_registry[MODEL_NAME]['class']       # The model class: QNet_MLP,...\n",
    "    MODEL_CONFIG = model_registry[MODEL_NAME]['config']     # The architecture of the model\n",
    "    match = re.search(r'v\\d+',MODEL_NAME)                   \n",
    "    MODEL_ID = match.group(0) if match else 404             # The model id: \"v0\", \"v1\"\n",
    "\n",
    "    LR = config['LR']\n",
    "    BUFFER_SIZE = config['BUFFER_SIZE']\n",
    "    MIN_REPLAY_SIZE = config['MIN_REPLAY_SIZE']\n",
    "    TARGET_UPDATE_FREQ = config['TARGET_UPDATE_FREQ']\n",
    "    GAMMA = config['GAMMA']\n",
    "    EPS_START = config['EPSILON_START']\n",
    "    EPS_END = config['EPSILON_END']\n",
    "    EPS_DECAY = config['EPSILON_DECAY']\n",
    "    EPISODE_TRAIN = config['EPISODE_TRAIN']\n",
    "    BATCH_SIZE = config['BATCH_SIZE']\n",
    "\n",
    "\n",
    "    experiment = DDQN_experiment(MODEL_NAME, model_registry, \n",
    "                                 LR, BUFFER_SIZE, TARGET_UPDATE_FREQ, GAMMA, \n",
    "                                 EPS_START, EPS_DECAY, EPS_END, \n",
    "                                 BATCH_SIZE, seed, cuda_enabled=CUDA_ENABLED)\n",
    "    \n",
    "    print(next(experiment.Q_net.parameters()).is_cuda)\n",
    "    # # Training information\n",
    "    # print(f'Trial {idx+1}/{num_config} - model {MODEL_NAME}, lr={LR}, buffer={BUFFER_SIZE}, target_freq={TARGET_UPDATE_FREQ}, gamma={GAMMA}, eps_decay={EPS_DECAY}, batch_size={BATCH_SIZE}')\n",
    "    # experiment.DDQN_train()\n",
    "    # experiment.DDQN_record()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a7f6894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(experiment.Q_net.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def4e72",
   "metadata": {},
   "source": [
    "# Load and Simulate Saved Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82cc398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(q_network: nn.Module, model_path):\n",
    "    checkpoint = torch.load(model_path)\n",
    "    q_network.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17746506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy_policy(env, obs, epsilon: float, q_network: QNet_MLP):\n",
    "    ''' Function to take an action according to an epsilon-greedy policy and a Q-network'''\n",
    "    if np.random.random() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
    "            q_values = q_network(state_tensor)\n",
    "            action = q_values.argmax().item()\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c848fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model after training\n",
    "\n",
    "# Manually select a folder/run to load\n",
    "run_number = 'run_00015'\n",
    "\n",
    "\n",
    "# Find the paths to the param_config and model checkpoint\n",
    "RESULT_DIR = os.path.dirname(experiment.save_path)\n",
    "RUN_DIR = os.path.join(RESULT_DIR, run_number)\n",
    "MODEL_PATH = os.path.join(RUN_DIR,'q_network_checkpoint.pth')\n",
    "\n",
    "# Find the model configuration\n",
    "with open(os.path.join(RUN_DIR,\"param_config.json\"),'r') as f:\n",
    "    data = json.load(f)\n",
    "model_id = 'DQN_MLP_' + data['parameters']['model_id']\n",
    "model_config = model_registry[model_id]['config']\n",
    "\n",
    "# Create the model according to the model version in the model registry\n",
    "q_network_loaded = QNet_MLP(experiment.obs_space, experiment.act_space, model_config)\n",
    "load_model(q_network_loaded, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968924ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual simulation of the network\n",
    "env_test_visual = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "num_test = 5\n",
    "\n",
    "for episode in range(num_test):\n",
    "    obs, _ = env_test_visual.reset()\n",
    "    done = False\n",
    "    eps_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = eps_greedy_policy(env_test_visual, obs, epsilon = 0, q_network=q_network_loaded)\n",
    "        next_obs, reward, term, trunc, _ = env_test_visual.step(action)\n",
    "\n",
    "        done = term or trunc\n",
    "        eps_reward += reward\n",
    "        obs = next_obs\n",
    "    \n",
    "    print(f\"Reward from episode {episode:3d} is {eps_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
