{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4c45525",
   "metadata": {},
   "source": [
    "# Monte-Carlo FrozenLake\n",
    "This section explores the Monte Carlo method for reinforcement learning\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67bb428",
   "metadata": {},
   "source": [
    "## Background Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e5d1a8",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48053d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time  0  |  s_t   0  |  a_t  2  |  s_t+1   4  |  reward 0.00  |  terminated  0  |  {'prob': 0.3333333333333333}\n",
      "Time  1  |  s_t   4  |  a_t  1  |  s_t+1   5  |  reward 0.00  |  terminated  1  |  {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "# Initialize the environment\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True, render_mode = \"ansi\")  # is_slippery=True for stochasticity\n",
    "env.reset()\n",
    "\n",
    "state = 0; t = 0\n",
    "# Simulate several steps by following a random policy\n",
    "for i in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, term, trunc, info = env.step(action)\n",
    "    print(f\"Time {t:2d}  |  s_t {state:3d}  |  a_t {action:2d}  |  s_t+1 {next_state:3d}  |  reward {reward:.2f}  |  terminated {term:2}  |  {info}\")\n",
    "    # print(env.render())\n",
    "    state = next_state; t += 1\n",
    "    if term or trunc:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b4f8dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "q_table = np.zeros((state_size,action_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51575680",
   "metadata": {},
   "source": [
    "In Monte Carlo RL, the agent is supposed to traverse entire episode(s) to observe the trajectory and reward. The return of each state is monitored and used for updating the state-action value $Q(S_t,A_t)$\n",
    "\n",
    "Need to implement the following functionalities:\n",
    "1. A function to play out one episode (trajectory) that makes the agent steps through the environemtn until a termination condition is reached. These trajectories are returned as three lists: 1. states, 2. rewards, and 3. actions\n",
    "2. A function to calculate the cumulated return $G_t$ when given a list of state and a list of rewards along a trajectory generated by the function above. \n",
    "3. Need an array to store the tabular Q-value mapping from state-action $(s_t, a_t)$ to Q-value - $Q(s_t,a_t)$\n",
    "4. Need an array to store the occurences of all state-action combinations - $N(s_t,a_t)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb0fb13",
   "metadata": {},
   "source": [
    "### Question to self\n",
    "1. How can I specify a policy to control the agent? Thus far env.step() randomly chooses the next action for the agent.\n",
    "- Earlier on, we randomly sample the action from the action space of the environment. This action was then used in the env.step() function to guide the agent through one step. \n",
    "- With MC control, we gradually update the Q-value of all the state-action pairs and choose the action according to an epsilon-greedy policy\n",
    "\n",
    "2. How can I visualize the state-action value on the environment to visualize the policy?\n",
    "\n",
    "\n",
    "\n",
    "2. Are there alternatives to epsilon-greedy policy?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "f3395776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_returns(rewards, gamma = 0.9):\n",
    "    ''' Function to calculate the return of each time step when given a list of rewards \n",
    "    \n",
    "    For each step of the trajectory (of length T):\n",
    "    - Extract the rewards from that step onward\n",
    "    - Each step is multiplied by the corresponding gamma ^ index \n",
    "        the first reward received from leaving the state is not discounted\n",
    "        the last reward received from the trajectory is discouned by gamma ^ (T-1)\n",
    "    - Sum these values together to obtain the return at each step\n",
    "    '''\n",
    "    returns = np.zeros(len(rewards))\n",
    "    \n",
    "    for step, _ in enumerate(rewards):\n",
    "        step_reward = rewards[step:]            # reward from the current step onward\n",
    "\n",
    "        # List of discounted rewards at each time step\n",
    "        return_val = [gamma ** i * step_reward[i] for i in range(len(step_reward))]\n",
    "        return_val = sum(return_val)\n",
    "        \n",
    "        returns[step] = return_val\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f1fdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03090315 0.03433684 0.03815204 0.04239116 0.04710129 0.05233476\n",
      " 0.05814974 0.06461082 0.0717898  0.07976644 0.08862938 0.09847709\n",
      " 0.10941899 0.12157665 0.13508517 0.15009464 0.16677182 0.18530202\n",
      " 0.20589113 0.22876792 0.25418658 0.28242954 0.3138106  0.34867844\n",
      " 0.38742049 0.43046721 0.4782969  0.531441   0.59049    0.6561\n",
      " 0.729      0.81       0.9        1.        ]\n",
      "[0, 4, 0, 4, 4, 0, 0, 1, 0, 0, 1, 1, 0, 4, 0, 0, 0, 4, 8, 8, 8, 9, 13, 13, 14, 13, 14, 14, 14, 14, 10, 14, 10, 14, 15]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "states, rewards, actions = get_one_episode()\n",
    "\n",
    "print(get_returns(rewards))\n",
    "print(states)\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "229385e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_episode():\n",
    "    ''' This function returns a full trajectory of the agent\n",
    "    Episode end conditions:\n",
    "    - the agent falls into the ice or the agent reaches the reward\n",
    "    - truncated when hitting a time limit\n",
    "\n",
    "    Inputs:\n",
    "    - the initial state \n",
    "    '''\n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "\n",
    "    initial_state, _ = env.reset()\n",
    "    states.append(initial_state)\n",
    "\n",
    "    term = False\n",
    "    trunc = False\n",
    "\n",
    "    while (not term) and (not trunc):\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, term, trunc, _ = env.step(action)\n",
    "        \n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        states.append(next_state)\n",
    "\n",
    "\n",
    "    return states, rewards, actions\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
