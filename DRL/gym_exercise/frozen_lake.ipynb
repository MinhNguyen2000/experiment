{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4c45525",
   "metadata": {},
   "source": [
    "# Monte-Carlo\n",
    "This section explores the Monte Carlo method for reinforcement learning\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67bb428",
   "metadata": {},
   "source": [
    "## Background Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e5d1a8",
   "metadata": {},
   "source": [
    "## Code - MC Policy Iteration Experiment\n",
    "\n",
    "In this part of the code, we are \"training\" the agent to traverse the gridworld. This is accomplished with generalized policy iteration, using MC method to estimate the actuation value function $Q_\\pi(s_t,a_t)$\n",
    "\n",
    "Note that a customized FrozenLake environment, derived from the base version of gymnasium, is used. In gymnasium FrozenLake, the reset() method always returns the agent to state 0. However, the customized version give it an exploring start, where the agent can start its trajectory in any non-terminal state. This method ensures that each action-state is visited more uniformly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48053d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import custom_frozenlake  # Make sure this runs\n",
    "\n",
    "\n",
    "# Initialize the environment\n",
    "env = gym.make(\"CustomFrozenLake-v0\", map_name=\"4x4\", is_slippery=True, render_mode = \"ansi\")  # is_slippery=True for stochasticity\n",
    "obs, info = env.reset()\n",
    "\n",
    "state = obs; t = 0\n",
    "# Simulate several steps by following a random policy\n",
    "for i in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, term, trunc, info = env.step(action)\n",
    "    print(f\"Time {t:2d}  |  s_t {state:3d}  |  a_t {action:2d}  |  s_t+1 {next_state:3d}  |  reward {reward:.2f}  |  terminated {term:2}  |  {info}\")\n",
    "    # print(env.render())\n",
    "    state = next_state; t += 1\n",
    "    if term or trunc:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4f8dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "q_table = np.zeros((state_size,action_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51575680",
   "metadata": {},
   "source": [
    "In Monte Carlo RL, the agent is supposed to traverse entire episode(s) to observe the trajectory and reward. The return of each state is monitored and used for updating the state-action value $Q(S_t,A_t)$\n",
    "\n",
    "Need to implement the following functionalities:\n",
    "1. A `get_one_episode()` function to play out one episode (trajectory) that makes the agent steps through the environment until a termination condition is reached. These trajectories are returned as three lists: 1. states, 2. rewards, and 3. actions\n",
    "2. A `get_returns()` function to calculate the cumulated return $G_t$ when given a list of state and a list of rewards along a trajectory generated by the function above. \n",
    "3. Need a `state_action_values()` array to store the tabular Q-value mapping from state-action $(s_t, a_t)$ to Q-value - $Q(s_t,a_t)$\n",
    "4. (optional) Need an array to store the occurences of all state-action combinations - $N(s_t,a_t)$. This can however be substituted with a constant $\\alpha$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb0fb13",
   "metadata": {},
   "source": [
    "### Question to self\n",
    "1. How can I specify a policy to control the agent? Thus far `env.step()` randomly chooses the next action for the agent.\n",
    "- Earlier on, we randomly sampled the action from the action space of the environment. This action was then used in the `env.step()` function to guide the agent through one step. \n",
    "- With MC control, we gradually update the Q-value of all the state-action pairs and choose the action according to an epsilon-greedy policy. This policy then replaces the initial random policy and is used for selecting an action for\n",
    "\n",
    "2. How can I visualize the state-action value on the environment to visualize the policy?\n",
    "\n",
    "3. When collecting the trajectories for Monte Carlo training (for lack of better terminology), do we use epsilon-greedy in some way or do we use greedy policy?\n",
    "- Initially, we start with some random policy and turn this into an epsilon-greedy policy to sample/roll out/obtain trajectories.\n",
    "- The reward from these trajectories are turned into the return at each <s,a> state-action pair. These returns are normalized over the number of occurences of <s,a> and is used as an estimate of the action value function $Q(s,a)$\n",
    "\n",
    "2. Are there alternatives to epsilon-greedy policy?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3395776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_returns(rewards, gamma = 0.9):\n",
    "    ''' Function to calculate the return of each time step when given a list of rewards \n",
    "    \n",
    "    For each step of th trajectory (of length T):\n",
    "    - Extract the rewards from that step onward\n",
    "    - Each step is multiplied by the corresponding gamma ^ index \n",
    "        the first reward received from leaving the state is not discounted\n",
    "        the last reward received from the trajectory is discouned by gamma ^ (T-1)\n",
    "    - Sum these values together to obtain the return at each step\n",
    "    '''\n",
    "    returns = np.zeros(len(rewards))\n",
    "    \n",
    "    for step, _ in enumerate(rewards):\n",
    "        step_reward = rewards[step:]            # reward from the current step onward\n",
    "\n",
    "        # List of discounted rewards at each time step\n",
    "        return_val = [gamma ** i * step_reward[i] for i in range(len(step_reward))]\n",
    "        return_val = sum(return_val)\n",
    "        \n",
    "        returns[step] = return_val\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229385e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_episode(stochastic_policy):\n",
    "    ''' This function returns a full trajectory of the agent\n",
    "    Episode end conditions:\n",
    "    - the agent falls into the ice or the agent reaches the reward\n",
    "    - truncated when hitting a time limit\n",
    "\n",
    "    Inputs:\n",
    "    - stochastic policy - the stochastic epsilon-greedy policy (array of size action_space x states) \n",
    "    '''\n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "\n",
    "    initial_state, _ = env.reset()\n",
    "    states.append(initial_state)\n",
    "\n",
    "    term = False\n",
    "    trunc = False\n",
    "\n",
    "    # Rollout an entire episode until either terminated or truncated\n",
    "    while (not term) and (not trunc):\n",
    "        action = np.random.choice(env.action_space.n,\n",
    "                                  p = stochastic_policy[:,state])\n",
    "        next_state, reward, term, trunc, _ = env.step(action)\n",
    "        \n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        states.append(next_state)\n",
    "\n",
    "\n",
    "    return states, rewards, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff765fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deterministic_to_stochastic_policy(deterministic_policy, epsilon=0.3):\n",
    "    ''' Function to convert a deterministic greedy policy to a stochastic epsilon-greedy policy \n",
    "    \n",
    "    A greedy policy selects the action a_t from the state s_t that maximizes Q(s_t,a_t). \n",
    "    On the other hand, epsilon-greedy lets the agent explore the environment by following the optimal\n",
    "    policy (at the time) with 1-epsilon probability, while taking other actions with epsilon probability\n",
    "\n",
    "    Given an action space of size a, this function converts from a deterministic policy to a stochastic policy\n",
    "    that follows the optimal action with 1-epsilon probability, and the other (a-1) actions randomly with epsilon probability\n",
    "    \n",
    "    Inputs:\n",
    "    - deterministic_policy - the greedy policy, an array of size 1 x state\n",
    "    - epsilon - the probability at which the agent take sub-optimal actions randomly to explore\n",
    "    '''\n",
    "\n",
    "    num_action = 4          # TO-DO: Improve to avoid hard-coding\n",
    "    num_state = len(deterministic_policy)\n",
    "\n",
    "    # Initialize the stochastic policy with all epsilon / 3\n",
    "    stochastic_policy = np.full((num_action, num_state), epsilon / (num_action - 1))\n",
    "    for state, action in enumerate(deterministic_policy):\n",
    "        stochastic_policy[action][state] = 1 - epsilon\n",
    "\n",
    "    return stochastic_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f1fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_policy_evaluation(stochastic_policy, n_episodes = 100, gamma = 0.9):\n",
    "    ''' This function samples trajectories under a specified stochastic policy to approximate the value function \n",
    "    \n",
    "    (Value function evaluation from multiple trajectories)\n",
    "\n",
    "    Inputs:\n",
    "    - stochastic_policy - the policy that the agent follows to roll out trajectories. A matrix of size num_actions x num_states\n",
    "    - n_episodes - the number of trajectories sampled over which we find the average return G_t (to approximate the true action value function)\n",
    "    \n",
    "    Outputs:\n",
    "    - policy - the deterministic greedy policy (the action in each state that maximizes the action value function)\n",
    "    - state_action_values - array of size (n_actions x n_states)\n",
    "    '''\n",
    "\n",
    "    n_actions = stochastic_policy.shape[0]      # size of action space\n",
    "    n_states = stochastic_policy.shape[1]       # size of state space\n",
    "\n",
    "    state_action_return_total = np.zeros((n_actions, n_states))\n",
    "    state_action_count = np.zeros((n_actions, n_states))\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        # Get one full episode\n",
    "        states, rewards, actions = get_one_episode(stochastic_policy)\n",
    "\n",
    "        # Calculate the return\n",
    "        returns = get_returns(rewards, gamma)\n",
    "\n",
    "        # Update the total return for each state-action pair in the trajectory\n",
    "        for idx, _ in enumerate(states[:-1]):       # iterate through the trajectory (terminal state excluded)\n",
    "            state_action_return_total[actions[idx],states[idx]] += returns[idx]\n",
    "            state_action_count[actions[idx],states[idx]] += 1\n",
    "\n",
    "    # Update the state-action value function by dividing the total state\n",
    "    state_action_values = state_action_return_total / (state_action_count + 0.0001)\n",
    "\n",
    "    # Deterministic greedy policy\n",
    "    policy = np.argmax(state_action_values, axis=0)\n",
    "\n",
    "    return policy, state_action_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a5245e",
   "metadata": {},
   "source": [
    "Now that we have a function to perform value function evaluation, we need to somehow incorporate this improved evaluation into the improvement of the control policy. The output of policy evaluation are: 1. the state-action values $Q(s,a)$ $\\forall s \\in \\mathcal{S}, \\, a \\in \\mathcal{A}$, and 2. the deterministic greedy policy.\n",
    "\n",
    "We will use this new deterministic greedy policy, first converted to an epsilon-greedy policy, for building out trajectories for the next policy evaluation process\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86af7503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial random deterministic policy\n",
    "policy = np.random.choice(size = env.observation_space.n,\n",
    "                          a = np.arange(0,4,1))\n",
    "gamma = 0.8                 # discount factor in calculating the return at each step\n",
    "epsilon = 0.3               # probability of exploratory moves in epsilon-greedy\n",
    "\n",
    "n_episodes = 100            # number of trajectories considered when evaluating the policy\n",
    "n_policy_updates = 200   # number of policy improvement iterations\n",
    "\n",
    "all_policies = []\n",
    "count = 0\n",
    "\n",
    "for policy_update in range(n_policy_updates):\n",
    "    # Convert deterministic greedy policy to stochastic epsilon-greedy\n",
    "    stochastic_policy = deterministic_to_stochastic_policy(policy,epsilon = 0.6)\n",
    "\n",
    "    # MC policy evaluation\n",
    "    policy, state_action_values = MC_policy_evaluation(stochastic_policy, \n",
    "                                                       n_episodes = 5000, \n",
    "                                                       gamma = gamma)\n",
    "\n",
    "    if (policy_update % 50 == 0) or (policy_update == n_policy_updates - 1):\n",
    "\n",
    "        all_policies.append(policy)\n",
    "        count += 1\n",
    "        print(f\"Updated policy {count:2d} - {policy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9dd1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state_action_values)\n",
    "print(np.argmax(state_action_values, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2100aee",
   "metadata": {},
   "source": [
    "Now we can simulate the trajectory of the agent according to the stochastic epsilon-greedy policy generated above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058a3245",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True, render_mode = \"human\")  # is_slippery=True for stochasticity\n",
    "env_test.reset()\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    env_test.reset()\n",
    "    state = 0; t = 0\n",
    "    stochastic_policy = deterministic_to_stochastic_policy(policy)\n",
    "    term = False; trunc = False\n",
    "\n",
    "    while (not term) and (not trunc):\n",
    "        action = np.random.choice(a = env_test.action_space.n, \n",
    "                                p = stochastic_policy[:,state])\n",
    "        next_state, reward, term, trunc, info = env_test.step(action)\n",
    "        # print(f\"Time {t:2d}  |  s_t {state:3d}  |  a_t {action:2d}  |  s_t+1 {next_state:3d}  |  reward {reward:.2f}  |  terminated {term:2}  |  {info}\")\n",
    "        # print(env.render())\n",
    "        state = next_state; t += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0726c464",
   "metadata": {},
   "source": [
    "We can also plot the value function at the various state-action pairs in a 3D plot. In this plot, the x-y axes are the state and action, creating a mesh grid. The z-axis is then the value function at each vertex of the meshgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75a2925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "# %matplotlib widget\n",
    "\n",
    "actions = np.arange(state_action_values.shape[0])\n",
    "states = np.arange(state_action_values.shape[1])\n",
    "\n",
    "A, S = np.meshgrid(actions,states)\n",
    "Q = state_action_values[A,S]\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(A, S, Q, cmap='viridis')\n",
    "ax.set_xlabel('X (row)')\n",
    "ax.set_ylabel('Y (col)')\n",
    "ax.set_zlabel('Value V(s)')\n",
    "ax.set_xticks(actions)\n",
    "ax.set_yticks(states)\n",
    "\n",
    "\n",
    "plt.title(\"Action-State Value Function Surface\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacde6fd",
   "metadata": {},
   "source": [
    "# TD(0) Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3b9014",
   "metadata": {},
   "source": [
    "In temporal difference learning for control (which is used for **policy evaluation**), the agent traverse the environment step-by-step to bootstrap the action value function. As the agent take one step from a state $S_t$, it will receive a reward $R_{t+1}$ and reach a state $S{t+1}$. From this state, the agent takes another action $A_{t+1}$, which has a corresponding action value of $Q(S_{t+1},A_{t+1})$. These two values are used for backing up and updating the action value at time t, $Q(S_t,A_t)$ as \n",
    "\n",
    "$$ \\begin{aligned} \n",
    "Q(S_t,A_t) &\\leftarrow Q(S_t,A_t) + \\alpha \\Big(R(S_t,A_t) + \\gamma Q\\left(S_{t+1},A_{t+1}\\right) - Q\\left(S_t, A_t\\right)\\Big) \\\\\n",
    "Q(S_t,A_t) &\\leftarrow (1-\\alpha) \\cdot Q(S_t,A_t) + \\alpha \\Big(R(S_t,A_t) + \\gamma Q\\left(S_{t+1},A_{t+1}\\right) \\Big)\n",
    "\\end{aligned}  $$ \n",
    "\n",
    "One key difference between Monte Carlo and Temporal Difference Learning is the frequency of action-value updates. While MC waits for an entire episode to simulate, TD can update the action-value function as the agent steps through the environment. One can see that when given an arbitrary Q function, and thus an arbitrary policy, the agent slowly updates the state near the end first. The known action value of those states (which is zero) propagates backward after multiple episodes.\n",
    "\n",
    "The process works slowly backward from the terminal states with known rewards, propagating outward to neighboring states before reaching the starting states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4af1a3",
   "metadata": {},
   "source": [
    "## Question to self\n",
    "1. How does the agent take the action $A_t$ from state $S_t$ or action $A_{t+1}$ from state $S_{t+1}$? \n",
    "- Since temporal difference learning is used for policy evaluation in control, I think that we are using TD to evaluate a given policy (initially a random epsilon-greedy policy)\n",
    "- We sample the optimal action in a given state according to this policy, with some chance of exploring other actions as well. \n",
    "\n",
    "2. With MC, it makes sense to incorporate the average of $R(S_t,A_t)$ from multiple trajectories to derive the action value function. However, how does running multiple trajectories for TD help? How does the knowledge gained from one trajectory carries to the next?\n",
    "- Running multiple trajectories help propagate the value backward from terminal states with known rewards, thus approaching closer to the true value function.\n",
    "- However, it has been proven that even with one episode of policy evaluation, we can still get a policy that is never worse than the previous policy, or $\\pi'(s) \\ge \\pi(s)$\n",
    "\n",
    "3. Does exploring start assist or sabotage the learning process?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c834abf6",
   "metadata": {},
   "source": [
    "We will reuse three functions defined earlier: `get_returns()`, `get_one_episode()`, and `deterministic_to_stochastic_policy()`. However, we will rewrite the update_MC_policy() to update according to TD(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf2c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom, simplified frozen lake drawing function to visualize the policy\n",
    "from frozen_lake_plot import plot_grid\n",
    "\n",
    "# Initialize the environment\n",
    "env = gym.make(\"CustomFrozenLake-v0\", map_name=\"4x4\", is_slippery=True, render_mode = \"ansi\")  # is_slippery=True for stochasticity\n",
    "obs, info = env.reset()\n",
    "\n",
    "state = obs; t = 0\n",
    "# Simulate several steps by following a random policy\n",
    "for i in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, term, trunc, info = env.step(action)\n",
    "    print(f\"Time {t:2d}  |  s_t {state:3d}  |  a_t {action:2d}  |  s_t+1 {next_state:3d}  |  reward {reward:.2f}  |  terminated {term:2}  |  {info}\")\n",
    "    # print(env.render())\n",
    "    state = next_state; t += 1\n",
    "    if term or trunc:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5106e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_policy_evaluation(stochastic_policy, n_episodes = 100, gamma = 0.9, alpha = 0.5):\n",
    "    ''' This function samples trajectories under a specified stochastic policy to approximate the value function according to TD updates \n",
    "    Value function approximation is done over a specified number of trajectories \n",
    "    \n",
    "\n",
    "    Inputs:\n",
    "    - stochastic_policy - the policy that the agent follows to roll out trajectories. A matrix of size num_actions x num_states\n",
    "    - n_episodes - the number of trajectories sampled over which we find the average return G_t (to approximate the true action value function)\n",
    "    - gamma - the discount factor for reward signals\n",
    "    - alpha - learning rate/update rate\n",
    "\n",
    "    Outputs:\n",
    "    - policy - the deterministic greedy policy (the action in each state that maximizes the action value function)\n",
    "    - state_action_values - array of size (n_actions x n_states)\n",
    "    '''\n",
    "\n",
    "    n_actions = stochastic_policy.shape[0]      # size of action space\n",
    "    n_states = stochastic_policy.shape[1]       # size of state space\n",
    "\n",
    "    state_action_values = np.zeros((n_actions, n_states))\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        start_state, info = env.reset()             # Reset the agent to an arbitrary start state (non-terminal)\n",
    "        state = start_state\n",
    "        term, trunc = False, False\n",
    "\n",
    "        # Choose an initial action from this initial state according to the stochastic policy\n",
    "        action = np.random.choice(a = range(n_actions), \n",
    "                                  p = stochastic_policy[:,state])\n",
    "        \n",
    "        # Step through the environment to update the action value function\n",
    "        while (not term) and (not trunc):\n",
    "            # Carry out the action previously chosen, then sample another action A_{t+1} when reaching state S_{t+1}\n",
    "            next_state, reward, term, trunc, info = env.step(action)\n",
    "            next_action = np.random.choice(a = range(n_actions), \n",
    "                                           p = stochastic_policy[:,next_state])\n",
    "            \n",
    "            # Update the action value function\n",
    "            state_action_values[action,state] += alpha * (reward + gamma * state_action_values[next_action, next_state] - state_action_values[action,state])\n",
    "\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "        policy = np.argmax(state_action_values, axis=0)    \n",
    "    return policy, state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d5c7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def success_metric(policy, n_episodes_test = 5000):\n",
    "\n",
    "    ''' Assess the success rate of the current policy in terms of success rate and the time to reach the goal'''\n",
    "    env_test = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True, render_mode = \"ansi\")  # is_slippery=True for stochasticity\n",
    "    env_test.reset()\n",
    "\n",
    "    success_count = 0           # Number of successful episode\n",
    "    step_count = 0              # Number of steps in a successful episode\n",
    "\n",
    "    for i in range(n_episodes_test):\n",
    "\n",
    "        env_test.reset()\n",
    "        state = 0; t = 0\n",
    "        stochastic_policy = deterministic_to_stochastic_policy(policy, epsilon=0)\n",
    "\n",
    "        term = False; trunc = False\n",
    "        states = []; states.append(state)\n",
    "\n",
    "        while (not term) and (not trunc):\n",
    "            \n",
    "            action = np.random.choice(a = env_test.action_space.n, \n",
    "                                    p = stochastic_policy[:,state])\n",
    "            next_state, reward, term, trunc, info = env_test.step(action)\n",
    "            # print(f\"Time {t:2d}  |  s_t {state:3d}  |  a_t {action:2d}  |  s_t+1 {next_state:3d}  |  reward {reward:.2f}  |  terminated {term:2}  |  {info}\")\n",
    "            # print(env.render())\n",
    "\n",
    "            states.append(next_state)\n",
    "\n",
    "            state = next_state; t += 1\n",
    "\n",
    "            if next_state == 15: \n",
    "                success_count += 1 \n",
    "                step_count += len(states)\n",
    "\n",
    "    success_rate = success_count / n_episodes_test * 100\n",
    "    step_average_per_eps = np.round(step_count / (success_count+0.0001))\n",
    "\n",
    "    return success_rate, step_average_per_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9728cd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_policy_improvement(policy, n_policy_eval, n_policy_updates,\n",
    "                          gamma, epsilon, eps_decay, eps_decay_rate, alpha):\n",
    "    \n",
    "    ''' Policy improvement function that incorporates SARSA TD policy evaluation \n",
    "    \n",
    "    Inputs:\n",
    "    - policy - an array of size (1 x state_space) representing the deterministic policy with one chosen action in each state\n",
    "    - n_policy_eval - the number of episodes simulated to evaluate the current policy Q approx q_pi\n",
    "    - n_policy_updates - the number of iterations to update the policy\n",
    "    - gamma - the discount factor for calculating the return from the rewards of a trajectory\n",
    "    - epsilon - the exploration rate for turning a deterministic greedy policy to an stochastic epsilon-greedy policy\n",
    "    - eps_decay - the epsilon decay factor\n",
    "    - eps_decay_rate - the frequency of epsilon decay (every eps_decay_rate amount of policy improvement, epsilon is multiplied by the eps_decay factor)\n",
    "    - alpha - the learning/updating rate of the Q-value funciton\n",
    "    '''\n",
    "    \n",
    "    all_policies = []           # Record the policies\n",
    "    success_rates = []\n",
    "    step_average_per_eps_list = []\n",
    "\n",
    "    for policy_update in range(n_policy_updates):\n",
    "        # Convert deterministic greedy policy to stochastic epsilon-greedy\n",
    "        stochastic_policy = deterministic_to_stochastic_policy(policy, epsilon)\n",
    "\n",
    "        # TD policy evaluation\n",
    "        policy, state_action_values = TD_policy_evaluation(stochastic_policy, \n",
    "                                                        n_episodes = n_policy_eval, \n",
    "                                                        gamma = gamma,\n",
    "                                                        alpha = alpha)\n",
    "        \n",
    "        # Step decay of epsilon\n",
    "        if (policy_update % eps_decay_rate == 0) and (policy_update != 0):           # Half the exploration rate every few policy updates\n",
    "            epsilon = epsilon * eps_decay\n",
    "            print(f\"Current epsilon value - {epsilon}\")\n",
    "\n",
    "        # # Exponential decay of epsilon\n",
    "        # epsilon *= eps_decay ** (policy_update/eps_decay_rate)\n",
    "\n",
    "        # Visualize the policy update and track the performance once in a while\n",
    "        if (policy_update % 50 == 0) or (policy_update == n_policy_updates - 1):\n",
    "            all_policies.append(policy)\n",
    "            print(f\"Updated policy {policy_update:2d} {policy} \\n {policy.reshape(4,-1)}\")\n",
    "            plot_grid(4, state_action_values, f\"Policy {policy_update:2d}\")\n",
    "\n",
    "            # Simulate several test runs with the current policy to obtain success rate and average time taken\n",
    "            success_rate, step_average_per_eps = success_metric(policy)\n",
    "\n",
    "            success_rates.append(success_rate)\n",
    "            step_average_per_eps_list.append(step_average_per_eps)\n",
    "\n",
    "    return policy, all_policies, state_action_values, success_rates, step_average_per_eps_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06666e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy = np.random.choice(size = env.observation_space.n, a = np.arange(0,4,1))\n",
    "policy = np.zeros(16, dtype=int)\n",
    "\n",
    "gamma = 0.8                 # discount factor in calculating the return at each step\n",
    "epsilon = 0.6               # probability of exploratory moves in epsilon-greedy\n",
    "alpha = 0.05\n",
    "\n",
    "n_policy_eval = 100         # number of trajectories considered when evaluating the policy\n",
    "n_policy_updates = 2500     # number of policy improvement iterations\n",
    "\n",
    "eps_decay = 0.25\n",
    "eps_decay_rate = 500\n",
    "\n",
    "all_policies = []\n",
    "count = 0\n",
    "\n",
    "policy, policy_history, state_action_values, success_rates, step_average_per_eps_list = TD_policy_improvement(policy, n_policy_eval, n_policy_updates, \n",
    "                                                                                                              gamma, epsilon, eps_decay, eps_decay_rate, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88a20aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, item in enumerate(zip(success_rates, step_average_per_eps_list)):\n",
    "    print(f\"At policy {idx*50:4d} - Success rate of {item[0]:5.2f}% and takes {item[1]:3.0f} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f71a858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the history of success rate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iter = np.zeros(len(success_rates))\n",
    "success_rate_list = np.zeros(len(success_rates))\n",
    "step_count_list = np.zeros(len(step_average_per_eps_list))\n",
    "\n",
    "\n",
    "for idx, item in enumerate(zip(success_rates, step_average_per_eps_list)):\n",
    "    iter[idx] = idx * 50\n",
    "    success_rate_list[idx] = item[0]\n",
    "    step_count_list[idx] = item[1]\n",
    "\n",
    "# Create a figure and two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6))  # 2 rows, 1 column\n",
    "\n",
    "# Plot y1 vs x on the first subplot\n",
    "ax1.plot(iter, success_rate_list, marker='o', color='blue')\n",
    "ax1.set_title('Success Rate vs Policy Update Iteration')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y1')\n",
    "\n",
    "# Plot y2 vs x on the second subplot\n",
    "ax2.plot(iter, step_count_list, marker='s', color='green')\n",
    "ax2.set_title('Number of steps vs Policy Update Iteration')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y2')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13a0ebb",
   "metadata": {},
   "source": [
    "Test out the policy above in a simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72d119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True, render_mode = \"human\")  # is_slippery=True for stochasticity\n",
    "env_test.reset()\n",
    "\n",
    "success_count = 0           # Number of successful episode\n",
    "step_count = 0              # Number of steps in a successful episode\n",
    "n_episodes_test = 5\n",
    "\n",
    "# Benchmark against a random deterministic policy\n",
    "random_policy = np.random.choice(size = env.observation_space.n, a = np.arange(0,4,1))\n",
    "\n",
    "\n",
    "for i in range(n_episodes_test):\n",
    "\n",
    "    env_test.reset()\n",
    "    state = 0; t = 0\n",
    "    stochastic_policy = deterministic_to_stochastic_policy(policy, epsilon=0)\n",
    "\n",
    "    term = False; trunc = False\n",
    "    states = []; states.append(state)\n",
    "    actions = []\n",
    "\n",
    "    while (not term) and (not trunc):\n",
    "        \n",
    "        action = np.random.choice(a = env_test.action_space.n, \n",
    "                                p = stochastic_policy[:,state])\n",
    "        next_state, reward, term, trunc, info = env_test.step(action)\n",
    "        # print(f\"Time {t:2d}  |  s_t {state:3d}  |  a_t {action:2d}  |  s_t+1 {next_state:3d}  |  reward {reward:.2f}  |  terminated {term:2}  |  {info}\")\n",
    "        # print(env.render())\n",
    "\n",
    "        states.append(next_state)\n",
    "        actions.append(action)\n",
    "\n",
    "        state = next_state; t += 1\n",
    "\n",
    "        if next_state == 15: \n",
    "            success_count += 1 \n",
    "            step_count += len(states)\n",
    "\n",
    "success_rate = success_count / n_episodes_test * 100\n",
    "step_average_per_eps = np.round(step_count / (success_count+0.0001))\n",
    "print(f\"Success rate of {success_rate:.5f}% \\n Average number of step required - {step_average_per_eps:3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92f5036",
   "metadata": {},
   "source": [
    "Visualize the state-action values on a 3D grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03be0eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "# %matplotlib widget\n",
    "\n",
    "actions = np.arange(state_action_values.shape[0])\n",
    "states = np.arange(state_action_values.shape[1])\n",
    "\n",
    "A, S = np.meshgrid(actions,states)\n",
    "Q = state_action_values[A,S]\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(A, S, Q, cmap='viridis')\n",
    "ax.set_xlabel('X (row)')\n",
    "ax.set_ylabel('Y (col)')\n",
    "ax.set_zlabel('Value V(s)')\n",
    "ax.set_xticks(actions)\n",
    "ax.set_yticks(states)\n",
    "\n",
    "\n",
    "plt.title(\"Action-State Value Function Surface\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7470e7",
   "metadata": {},
   "source": [
    "# TD($\\lambda$) Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e627e29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d028a60",
   "metadata": {},
   "source": [
    "# REINFORCE algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9857b5",
   "metadata": {},
   "source": [
    "This experiment explores the policy gradient algorithm REINFORCE in the FrozenLake environment with discrete action space.\n",
    "\n",
    "TODO:\n",
    "- Policy network class\n",
    "- Initialize the environment, network, and optimizer\n",
    "\n",
    "- Training loop through num_episode_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f70a982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import custom_frozenlake\n",
    "from frozen_lake_plot import plot_grid\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torchsummary import summary \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9fd34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device_idx = torch.cuda.current_device()\n",
    "    print(f\"Number of GPU(s): {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {device_idx}|{torch.cuda.get_device_name(device_idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c499453",
   "metadata": {},
   "source": [
    "## Setup and Hyperparameter Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f4e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4\n",
    "GAMMA = 0.95\n",
    "N_EPISODE_TRAIN = 10000\n",
    "N_EPISODE_TESTING = 10           # for testing functionalities, not actual model testing\n",
    "CUDA_ENABLED = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08c9d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_layers=[64,32]):\n",
    "        ''' Initialize the model and create a list of layers '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        for layer_size in hidden_layers:\n",
    "            self.layers.append(nn.Linear(input_dim, layer_size))\n",
    "            input_dim = layer_size\n",
    "\n",
    "        self.layers.append(nn.Linear(input_dim, self.output_dim))\n",
    "        if CUDA_ENABLED: self.cuda()\n",
    "\n",
    "    def forward(self, input):\n",
    "        ''' input is a one hot encoded state vector '''\n",
    "        # If tensor of state_idx, convert to \n",
    "        for layer in self.layers[:-1]:\n",
    "            input = torch.relu(layer(input))\n",
    "        logits = self.layers[-1](input)\n",
    "        probs = torch.softmax(logits,dim=-1)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaec8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net_test = PolicyNet(16,4)\n",
    "print(next(policy_net_test.parameters()).is_cuda)\n",
    "summary(policy_net_test, (16,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c80a301",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CustomFrozenLake-v0\")\n",
    "obs_space = env.observation_space.n\n",
    "act_space = env.action_space.n\n",
    "\n",
    "# Simulate several episode in this environment\n",
    "for i in range(5):\n",
    "    print(f\"Episode {i+1}\")\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        next_obs, reward, term, trunc, _ = env.step(action)\n",
    "        print(f\"{obs:2d} | {action:2d} | {reward:2.1f} | {next_obs:2d}\")\n",
    "        obs = next_obs\n",
    "        done = term or trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b906036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(state_space, state_idx):\n",
    "    # Convert scalar state value to one hot encoding\n",
    "    one_hot_encoded_state = torch.zeros(state_space, device='cuda' if CUDA_ENABLED else 'cpu')\n",
    "    one_hot_encoded_state[state_idx] = 1.0\n",
    "    return one_hot_encoded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dd35d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prob_grid(grid_size, policy, title):\n",
    "    prob_value_array = []\n",
    "    for state_idx in range(grid_size * grid_size):\n",
    "        prob_value_array.append(policy(one_hot_encode(obs_space, state_idx)))\n",
    "    \n",
    "    prob_value_array_np = torch.stack(prob_value_array)\n",
    "    prob_value_array_np = prob_value_array_np.cpu().detach().numpy().T\n",
    "    plot_grid(grid_size, prob_value_array_np, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141adb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_returns(rewards, gamma = 0.9):\n",
    "    ''' Function to calculate the return of each time step when given a list of rewards \n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    rewards : list\n",
    "        a list of rewards achieved throughout the agent's trajectory\n",
    "    gamma   : float\n",
    "        the discount factor\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    returns : list\n",
    "        a list of returns G_t at each step of the trajectory\n",
    "        \n",
    "    For each step of th trajectory (of length T):\n",
    "    - Extract the rewards from that step onward\n",
    "    - Each step is multiplied by the corresponding gamma ^ index \n",
    "        the first reward received from leaving the state is not discounted\n",
    "        the last reward received from the trajectory is discouned by gamma ^ (T-1)\n",
    "    - Sum these values together to obtain the return at each step\n",
    "    '''\n",
    "    returns = np.zeros(len(rewards))\n",
    "    \n",
    "    for step, _ in enumerate(rewards):\n",
    "        step_reward = rewards[step:]            # reward from the current step onward\n",
    "\n",
    "        # List of discounted rewards at each time step\n",
    "        return_val = [gamma ** i * step_reward[i] for i in range(len(step_reward))]\n",
    "        return_val = sum(return_val)\n",
    "        \n",
    "        returns[step] = return_val\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3473c8e3",
   "metadata": {},
   "source": [
    "## Training the policy net with REINFORCE (Monte Carlo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564621c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = PolicyNet(obs_space, act_space, [64,32])\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "value_history = np.zeros(N_EPISODE_TRAIN)\n",
    "loss_history = np.zeros(N_EPISODE_TRAIN)\n",
    "\n",
    "# summary(policy_net, (obs_space, ), device = 'cuda' if CUDA_ENABLED else 'cpu')\n",
    "NUM_RUN = N_EPISODE_TRAIN\n",
    "\n",
    "for i in range(NUM_RUN):  \n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    log_probs = []          # Record a history of lob probability of the policy at each step\n",
    "    rewards = []            # Record the reward throughtout the episode\n",
    "\n",
    "    while not done: # Follow a policy network until terminated\n",
    "        # Select action according to the current policy network\n",
    "        obs_onehot = one_hot_encode(obs_space, obs)\n",
    "        # with torch.no_grad():\n",
    "        action_probs = policy_net(obs_onehot)\n",
    "        # print(action_probs)\n",
    "        action_dist = Categorical(action_probs)\n",
    "        action = action_dist.sample()\n",
    "\n",
    "        next_obs, reward, term, trunc, _ = env.step(action.item())\n",
    "        # print(f\"{obs:2d} | {action:2d} | {reward:2.1f} | {next_obs:2d}\")\n",
    "        rewards.append(reward)\n",
    "        obs = next_obs\n",
    "        done = term or trunc\n",
    "    \n",
    "        log_probs.append(action_dist.log_prob(action))  # this is not wrapped in torch.no_grad() since it is used for loss calculation\n",
    "\n",
    "    returns = get_returns(rewards, gamma = GAMMA)\n",
    "    value_history[i] = returns[0]\n",
    "\n",
    "    log_freq = 100\n",
    "    if i == 0 or i % log_freq == (log_freq - 1):\n",
    "        print(f\"Training Episode {i+1:4d}/{NUM_RUN} \\n- Rewards {rewards} \\n- Returns {returns} \\n- Return R_1 {returns[0]:3.3f}\", end = \"\\r\")\n",
    "        plot_prob_grid(4, policy_net, title = f\"FrozenLake episode {i+1:4d}\")\n",
    "        \n",
    "    \n",
    "    # Normalize the returns to reduce variance\n",
    "    # returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "    returns = torch.tensor(returns, dtype = torch.float32, device='cuda' if CUDA_ENABLED else 'cpu')\n",
    "\n",
    "    loss = -torch.sum(torch.stack(log_probs) * returns)\n",
    "    loss_history[i] = loss\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e972ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(value_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2565a61",
   "metadata": {},
   "source": [
    "## Simulate episodes with the current policy net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4057f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test = gym.make(\"FrozenLake-v1\")\n",
    "obs_space = env_test.observation_space.n\n",
    "\n",
    "success_count = 0\n",
    "N_EPISODE_TEST = 10000\n",
    "\n",
    "for epsidode in range(N_EPISODE_TEST):\n",
    "    obs, _ = env_test.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        obs = one_hot_encode(obs_space, obs)\n",
    "        action_probs = policy_net(obs)\n",
    "        action_dist = Categorical(action_probs)\n",
    "        action = action_dist.sample()\n",
    "\n",
    "        obs, reward, term, trunc, _ = env_test.step(action.item())\n",
    "        if obs == 15:\n",
    "            success_count += 1\n",
    "        done = term or trunc\n",
    "\n",
    "print(f\"Success rate of {success_count/N_EPISODE_TEST*100:4.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
