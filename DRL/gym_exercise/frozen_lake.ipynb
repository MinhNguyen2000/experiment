{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4c45525",
   "metadata": {},
   "source": [
    "# Monte-Carlo FrozenLake\n",
    "This section explores the Monte Carlo method for reinforcement learning\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67bb428",
   "metadata": {},
   "source": [
    "## Background Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e5d1a8",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48053d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time  0  |  s_t   0  |  a_t  1  |  s_t+1   1  |  reward 0.00  |  terminated  0  |  {'prob': 0.3333333333333333}\n",
      "Time  1  |  s_t   1  |  a_t  3  |  s_t+1   1  |  reward 0.00  |  terminated  0  |  {'prob': 0.3333333333333333}\n",
      "Time  2  |  s_t   1  |  a_t  0  |  s_t+1   1  |  reward 0.00  |  terminated  0  |  {'prob': 0.3333333333333333}\n",
      "Time  3  |  s_t   1  |  a_t  1  |  s_t+1   5  |  reward 0.00  |  terminated  1  |  {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "# Initialize the environment\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True, render_mode = \"ansi\")  # is_slippery=True for stochasticity\n",
    "env.reset()\n",
    "\n",
    "state = 0; t = 0\n",
    "# Simulate several steps by following a random policy\n",
    "for i in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, term, trunc, info = env.step(action)\n",
    "    print(f\"Time {t:2d}  |  s_t {state:3d}  |  a_t {action:2d}  |  s_t+1 {next_state:3d}  |  reward {reward:.2f}  |  terminated {term:2}  |  {info}\")\n",
    "    # print(env.render())\n",
    "    state = next_state; t += 1\n",
    "    if term or trunc:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b4f8dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "q_table = np.zeros((state_size,action_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51575680",
   "metadata": {},
   "source": [
    "In Monte Carlo RL, the agent is supposed to traverse entire episode(s) to observe the trajectory and reward. The return of each state is monitored and used for updating the state-action value $Q(S_t,A_t)$\n",
    "\n",
    "Need to implement the following functionalities:\n",
    "1. A function to play out one episode (trajectory) that makes the agent steps through the environemtn until a termination condition is reached. These trajectories are returned as three lists: 1. states, 2. rewards, and 3. actions\n",
    "2. A function to calculate the cumulated return $G_t$ when given a list of state and a list of rewards along a trajectory generated by the function above. \n",
    "3. Need an array to store the tabular Q-value mapping from state-action $(s_t, a_t)$ to Q-value - $Q(s_t,a_t)$\n",
    "4. (optional) Need an array to store the occurences of all state-action combinations - $N(s_t,a_t)$. This can however be substituted with a constant $\\alpha$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb0fb13",
   "metadata": {},
   "source": [
    "### Question to self\n",
    "1. How can I specify a policy to control the agent? Thus far env.step() randomly chooses the next action for the agent.\n",
    "- Earlier on, we randomly sample the action from the action space of the environment. This action was then used in the env.step() function to guide the agent through one step. \n",
    "- With MC control, we gradually update the Q-value of all the state-action pairs and choose the action according to an epsilon-greedy policy\n",
    "\n",
    "2. How can I visualize the state-action value on the environment to visualize the policy?\n",
    "\n",
    "3. When collecting the trajectories for Monte Carlo training (for lack of better terminology), do we use epsilon-greedy in some way or do we use greedy policy?\n",
    "- Initially, we start with some random policy and turn this into an epsilon-greedy policy to sample/roll out/obtain trajectories.\n",
    "- The reward from these trajectories are turned into the return at each <s,a> state-action pair. These returns are normalized over the number of occurences of <s,a> and is used as an estimate of the action value function $Q(s,a)$\n",
    "\n",
    "2. Are there alternatives to epsilon-greedy policy?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3395776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_returns(rewards, gamma = 0.9):\n",
    "    ''' Function to calculate the return of each time step when given a list of rewards \n",
    "    \n",
    "    For each step of the trajectory (of length T):\n",
    "    - Extract the rewards from that step onward\n",
    "    - Each step is multiplied by the corresponding gamma ^ index \n",
    "        the first reward received from leaving the state is not discounted\n",
    "        the last reward received from the trajectory is discouned by gamma ^ (T-1)\n",
    "    - Sum these values together to obtain the return at each step\n",
    "    '''\n",
    "    returns = np.zeros(len(rewards))\n",
    "    \n",
    "    for step, _ in enumerate(rewards):\n",
    "        step_reward = rewards[step:]            # reward from the current step onward\n",
    "\n",
    "        # List of discounted rewards at each time step\n",
    "        return_val = [gamma ** i * step_reward[i] for i in range(len(step_reward))]\n",
    "        return_val = sum(return_val)\n",
    "        \n",
    "        returns[step] = return_val\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229385e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_episode(stochastic_policy):\n",
    "    ''' This function returns a full trajectory of the agent\n",
    "    Episode end conditions:\n",
    "    - the agent falls into the ice or the agent reaches the reward\n",
    "    - truncated when hitting a time limit\n",
    "\n",
    "    Inputs:\n",
    "    - stochastic policy - the stochastic epsilon-greedy policy (array of size action_space x states) \n",
    "    '''\n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "\n",
    "    initial_state, _ = env.reset()\n",
    "    states.append(initial_state)\n",
    "\n",
    "    term = False\n",
    "    trunc = False\n",
    "\n",
    "    while (not term) and (not trunc):\n",
    "        action = np.random.choice(env.action_space.n,\n",
    "                                  p = stochastic_policy[:,state])\n",
    "        next_state, reward, term, trunc, _ = env.step(action)\n",
    "        \n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        states.append(next_state)\n",
    "\n",
    "\n",
    "    return states, rewards, actions\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff765fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deterministic_to_stochastic_policy(deterministic_policy, epsilon=0.3):\n",
    "    ''' Function to convert a deterministic greedy policy to a stochastic epsilon-greedy policy \n",
    "    \n",
    "    A greedy policy selects the action a_t from the state s_t that maximizes Q(s_t,a_t). \n",
    "    On the other hand, epsilon-greedy lets the agent explore the environment by following the optimal\n",
    "    policy (at the time) with 1-epsilon probability, while taking other actions with epsilon probability\n",
    "\n",
    "    Given an action space of size a, this function converts from a deterministic policy to a stochastic policy\n",
    "    that follows the optimal action with 1-epsilon probability, and the other (a-1) actions randomly with epsilon probability\n",
    "    \n",
    "    Inputs:\n",
    "    - deterministic_policy - the greedy policy, an array of size 1 x state\n",
    "    - epsilon - the probability at which the agent take sub-optimal actions randomly to explore\n",
    "    '''\n",
    "\n",
    "    num_action = 4          # TO-DO: Improve to avoid hard-coding\n",
    "    num_state = len(deterministic_policy)\n",
    "\n",
    "    # Initialize the stochastic policy with all epsilon / 3\n",
    "    stochastic_policy = np.full((num_action, num_state), epsilon / (num_action - 1))\n",
    "    for state, action in enumerate(deterministic_policy):\n",
    "        stochastic_policy[action][state] = 1 - epsilon\n",
    "\n",
    "    return stochastic_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f1fdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n",
      "[0, 4, 5]\n",
      "[0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "def update_MC_policy(stochastic_policy, n_episodes = 100, gamma = 0.9):\n",
    "    ''' This function samples trajectories under a specified stochastic policy to approximate the value function \n",
    "    \n",
    "    (Value function evaluation from multiple trajectories)\n",
    "\n",
    "    Inputs:\n",
    "    - stochastic_policy - the policy that the agent follows to roll out trajectories. A matrix of size num_actions x num_states\n",
    "    - n_episodes - the number of trajectories sampled over which we find the average return G_t (to approximate the true action value function)\n",
    "    \n",
    "    Outputs:\n",
    "    - policy - the deterministic greedy policy (the action in each state that maximizes the action value function)\n",
    "    - state_action_values - array of size (n_actions x n_states)\n",
    "    '''\n",
    "\n",
    "    n_actions = stochastic_policy.shape[0]\n",
    "    n_states = stochastic_policy.shape[0]\n",
    "\n",
    "    state_action_return_total = np.zeros((n_actions, n_states))\n",
    "    state_action_count = np.zeros((n_actions, n_states))\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        # Get one full episode\n",
    "        states, rewards, actions = get_one_episode(stochastic_policy)\n",
    "\n",
    "        # Calculate the return\n",
    "        returns = get_returns(rewards, gamma)\n",
    "\n",
    "        # Update the total return for each state-action pair in the trajectory\n",
    "        for idx, _ in enumerate(states[:-1]):       # iterate through the trajectory (terminal state excluded)\n",
    "            state_action_return_total[actions[idx],states[idx]] += returns[idx]\n",
    "            state_action_count[actions[idx],states[idx]] += 1\n",
    "\n",
    "    # Update the state-action value function by dividing the total state\n",
    "    state_action_values = state_action_return_total / (state_action_count + 0.0001)\n",
    "\n",
    "    # Deterministic greedy policy\n",
    "    policy = np.argmax(state_action_values, axis=0)\n",
    "\n",
    "    return policy, state_action_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a5245e",
   "metadata": {},
   "source": [
    "Now that we have a function to perform value function evaluation, we need to somehow incorporate this improved evaluation into the improvement of the control policy. The output of policy evaluation are: 1. the state-action values $Q(s,a)$ $\\forall s \\in \\mathcal{S}, \\, a \\in \\mathcal{A}$, and 2. the deterministic greedy policy.\n",
    "\n",
    "We will use this new deterministic greedy policy for building out trajectories for the next policy evaluation process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86af7503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
