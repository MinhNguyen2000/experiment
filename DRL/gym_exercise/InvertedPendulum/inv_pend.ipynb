{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54c818cf",
   "metadata": {},
   "source": [
    "This is a continuation of the experiment done on the gymnasium cart pole example in the classic-control environment package. Different from the cart pole environment, the inverted pendulum environment in the MuJoCo package extends to continuous action space. Therefore, the output of policy approximation is no longer the probability of each action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f321130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal      # normal dist\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean, stdev\n",
    "\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9a7765",
   "metadata": {},
   "source": [
    "# Explore the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eb1c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"InvertedPendulum-v5\",render_mode='human')\n",
    "obs_space = env.observation_space.shape[0]\n",
    "act_space = env.action_space.shape[0]\n",
    "print(f\"This environment has {obs_space} continuous state observations \"\n",
    "      f\"and {act_space} continuous action\")\n",
    "\n",
    "# Simulate several episodes\n",
    "for i in range(1):\n",
    "    obs, _ = env.reset()\n",
    "    t = 1\n",
    "    done = False\n",
    "    while not done:\n",
    "        random_action = env.action_space.sample()\n",
    "        \n",
    "        next_obs, reward, term, trunc, _ = env.step(random_action)\n",
    "        print(f\"Time {t:2d}  |  s_t {obs}  |  a_t {random_action}  |  reward {reward} |  s_t+1 {next_obs}\")\n",
    "        obs = next_obs; t += 1\n",
    "        done = term or trunc\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845216b2",
   "metadata": {},
   "source": [
    "# Simplified PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8150692f",
   "metadata": {},
   "source": [
    "## Parameter and Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b22b5daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter definitions\n",
    "MODEL_NAME = 'PPO_MLP_v0'\n",
    "ALPHA = 1e-5\n",
    "BETA = 1e-3\n",
    "GAMMA = 0.99                    # discount factor for return calculation\n",
    "EPS = 0.1\n",
    "\n",
    "BUFFER_SIZE = 2048\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "N_ITERATION = 100            # number of episodes to train the Q-network\n",
    "N_EPOCH = 10\n",
    "N_EPISODE_TEST = 5\n",
    "CUDA_ENABLED = False\n",
    "SUCCESS_CRITERIA = 450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "77bd0016",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_layers=[64,64], cuda_enabled=False):\n",
    "        ''' Initialize the model and create a list of layers '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim        # (action_space,) to estimate mean of the Gaussian action distribution in a state\n",
    "        # hidden layers\n",
    "        for layer_size in hidden_layers:\n",
    "            self.layers.append(nn.Linear(input_dim, layer_size))\n",
    "            input_dim = layer_size\n",
    "        # output layers\n",
    "        self.layers.append(nn.Linear(input_dim, self.output_dim))\n",
    "\n",
    "        # Trainable log_std (one per action dim)\n",
    "        log_std = -0.5 * torch.ones(output_dim, dtype=torch.float32, device='cuda' if cuda_enabled else 'cpu')\n",
    "        self.log_std = nn.Parameter(log_std)\n",
    "\n",
    "        if cuda_enabled: self.cuda()\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers[:-1]:\n",
    "            input = torch.relu(layer(input))\n",
    "        mu = self.layers[-1](input)\n",
    "        std = torch.exp(self.log_std)\n",
    "        return mu, std\n",
    "    \n",
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_layers=[32,32], cuda_enabled=False):\n",
    "        ''' Initialize the model and create a list of layers '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        for layer_size in hidden_layers:\n",
    "            self.layers.append(nn.Linear(input_dim, layer_size))\n",
    "            input_dim = layer_size\n",
    "\n",
    "        self.layers.append(nn.Linear(input_dim, self.output_dim))\n",
    "        if cuda_enabled: self.cuda()\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers[:-1]:\n",
    "            input = torch.relu(layer(input))\n",
    "        output = self.layers[-1](input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "255f747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_registry = {\n",
    "    'PPO_MLP_v0': {\n",
    "        'class': PolicyNet,\n",
    "        'config': [64,32],\n",
    "        'value_class': ValueNet,\n",
    "        'value_config': [32]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2c2704a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOBuffer():\n",
    "    def __init__(self, obs_dim, act_dim, buffer_size):\n",
    "        self.obs_buf = np.zeros((buffer_size, obs_dim), dtype = np.float32)\n",
    "        self.act_buf = np.zeros(buffer_size, dtype = np.float32)\n",
    "        self.rew_buf = np.zeros(buffer_size, dtype = np.float32)\n",
    "        self.nobs_buf = np.zeros((buffer_size, obs_dim), dtype = np.float32)    # Next observation\n",
    "        self.logp_buf = np.zeros(buffer_size, dtype = np.float32)               # pi(a_t|s_t) or the prob of the current action given the current observation           \n",
    "        self.done_buf = np.zeros(buffer_size, dtype = np.bool_)\n",
    "\n",
    "        self.val_buf = np.zeros(buffer_size, dtype = np.float32)                # Estimated state value of current obs\n",
    "        self.adv_buf = np.zeros(buffer_size, dtype = np.float32)\n",
    "        self.tar_buf = np.zeros(buffer_size, dtype = np.float32)                # Buffer of critic target\n",
    "\n",
    "        self.ptr = 0\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, logp, done):            # Store the transitions\n",
    "        assert self.ptr < self.buffer_size\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.nobs_buf[self.ptr] = next_obs\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr += 1\n",
    "\n",
    "    def get_transition(self):\n",
    "        ''' Get the data to calculate state value and critic target before training'''\n",
    "        data = dict(obs=self.obs_buf,\n",
    "                    act=self.act_buf,\n",
    "                    rew=self.rew_buf,\n",
    "                    next_obs = self.nobs_buf,\n",
    "                    logp=self.logp_buf,\n",
    "                    done=self.done_buf)\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    def get_data(self):\n",
    "        data = dict(obs=self.obs_buf, act=self.act_buf,\n",
    "                    rew=self.rew_buf, next_obs = self.nobs_buf,\n",
    "                    logp=self.logp_buf, done=self.done_buf,\n",
    "                    val=self.val_buf,\n",
    "                    adv=self.adv_buf,\n",
    "                    tar=self.tar_buf)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d1c95da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO():\n",
    "    def __init__(self, model_name: str,\n",
    "                 model_registry,\n",
    "                 alpha: float, beta: float, gamma: float,\n",
    "                 eps: float,\n",
    "                 buffer_size: int, batch_size: int,\n",
    "                 n_iter_train = 5000, n_epoch = 10,\n",
    "                 result_folder = 'invpend_PPO_results',\n",
    "                 seed = 42,\n",
    "                 cuda_enabled = False,\n",
    "                 verbose = True):\n",
    "        \n",
    "        self.result_folder = result_folder\n",
    "        self.SEED = seed\n",
    "        self.CUDA_ENABLED = cuda_enabled\n",
    "        self.VERBOSE = verbose\n",
    "        self.LOG_PERIOD = 1\n",
    "\n",
    "        self.alpha, self.beta, self.gamma = alpha, beta, gamma\n",
    "        self.eps = eps\n",
    "        self.buffer_size, self.batch_size = buffer_size, batch_size\n",
    "        self.N_ITER_TRAIN = n_iter_train\n",
    "        self.N_EPOCH = n_epoch\n",
    "\n",
    "        # Initialize the train and validation environments\n",
    "        self.env = gym.make(\"InvertedPendulum-v5\", render_mode=\"rgb_array\")\n",
    "        self.env_val = gym.make(\"InvertedPendulum-v5\", render_mode=\"rgb_array\")\n",
    "        self.obs_space = self.env.observation_space.shape[0]\n",
    "        self.act_space = self.env.action_space.shape[0]\n",
    "        \n",
    "        ''' Experiment hyperparameters '''  \n",
    "        # Policy model configuration\n",
    "        self.model_name = model_name\n",
    "        match = re.search(r'v\\d+', self.model_name)\n",
    "        self.model_id = match.group(0) if match else 404\n",
    "\n",
    "        self.policy_model_class = model_registry[self.model_name]['class']\n",
    "        self.policy_model_config = model_registry[self.model_name]['config']\n",
    "        self.value_model_class = model_registry[self.model_name]['value_class']\n",
    "        self.value_model_config = model_registry[self.model_name]['value_config']\n",
    "\n",
    "        # Instantiate and initialize the policy network\n",
    "        self.policy_net = self.policy_model_class(self.obs_space, self.act_space, self.policy_model_config)\n",
    "        self.policy_net.apply(self.init_weights)\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr = self.alpha)\n",
    "\n",
    "        self.policy_net_old = self.policy_model_class(self.obs_space, self.act_space, self.policy_model_config)\n",
    "        self.policy_net_old.load_state_dict(self.policy_net.state_dict())\n",
    "        self.policy_net_old.eval()                              # Set to only do inference\n",
    "        \n",
    "        # Instantiate and initialize the state value network\n",
    "        self.value_net = self.value_model_class(self.obs_space, 1, self.value_model_config)\n",
    "        self.value_net.apply(self.init_weights)\n",
    "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr = self.beta)\n",
    "\n",
    "        self.save_path = ''\n",
    "        self.model_path = ''\n",
    "        self.hyperparam_config = ''\n",
    "        self.reward_history = []\n",
    "        self.val_history = {}                                           # Monitor which episode had a validation run, the train reward, and the validation (test) reward \n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def create_directory(self):\n",
    "        ''' Function that creates directory to save model state_dict, architecture, training configuration, and history\n",
    "\n",
    "        Parameters: \n",
    "        ------------\n",
    "        (hyperparameters for differentiating between different directory)\n",
    "        \n",
    "        lr : float\n",
    "            the learning rate to optimize the Q network\n",
    "        gamma : float \n",
    "            the discount rate in Q learning\n",
    "        \n",
    "        Returns\n",
    "        ------------\n",
    "        name_codified : str\n",
    "            the shortened name for the current experiment \n",
    "        hyperparameters_codified : str\n",
    "            the shortened string of hyperparameter configuration\n",
    "        OUTPUT_DIR : path\n",
    "            the directory to which the training results and model (state_dict and architecture) will be saved\n",
    "        '''\n",
    "        timestamp = datetime.now().strftime(\"%y%m%d_%H%M\")\n",
    "\n",
    "        RESULT_FOLDER = self.result_folder\n",
    "        BASE_DIR = os.getcwd()\n",
    "        RESULT_DIR = os.path.join(BASE_DIR, RESULT_FOLDER)\n",
    "        os.makedirs(RESULT_DIR, exist_ok=True)      # Create the directory if one does not already exist\n",
    "\n",
    "        # Find the trial # of the latest run\n",
    "        existing_runs = [d for d in os.listdir(RESULT_DIR) if os.path.isdir(os.path.join(RESULT_DIR,d))]\n",
    "        run_numbers = [int(re.search(r'run_(\\d{5})',d).group(1)) for d in existing_runs if re.match(r'run_\\d{5}',d)]\n",
    "        trial_number = max(run_numbers,default=-1)+1\n",
    "\n",
    "        # Create a folder for the run\n",
    "        name_codified = f\"run_{trial_number:05d}\"\n",
    "        OUTPUT_DIR = os.path.join(RESULT_DIR,name_codified)\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)      # Create the directory\n",
    "\n",
    "        # Append the mapping from run # to hyperparameter configuration in a JSON file inside RESULT_DIR\n",
    "        trial_to_param_path = os.path.join(RESULT_DIR,'trial_to_param.json')\n",
    "        if os.path.exists(trial_to_param_path):\n",
    "            with open(trial_to_param_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "        else:\n",
    "            data = {name_codified: \"\"}\n",
    "\n",
    "        hyperparam_codified = \"PPO_\"\n",
    "        hyperparam_codified += \"OOP_\"\n",
    "        hyperparam_codified += \"CUDA_\" if self.CUDA_ENABLED else \"nCUDA_\"\n",
    "        hyperparam_codified += f\"{self.model_id}_{self.alpha}_{self.beta}_{self.gamma}\"\n",
    "        hyperparam_codified_time = f\"{timestamp}_\" + hyperparam_codified\n",
    "\n",
    "        data[name_codified] = hyperparam_codified_time\n",
    "\n",
    "        with open(trial_to_param_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "        # Store the training configs in JSON file\n",
    "        training_params = {\n",
    "            'OOP':                  True,\n",
    "            'CUDA':                 self.CUDA_ENABLED,\n",
    "            'device':               torch.cuda.get_device_name(torch.cuda.current_device()),\n",
    "            'model_name':           self.model_name,\n",
    "            'alpha':                self.alpha,\n",
    "            'beta':                 self.beta,\n",
    "            'gamma':                self.gamma,\n",
    "            'eps':                  self.eps,\n",
    "            'buffer_size':          self.buffer_size,\n",
    "            'batch_size':           self.batch_size,\n",
    "            'epoch':                self.N_EPOCH\n",
    "        }\n",
    "\n",
    "        # Store training parameters in each run \n",
    "        param_path = os.path.join(OUTPUT_DIR, \"param_config.json\")\n",
    "        with open(param_path, \"w\") as f:\n",
    "            json.dump({\"parameters\": training_params}, f, indent=2)\n",
    "\n",
    "        return name_codified, hyperparam_codified, OUTPUT_DIR\n",
    "    \n",
    "    def get_first_return(self, eps_reward_history):\n",
    "        ''' Function to calculate the return of the first state '''\n",
    "        gamma = self.gamma\n",
    "        \n",
    "        return_val = [gamma ** i * eps_reward_history[i] for i in range(len(eps_reward_history))]\n",
    "        return sum(return_val)\n",
    "\n",
    "    def policy_eval(self, env: gym.Env, n_iter_test = 500, verbose = True):\n",
    "        ''' Assess the average reward when following the policy net in a test environment with random state initialization\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        env : gymnasium environment\n",
    "            this environment can be either the self.env_test or self.env_val environment (whether they are the same)\n",
    "        n_episode_test : int \n",
    "            the number of evaluation episodes\n",
    "        verbose : bool\n",
    "            whether to print testing information \n",
    "\n",
    "        Return:\n",
    "        ----------\n",
    "        average_reward : float\n",
    "            the average reward received from running the test\n",
    "        '''\n",
    "\n",
    "        reward_history = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(n_iter_test):\n",
    "                obs,_ = env.reset()\n",
    "                done = False\n",
    "                eps_reward = 0\n",
    "\n",
    "                while not done:                 # Step thorugh the episode deterministically (no exploration)\n",
    "                    obs_tensor = torch.tensor(obs, dtype = torch.float32, device = 'cuda' if self.CUDA_ENABLED else 'cpu')\n",
    "                    action_mu, action_std = self.policy_net(obs_tensor)\n",
    "                    action_dist = Normal(action_mu, action_std)\n",
    "                    action = action_dist.sample()\n",
    "                    next_obs, reward, term, trunc, _ = env.step(action)\n",
    "\n",
    "                    # Strategy 1 - Accumulate the reward from the environment\n",
    "                    eps_reward += reward\n",
    "\n",
    "                    # TODO - Strategy 2 - evaluate the strategy based on states\n",
    "\n",
    "                    obs = next_obs\n",
    "                    done = term or trunc\n",
    "            \n",
    "                reward_history.append(eps_reward)\n",
    "                if verbose:\n",
    "                    print(f\"Validation episode {i+1:3d}/{n_iter_test}  |   Reward = {eps_reward:4.0f}\",end=\"\\r\")\n",
    "        reward_mean = mean(reward_history)\n",
    "        reward_stdev = stdev(reward_history)\n",
    "        \n",
    "        return reward_mean, reward_stdev\n",
    "    \n",
    "    def EMA_filter(self, reward: list, alpha):\n",
    "        ''' Function that runs an exponential moving average filter along a datastream '''\n",
    "        output = np.zeros(len(reward)+1)\n",
    "        output[0] = reward[0]\n",
    "        for idx, item in enumerate(reward):\n",
    "            output[idx+1] = (1 - alpha) * output[idx] + alpha * item\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def plot_reward_hist(self, alpha = 0.1):\n",
    "        ''' Function that plots the reward and filtered reward per episode, then saves the plot in a specified save directory'''\n",
    "        n_episodes= len(self.reward_history)\n",
    "        episodes = range(n_episodes)\n",
    "        filtered_reward_hist = self.EMA_filter(self.reward_history, alpha)\n",
    "        fig, axes = plt.subplots(1,1, figsize=(20,6))\n",
    "\n",
    "        axes.plot(episodes, self.reward_history[:n_episodes], label = 'Total reward', color = \"blue\")\n",
    "        axes.plot(episodes, filtered_reward_hist[:n_episodes], label = 'Filtered reward', color = \"red\")\n",
    "        axes.set_title(f'Total reward per episode - {self.hyperparam_config}')\n",
    "        axes.set_xlabel('Episode')\n",
    "        axes.set_ylabel('Reward')\n",
    "        axes.legend()\n",
    "        \n",
    "        # axes[0].plot(episodes, self.reward_history[:n_episodes], label = 'Total reward', color = \"blue\")\n",
    "        # axes[0].plot(episodes, filtered_reward_hist[:n_episodes], label = 'Filtered reward', color = \"red\")\n",
    "        # axes[0].set_title(f'Total reward per episode - {self.hyperparam_config}')\n",
    "        # axes[0].set_xlabel('Episode')\n",
    "        # axes[0].set_ylabel('Reward')\n",
    "        # axes[0].legend()\n",
    "\n",
    "        # n_episodes= len(self.value_history)\n",
    "        # episodes = range(n_episodes)\n",
    "        # axes[1].plot(episodes, self.value_history[:n_episodes], label = \"v(s_0)\", color = \"blue\")\n",
    "        # # If using baseline with a value estimation model, plot this as well\n",
    "        # axes[1].plot(episodes, self.value_est_history[:n_episodes], label = r\"$\\hat v$(s_0)\", color = \"red\")\n",
    "        # axes[1].set_title(f'Return in state s_0 - {self.hyperparam_config}')\n",
    "        # axes[1].set_xlabel('Episode')\n",
    "        # axes[1].set_ylabel('v(s_0)')\n",
    "        # axes[1].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if self.save_path:\n",
    "            plt.savefig(os.path.join(self.save_path,'reward_history.png'))\n",
    "        plt.show()\n",
    "\n",
    "    def save_model(self):\n",
    "        ''' Function to save the model and optimizer state_dict for inference or continued training '''\n",
    "        self.model_path = os.path.join(self.save_path, 'q_network_checkpoint.pth')\n",
    "        torch.save({\n",
    "            'model_state_dict': self.policy_net.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, self.model_path)\n",
    "\n",
    "    def load_model(self):\n",
    "        ''' This code overwrite the Q_net with the parameters store in the instance's save_path '''\n",
    "        checkpoint = torch.load(self.model_path)\n",
    "        self.policy_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    def prefill_buffer(self):\n",
    "        ''' Function that prefill a buffer with transitions (s,a,r,s',d) under the current policy '''\n",
    "        self.buffer = PPOBuffer(self.obs_space, self.act_space, self.buffer_size)\n",
    "        \n",
    "        buffer_filled = False\n",
    "        count = 0\n",
    "        eps_return = []     # Store v(s_0) of each episode in the buffer\n",
    "        eps_length = []\n",
    "        # Collect experience using the old (frozen) policy\n",
    "        while not buffer_filled:\n",
    "            obs, _ = self.env.reset()\n",
    "            done = False\n",
    "            eps_reward_history = []\n",
    "            \n",
    "            while not done:\n",
    "                with torch.no_grad():\n",
    "                    obs_tensor = torch.tensor(obs, dtype = torch.float32, device='cuda' if self.CUDA_ENABLED else 'cpu')\n",
    "                    action_mu, action_std = self.policy_net_old(obs_tensor)\n",
    "                    action_dist = Normal(action_mu, action_std)\n",
    "                    action = action_dist.sample()\n",
    "                    action_logp = action_dist.log_prob(action)\n",
    "                next_obs, reward, term, trunc, _ = self.env.step(action) \n",
    "                eps_reward_history.append(reward)\n",
    "                done = term or trunc\n",
    "\n",
    "                self.buffer.store(obs_tensor, action.item(), reward, next_obs, action_logp, done)\n",
    "                obs = next_obs\n",
    "\n",
    "                count += 1\n",
    "\n",
    "                if count >= self.buffer_size: \n",
    "                    buffer_filled = True\n",
    "                    break\n",
    "            \n",
    "            # Only account for full episodes (exclude the last one that is terminated due to buffer size)\n",
    "            if not buffer_filled: \n",
    "                eps_length.append(len(eps_reward_history))\n",
    "                eps_return.append(self.get_first_return(eps_reward_history))\n",
    "\n",
    "        # Average value V(s_0) and average episode length \n",
    "        self.avg_return = np.mean(eps_return)\n",
    "        self.avg_length = np.mean(eps_length)\n",
    "\n",
    "        # Calculate the estimated state value, critic target, and advantage\n",
    "        data = self.buffer.get_transition()\n",
    "        obs_tensor = torch.tensor(data['obs'], dtype = torch.float32, device='cuda' if self.CUDA_ENABLED else 'cpu')\n",
    "        rew_tensor = torch.tensor(data['rew'], dtype = torch.float32, \n",
    "                                  device='cuda' if self.CUDA_ENABLED else 'cpu').unsqueeze(dim=1)\n",
    "        nobs_tensor = torch.tensor(data['next_obs'], dtype = torch.float32, device='cuda' if self.CUDA_ENABLED else 'cpu')\n",
    "        done_tensor = torch.tensor(data['done'], dtype = torch.float32, \n",
    "                                   device='cuda' if self.CUDA_ENABLED else 'cpu').unsqueeze(dim=1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_tensor = self.value_net(obs_tensor)\n",
    "            next_val_tensor = self.value_net(nobs_tensor)\n",
    "        tar_tensor = rew_tensor + self.gamma * (1-done_tensor) * next_val_tensor\n",
    "        adv_tensor = tar_tensor - val_tensor\n",
    "\n",
    "        ## TODO - See if normalizing the advantage improvement training\n",
    "        adv_tensor = (adv_tensor - adv_tensor.mean()) / (adv_tensor.std() + 1e-8)\n",
    "\n",
    "        self.buffer.val_buf = val_tensor.cpu().numpy().squeeze()\n",
    "        self.buffer.tar_buf = tar_tensor.cpu().numpy().squeeze()\n",
    "        self.buffer.adv_buf = adv_tensor.cpu().numpy().squeeze() \n",
    "\n",
    "    def train_policy(self):\n",
    "        msg = \"Training ended with no good model found :<\"\n",
    "\n",
    "        # # Create the directory to store results\n",
    "        # self.run_number, self.hyperparam_config, self.save_path = self.create_directory()\n",
    "\n",
    "        # Training information\n",
    "        title = f\"PPO   |   Model {self.model_name}, alpha={self.alpha}, beta={self.beta}, gamma={self.gamma}\" \n",
    "        if self.VERBOSE: print(title)\n",
    "\n",
    "        self.reward_history = []                # Track the total reward per episode\n",
    "        self.val_history = {}                   # Reset the validation history\n",
    "        self.value_history = []                 # History of the v(s_0) calculated using G_t throughout training\n",
    "        self.value_est_history = []             # History of the \\hat v(s_0) calculated using the value net (baseline case)\n",
    "        self.val_time = 0                       # Time used for validation (s)\n",
    "\n",
    "        # Control of early stopping\n",
    "        consecutive_pass_count = 0\n",
    "        CONSECUTIVE_PASS_LIMIT = 3\n",
    "        EPISODE_REWARD_LIMIT = SUCCESS_CRITERIA\n",
    "        best_reward = 0\n",
    "        self.best_model_episode = None\n",
    "        performance_crit = False                # Whether desired performance is met consistently\n",
    "        train_terminated = False\n",
    "        \n",
    "\n",
    "        self.train_time_start = time.time()\n",
    "        \n",
    "        # while not train_terminated:             # Experiment level - iterate over training episodes\n",
    "        for iter in range(self.N_ITER_TRAIN):\n",
    "            self.policy_net_old.load_state_dict(self.policy_net.state_dict())\n",
    "            self.prefill_buffer()               # Generate the training data using the old policy\n",
    "\n",
    "            self.reward_history.append(self.avg_length)\n",
    "\n",
    "            # Periodic data logger\n",
    "            if iter % self.LOG_PERIOD == 0 and self.VERBOSE:\n",
    "                printout_msg = f\"Iteration {iter:3d} : Mean V(s_0) {self.avg_return:7.2f} | Mean Length {self.avg_length:7.2f}\"\n",
    "                print(printout_msg, end='\\r')\n",
    "\n",
    "            # Early stopping condition\n",
    "            if self.avg_length >= EPISODE_REWARD_LIMIT:\n",
    "                # Evaluate the current good policy and record the validation time\n",
    "                self.val_time_start = time.time()\n",
    "                test_reward, _ = self.policy_eval(self.env_val, 20, verbose=False)\n",
    "                self.val_time += time.time() - self.val_time_start\n",
    "\n",
    "                self.val_history[iter] = [self.avg_length, test_reward]\n",
    "\n",
    "                if test_reward >= best_reward:           # Set the new best reward\n",
    "                    best_reward = test_reward\n",
    "                    self.save_model()\n",
    "                    self.best_model_iteration = iter\n",
    "                    msg = f\"Training terminated due to iteration limit, best model saved at episode {self.best_model_iteration:5d}\"\n",
    "                if test_reward > EPISODE_REWARD_LIMIT: \n",
    "                    consecutive_pass_count += 1\n",
    "                else: consecutive_pass_count = 0\n",
    "            else:\n",
    "                consecutive_pass_count = 0\n",
    "\n",
    "            # Performance criteria - if good results for several episodes => training performance satisfied and terminate early\n",
    "            if consecutive_pass_count >= CONSECUTIVE_PASS_LIMIT:\n",
    "                self.save_model()\n",
    "                self.best_model_iteration = iter\n",
    "                performance_crit = True \n",
    "                msg = f\"Early termination at iteration {self.best_model_iteration:5d}, desired performance reached\"\n",
    "\n",
    "            # Checking for early training termination or truncation\n",
    "            train_terminated = performance_crit\n",
    "            if train_terminated:\n",
    "                break\n",
    "            \n",
    "            # Loop through epochs\n",
    "            for epoch in range(self.N_EPOCH):       # Iterate over multiple epochs of training data\n",
    "                indices = np.arange(self.buffer_size)\n",
    "                np.random.shuffle(indices)\n",
    "\n",
    "                # Iterate over mini batches\n",
    "                for start in range(0, self.buffer_size, self.batch_size):\n",
    "                    end = start + self.batch_size\n",
    "                    batch_idx = indices[start:end]\n",
    "                    \n",
    "                    batch_data = self.buffer.get_data()\n",
    "                    obs_batch = torch.tensor(batch_data['obs'][batch_idx], dtype = torch.float32, device='cuda' if self.CUDA_ENABLED else 'cpu')\n",
    "                    act_batch = torch.tensor(batch_data['act'][batch_idx], dtype = torch.float32, device='cuda' if self.CUDA_ENABLED else 'cpu')\n",
    "                    logp_old = torch.tensor(batch_data['logp'][batch_idx], dtype = torch.float32, device='cuda' if self.CUDA_ENABLED else 'cpu')\n",
    "                    adv_batch = torch.tensor(batch_data['adv'][batch_idx], dtype = torch.float32, device='cuda' if self.CUDA_ENABLED else 'cpu')\n",
    "                    tar_batch = torch.tensor(batch_data['tar'][batch_idx], dtype = torch.float32, \n",
    "                                             device='cuda' if self.CUDA_ENABLED else 'cpu').unsqueeze(dim=1)\n",
    "\n",
    "                    action_mu, action_std = self.policy_net(obs_batch)\n",
    "                    action_dist = Normal(action_mu, action_std)\n",
    "                    logp = action_dist.log_prob(act_batch)\n",
    "\n",
    "                    ratios = torch.exp(logp - logp_old)\n",
    "                    clip_ratios = torch.clamp(ratios, 1 - self.eps, 1 + self.eps)\n",
    "                    actor_loss = -torch.mean(torch.min(ratios * adv_batch, clip_ratios * adv_batch))\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    actor_loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    # Optimize the critic network\n",
    "                    val_tensor = self.value_net(obs_batch)\n",
    "                    critic_loss = nn.MSELoss()(tar_batch, val_tensor)\n",
    "                    self.value_optimizer.zero_grad()\n",
    "                    critic_loss.backward()\n",
    "                    self.value_optimizer.step()\n",
    "\n",
    "        self.train_time = time.time() - self.train_time_start\n",
    "\n",
    "        print(f\"\\nTotal runtime - {self.train_time:5.2f}\")\n",
    "        print(msg)\n",
    "        return\n",
    "\n",
    "    def record(self):\n",
    "        # Load the best policy net parameter from the experiment\n",
    "        if self.model_path:\n",
    "            self.load_model()\n",
    "\n",
    "        # Average test reward of the resulting policy - if best policy does not exists use the last one\n",
    "        self.env_test = gym.make('CartPole-v1')\n",
    "        print('Testing the best policy network performance')\n",
    "        reward_mean, reward_stdev = self.policy_eval(self.env_test, n_iter_test=500, verbose=True)\n",
    "        print(f\"\\nValidation average reward {reward_mean:4.2f} (SD = {reward_stdev:4.2f})\")\n",
    "\n",
    "        # Store the validation history and average test reward in the param_config JSON file\n",
    "        param_path = os.path.join(self.save_path, 'param_config.json')\n",
    "        if os.path.exists(param_path):\n",
    "            with open(param_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "        else:\n",
    "            data = {}\n",
    "\n",
    "        data['runtime'] = self.train_time\n",
    "        data['valtime'] = self.val_time\n",
    "        data['best_model_at'] = self.best_model_iteration\n",
    "        data['val_history'] = self.val_history\n",
    "        data['test_result'] = [reward_mean, reward_stdev]\n",
    "\n",
    "        with open(param_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "        # Plot\n",
    "        self.plot_reward_hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3920388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_experiment = PPO(MODEL_NAME, model_registry,\n",
    "                     ALPHA, BETA, GAMMA, EPS,\n",
    "                     BUFFER_SIZE, BATCH_SIZE, \n",
    "                     N_ITERATION, N_EPOCH)\n",
    "PPO_experiment.prefill_buffer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d9c01477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'obs': array([[ 3.1323014e-03, -3.2982768e-03,  4.9901577e-03, -6.6812392e-03],\n",
      "       [-1.6027547e-03,  7.8092348e-03, -2.4108042e-01,  5.5634773e-01],\n",
      "       [-1.6208224e-02,  4.1146375e-02, -4.8891565e-01,  1.1091479e+00],\n",
      "       ...,\n",
      "       [ 4.8272859e-02, -1.1668183e-01,  6.9952816e-01, -1.6535934e+00],\n",
      "       [ 7.1115427e-02, -1.7151538e-01,  4.4475520e-01, -1.1078923e+00],\n",
      "       [-7.2720787e-03,  6.1810240e-03, -2.0019398e-03,  4.3387292e-03]],\n",
      "      dtype=float32), 'act': array([-0.7422152 , -0.7557305 ,  0.33149806, ..., -0.7736465 ,\n",
      "       -0.3272256 ,  0.2887293 ], dtype=float32), 'rew': array([1., 1., 1., ..., 1., 0., 1.], dtype=float32), 'next_obs': array([[-0.00160275,  0.00780923, -0.24108042,  0.5563477 ],\n",
      "       [-0.01620822,  0.04114638, -0.48891565,  1.1091479 ],\n",
      "       [-0.03348408,  0.07993656, -0.37605894,  0.84196925],\n",
      "       ...,\n",
      "       [ 0.07111543, -0.17151538,  0.4447552 , -1.1078923 ],\n",
      "       [ 0.08693998, -0.21333702,  0.34750023, -0.9933647 ],\n",
      "       [-0.00544869,  0.00209615,  0.09292189, -0.20643227]],\n",
      "      dtype=float32), 'logp': array([-1.1787884 , -1.3621368 , -0.47164536, ..., -1.0526668 ,\n",
      "       -0.5139536 , -0.5229416 ], dtype=float32), 'done': array([False, False, False, ..., False,  True, False]), 'val': array([ 0.01719627, -0.04620116, -0.1037503 , ...,  0.28674442,\n",
      "        0.20931207,  0.01127233], dtype=float32), 'adv': array([ 0.07147332,  0.09285286,  0.36901784, ...,  0.01625892,\n",
      "       -3.7438529 ,  0.4029632 ], dtype=float32), 'tar': array([0.9542608 , 0.8972872 , 0.92271626, ..., 1.2072189 , 0.        ,\n",
      "       1.0479383 ], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "buffer_data = PPO_experiment.buffer.get_data()\n",
    "print(buffer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dd99da",
   "metadata": {},
   "source": [
    "## PPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "040c4874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO   |   Model PPO_MLP_v0, alpha=1e-05, beta=0.001, gamma=0.99\n",
      "Iteration  42 : Mean V(s_0)    9.25 | Mean Length   10.81\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m PPO_experiment \u001b[38;5;241m=\u001b[39m PPO(MODEL_NAME, model_registry,\n\u001b[0;32m      2\u001b[0m                      ALPHA, BETA, GAMMA, EPS,\n\u001b[0;32m      3\u001b[0m                      BUFFER_SIZE, BATCH_SIZE, \n\u001b[0;32m      4\u001b[0m                      N_ITERATION, N_EPOCH)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mPPO_experiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[55], line 423\u001b[0m, in \u001b[0;36mPPO.train_policy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    420\u001b[0m actor_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39mmin(ratios \u001b[38;5;241m*\u001b[39m adv_batch, clip_ratios \u001b[38;5;241m*\u001b[39m adv_batch))\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 423\u001b[0m \u001b[43mactor_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    426\u001b[0m \u001b[38;5;66;03m# Optimize the critic network\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\caomi\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\caomi\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\caomi\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PPO_experiment = PPO(MODEL_NAME, model_registry,\n",
    "                     ALPHA, BETA, GAMMA, EPS,\n",
    "                     BUFFER_SIZE, BATCH_SIZE, \n",
    "                     N_ITERATION, N_EPOCH)\n",
    "\n",
    "PPO_experiment.train_policy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
