{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "177b3eb5",
   "metadata": {},
   "source": [
    "# Inverted Pendulum DQN - Grid Search\n",
    "\n",
    "This script is an extension to the inverted pendulum DQN algorithm in inv_pend.ipynb. This scripts automates the grid search in a hyperparameter space to explore the best performance of DQN.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5928a33",
   "metadata": {},
   "source": [
    "TODO\n",
    "- [ ] Package functions related to the experiment in a `GridSearch` class\n",
    "    - **Attributes:**\n",
    "        - Hyperparameters - model arch, lr , buffer_size, min_replay_szie, target_update_freq, gamma, eps_start, eps_end, eps_decay, episode_train, and batch_size\n",
    "        - The simulation environment\n",
    "    - **Methods:** \n",
    "        - `create_directory()` - for storing the training results for each hyperparameter configuration\n",
    "        - `eps_greedy_policy()` - for picking out an action given the observation\n",
    "        - `DQN_train()` - for training a Q network from simulation\n",
    "\n",
    "- [ ] Save the model (parameters and architecture) following training in the grid searchd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d299b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random                       # To draw a random batch of samples form the replay buffer\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os                                               # For saving models and training results\n",
    "from datetime import datetime                           # For creating the directory of each training run\n",
    "import json                                             # For storing training parameters during each run\n",
    "import re                                               # For checking the latest trial #\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "from collections import deque       # replay buffer is a double ended queue that allows elements to be added either to the end or the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3ab4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc95fe8",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978d2147",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet_MLP(nn.Module):\n",
    "    ''' A QNetwork class that dynamically initialize an MLP Q Net'''\n",
    "    def __init__(self,input_dim,output_dim,hidden_layer = [64,32]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        for size in hidden_layer:\n",
    "            self.layers.append(nn.Linear(self.input_dim, size))\n",
    "            self.input_dim = size\n",
    "        \n",
    "        self.layers.append(nn.Linear(self.input_dim,self.output_dim))\n",
    "\n",
    "        self.cuda()\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        for layer in self.layers[:-1]:\n",
    "            input_data = torch.relu(layer(input_data))\n",
    "        return self.layers[-1](input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22aa2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(model_id: int,\n",
    "                     lr: float, \n",
    "                     gamma: float,\n",
    "                     epsilon_decay: int,\n",
    "                     batch_size: int, \n",
    "                     buffer_size: int,\n",
    "                     target_update_freq: int):\n",
    "    ''' Function that creates directory to save model state_dict, architecture, training configuration, and history\n",
    "\n",
    "    Parameters: \n",
    "    ------------\n",
    "    (hyperparameters for differentiating between different directory)\n",
    "    \n",
    "    lr : float\n",
    "        the learning rate to optimize the Q network\n",
    "    gamma : float \n",
    "        the discount rate in Q learning\n",
    "    epsilon_decay : integer\n",
    "        the amount of episode over which the exploratory rate (epsilon) decays\n",
    "    batch_size : integer\n",
    "        number of experience drawn from replay buffer to train the behaviour network\n",
    "    buffer_size : integer\n",
    "        the number of samples in the replay buffer at a time\n",
    "    target_udpate_freq : integer\n",
    "        the amount of step count during the training process before updating the target Q net (loading the parameters of the behaviour net onto the target Q Net)\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    ------------\n",
    "    name_codified : str\n",
    "        the shortened name for the current experiment \n",
    "    hyperparameters_codified : str\n",
    "        the shortened string of hyperparameter configuration\n",
    "    OUTPUT_DIR : path\n",
    "        the directory to which the training results and model (state_dict and architecture) will be saved\n",
    "    '''\n",
    "    timestamp = datetime.now().strftime(\"%y%m%d_%H%M\")\n",
    "\n",
    "    BASE_DIR = os.getcwd()\n",
    "    RESULT_DIR = os.path.join(BASE_DIR, \"inv_pend_results\")\n",
    "    os.makedirs(RESULT_DIR, exist_ok=True)      # Create the directory if one does not already exist\n",
    "\n",
    "    # Find the trial # of the latest run\n",
    "    existing_runs = [d for d in os.listdir(RESULT_DIR) if os.path.isdir(os.path.join(RESULT_DIR,d))]\n",
    "    run_numbers = [int(re.search(r'run_(\\d{5})',d).group(1)) for d in existing_runs if re.match(r'run_\\d{5}',d)]\n",
    "    trial_number = max(run_numbers,default=-1)+1\n",
    "\n",
    "    # Create a folder for the run\n",
    "    name_codified = f\"run_{trial_number:05d}\"\n",
    "    OUTPUT_DIR = os.path.join(BASE_DIR,\"inv_pend_results\",name_codified)\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)      # Create the directory\n",
    "\n",
    "    \n",
    "    # Store the training configs in JSON file\n",
    "    training_params = {\n",
    "        'model_id': model_id,\n",
    "        'lr': lr,\n",
    "        'gamma': gamma,\n",
    "        'epsilon_decay': epsilon_decay,\n",
    "        'batch_size': batch_size,\n",
    "        'buffer_size': buffer_size,\n",
    "        'target_update_freq': target_update_freq\n",
    "    }\n",
    "\n",
    "    # Append the mapping from run # to hyperparameter configuration in a JSON file inside RESULT_DIR\n",
    "    trial_to_param_path = os.path.join(RESULT_DIR,'trial_to_param.json')\n",
    "    if os.path.exists(trial_to_param_path):\n",
    "        with open(trial_to_param_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        data = {name_codified: []}\n",
    "\n",
    "    hyperparam_codified = f\"{model_id}_{lr}_{buffer_size}_{target_update_freq}_{gamma}_{epsilon_decay}_{batch_size}\"\n",
    "    hyperparam_codified_time = f\"{timestamp}_{model_id}_{lr}_{buffer_size}_{target_update_freq}_{gamma}_{epsilon_decay}_{batch_size}\"\n",
    "    data[name_codified] = hyperparam_codified_time\n",
    "\n",
    "    with open(trial_to_param_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "    # Store training parameters in each run \n",
    "    param_path = os.path.join(OUTPUT_DIR, \"param_config.json\")\n",
    "    with open(param_path, \"w\") as f:\n",
    "        json.dump({\"parameters\": training_params}, f, indent=2)\n",
    "\n",
    "    return name_codified, hyperparam_codified, OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d464d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy_policy(env, obs, epsilon: float, q_network: QNet_MLP):\n",
    "    ''' Function to take an action according to an epsilon-greedy policy and a Q-network'''\n",
    "    if np.random.random() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.tensor(obs, dtype=torch.float32, device = 'cuda').unsqueeze(0)\n",
    "            q_values = q_network(state_tensor)\n",
    "            action = q_values.argmax().item()\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef2a6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(env_test, q_network, n_episode_test = 500):\n",
    "    ''' Assess the average reward when following a q_network in a test environment with random state initialization '''\n",
    "    \n",
    "    total_reward = 0\n",
    "    for i in range(n_episode_test):\n",
    "        obs,_ = env_test.reset()\n",
    "        done = False\n",
    "        eps_reward = 0\n",
    "\n",
    "        while not done:                 # Step thorugh the episode\n",
    "            action = eps_greedy_policy(env_test, obs, epsilon=0, q_network=q_network)\n",
    "            next_obs, reward, term, trunc, _ = env_test.step(action)\n",
    "\n",
    "            eps_reward += reward\n",
    "\n",
    "            obs = next_obs\n",
    "            done = term or trunc\n",
    "    \n",
    "        total_reward += eps_reward\n",
    "    average_reward = total_reward / n_episode_test\n",
    "\n",
    "    return average_reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f086ecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(q_network: nn.Module, optimizer: torch.optim.Optimizer, save_path):\n",
    "    ''' Function to save the model and optimizer state_dict for inference or continued training '''\n",
    "    torch.save({\n",
    "        'model_state_dict': q_network.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, os.path.join(save_path, 'q_network_checkpoint.pth'))\n",
    "\n",
    "def load_model(q_network: nn.Module, model_path):\n",
    "    checkpoint = torch.load(model_path)\n",
    "    q_network.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45dba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN_train(env: gym.Env, env_test: gym.Env,\n",
    "              q_network: nn.Module, target_net: nn.Module, optimizer: torch.optim.Optimizer, \n",
    "              replay_buffer,\n",
    "              target_update_freq,\n",
    "              gamma,\n",
    "              eps_start, eps_end, eps_decay,\n",
    "              episode_train,\n",
    "              batch_size, \n",
    "              save_path,\n",
    "              seed = 42):\n",
    "    ''' Function to train a policy for a set of hyperparameters '''\n",
    "\n",
    "    msg = \"Training ended, no good model found!\"\n",
    "\n",
    "    reward_history = np.zeros(episode_train)\n",
    "    epsilon = eps_start\n",
    "    step_count = 0\n",
    "    episode = 0\n",
    "    target_network_update_count = 0\n",
    "\n",
    "    # Control of early stopping\n",
    "    consecutive_pass_count = 0           # Number of consecutive episodes where performance exceeds a threshold\n",
    "    CONSECUTIVE_PASS_LIMIT = 3          # No. of consecutive episodes with higher performance than reward limit\n",
    "    EPISODE_REWARD_LIMIT = 450\n",
    "    best_reward = 0\n",
    "    performance_crit = False\n",
    "    train_terminated = False\n",
    "    val_history = {}                    # Monitor which episode had a validation run, the train reward, and the validation (test) reward \n",
    "\n",
    "    while not train_terminated:     # Experiment level - loop through episodes\n",
    "        obs, _ = env.reset(seed=seed)\n",
    "        eps_rewards = 0\n",
    "    \n",
    "        while True:                 # Episode level - loop through steps\n",
    "            action = eps_greedy_policy(env, obs, epsilon, q_network)\n",
    "\n",
    "            # Interact with the environment\n",
    "            next_obs, reward, term, trunc, _ = env.step(action)\n",
    "            done = term or trunc\n",
    "            replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "            obs = next_obs\n",
    "            eps_rewards += reward\n",
    "            step_count += 1\n",
    "\n",
    "            # Train the Q-net using a batch of samples from the experience replay\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch = random.sample(replay_buffer, batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                states =        torch.tensor(np.array(states),      dtype = torch.float32,  device='cuda')           # Shape of (BATCH_SIZE, 4). Data structure of [[p1 v1 theta1 dtheta1], [p2 v2 theta2 dtheta2], ...]\n",
    "                actions =       torch.tensor(actions,               dtype = torch.long,     device='cuda').unsqueeze(1)        # Must unsqueeze to have shape (BATCH_SIZE, 1). From [a1, a2, ...] to [[a1], [a2], ... [a_BATCH_SIZE]]\n",
    "                rewards =       torch.tensor(rewards,               dtype = torch.float32,  device='cuda').unsqueeze(1)\n",
    "                next_states =   torch.tensor(np.array(next_states), dtype = torch.float32,  device = 'cuda')\n",
    "                dones =         torch.tensor(dones,                 dtype = torch.float32,  device='cuda').unsqueeze(1)\n",
    "                \n",
    "                # Compute targets using target network Q(s',a',w_i^-)\n",
    "                with torch.no_grad():\n",
    "                    target_q_values = target_net(next_states)       # Find a batch of Q(s',a',w_i^-) from the batch of next_states\n",
    "                    max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]\n",
    "                    targets = rewards + gamma * max_target_q_values * (1 - dones)       # (1-dones) to avoid over estimating the value of terminal states. The target should only be the reward in the terminal states\n",
    "            \n",
    "                # Compute the current Q values for the actions taken Q(s,a,w_i)\n",
    "                q_values = q_network(states).gather(1, actions)         # obtain the q_values associated to the actual action taken in each sample\n",
    "\n",
    "                # Update the parameters of the behaviour q_network\n",
    "                loss = nn.MSELoss()(q_values, targets)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Periodically update the target network by loading the weights from the behavior network\n",
    "            if step_count % target_update_freq == 0:\n",
    "                target_network_update_count += 1\n",
    "                target_net.load_state_dict(q_network.state_dict())\n",
    "\n",
    "            if done:        # End of a training episode\n",
    "                break\n",
    "\n",
    "        # Decay epsilon after an episode\n",
    "        epsilon = max(eps_end, epsilon - (eps_start - eps_end)/eps_decay)\n",
    "\n",
    "        reward_history[episode] = eps_rewards\n",
    "        # print(f\"Episode {episode:5d}: Total reward = {eps_rewards:5.1f}, Epsilon = {epsilon:.3f}, Step count = {step_count:5d}, Target update count = {target_network_update_count:3d}\")\n",
    "        # print(f\"Average reward {sum(reward_history[:episode])/(episode+1):.2f}\")\n",
    "\n",
    "        \n",
    "        if episode % 10 == 0:                   # print progress periodically\n",
    "            print(f\"Episode {episode:5d}: Total reward = {eps_rewards:5.1f}, Epsilon = {epsilon:.3f}\", end = \"\\r\")\n",
    "\n",
    "        # Early stopping condition\n",
    "        if eps_rewards >= EPISODE_REWARD_LIMIT:\n",
    "            test_reward = policy_eval(env_test, q_network, 100)\n",
    "            val_history[episode] = [eps_rewards, test_reward]\n",
    "\n",
    "            if test_reward >= best_reward:           # Set the new best reward\n",
    "                best_reward = test_reward\n",
    "                save_model(q_network, optimizer, save_path)\n",
    "                msg = f\"Training terminated due to episode limit, best model saved at episode {episode:5d}\"\n",
    "            if test_reward > EPISODE_REWARD_LIMIT: \n",
    "                consecutive_pass_count += 1\n",
    "            else: consecutive_pass_count = 0\n",
    "        else:\n",
    "            consecutive_pass_count = 0\n",
    "            \n",
    "        # Performance criteria - if good results for several episodes => training performance satisfied and terminate early\n",
    "        if consecutive_pass_count >= CONSECUTIVE_PASS_LIMIT:\n",
    "            performance_crit = True \n",
    "            msg = f\"Early termination at episode {episode:5d}, desired performance reached\"\n",
    "\n",
    "\n",
    "        episode += 1\n",
    "\n",
    "        # Checking for early training termination or truncation\n",
    "        train_terminated = (episode >= episode_train) or (performance_crit)\n",
    "    print(\"\\n\")\n",
    "    return reward_history, val_history, msg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b9ed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for EMA filters and plotting data\n",
    "def EMA_filter(reward: list, alpha):\n",
    "    ''' Function that runs an exponential moving average filter along a datastream '''\n",
    "    output = np.zeros(len(reward)+1)\n",
    "    output[0] = reward[0]\n",
    "    for idx, item in enumerate(reward):\n",
    "        output[idx+1] = (1 - alpha) * output[idx] + alpha * item\n",
    "    \n",
    "    return output\n",
    "\n",
    "def plot_reward_hist(reward_history: list, hyperparam_config: str, save_path = None, alpha = 0.1):\n",
    "    ''' Function that plots the reward and filtered reward per episode, then saves the plot in a specified save directory'''\n",
    "    n_episodes= len(reward_history)\n",
    "    episodes = range(n_episodes)\n",
    "    filtered_reward_hist = EMA_filter(reward_history, alpha)\n",
    "\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.plot(episodes, reward_history[:n_episodes], color = \"blue\")\n",
    "    plt.plot(episodes, filtered_reward_hist[:n_episodes], color = \"red\")\n",
    "    plt.title(f'Total reward per episode - {hyperparam_config}')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend(['Total reward','Filtered reward'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(os.path.join(save_path,'reward_history.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58d19d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script test --no-raise-error\n",
    "# Test creating the Q network with dynamic hidden layer when specifying the list of hidden nodes\n",
    "\n",
    "Q_net = QNet_MLP(obs_space,action_space,[64,32])\n",
    "summary(Q_net, (obs_space,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a80f799",
   "metadata": {},
   "source": [
    "# Parameter Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d83409",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_registry = {\n",
    "    'DQN_MLP_v0': {\n",
    "        'class': QNet_MLP,\n",
    "        'config': [64,32]\n",
    "    },\n",
    "    'DQN_MLP_v1': {\n",
    "        'class': QNet_MLP,\n",
    "        'config': [32,32]\n",
    "    },\n",
    "    'DQN_MLP_v2': {\n",
    "        \n",
    "        'class': QNet_MLP,\n",
    "        'config': [32,16]\n",
    "    },\n",
    "    'DQN_MLP_v3': {\n",
    "        'class': QNet_MLP,\n",
    "        'config': [16,16]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e314f87",
   "metadata": {},
   "source": [
    "The block below is used for testing the model creation automation (Uncomment the first line to run the test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43edbf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script test --no-raise-error\n",
    "for model in model_registry:\n",
    "    Q_net = QNet_MLP(obs_space, action_space, model_registry[model]['config'])\n",
    "    # print(model_registry[model]['config'])\n",
    "    print(Q_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060fc8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual parameter grid\n",
    "param_grid = {\n",
    "    'MODEL': [model for model in model_registry],\n",
    "    'LR': [5e-4, 1e-3, 5e-3, 1e-2],\n",
    "    \"BUFFER_SIZE\": [1000, 5000, 10000],\n",
    "    \"MIN_REPLAY_SIZE\": [1000],\n",
    "    \"TARGET_UPDATE_FREQ\": [1000, 5000, 10000],\n",
    "\n",
    "    \"GAMMA\": [0.90, 0.95, 0.98],\n",
    "    \"EPSILON_START\": [1.0],\n",
    "    \"EPSILON_END\": [0.1],\n",
    "    \"EPSILON_DECAY\": [1000, 5000, 10000],\n",
    "\n",
    "    \"EPISODE_TRAIN\": [5000],                # training truncation criteria\n",
    "    \"BATCH_SIZE\": [32, 64, 128]\n",
    "}\n",
    "\n",
    "success_criteria = 450                      #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bf3c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparam grid - vary one at a time\n",
    "param_grid = {\n",
    "    'MODEL': ['DQN_MLP_v0'],\n",
    "    'LR': [5e-4],\n",
    "    \"BUFFER_SIZE\": [5000],\n",
    "    \"MIN_REPLAY_SIZE\": [1000],\n",
    "    \"TARGET_UPDATE_FREQ\": [1000],\n",
    "\n",
    "    \"GAMMA\": [0.90, 0.95, 0.98],\n",
    "    \"EPSILON_START\": [1.0],\n",
    "    \"EPSILON_END\": [0.1],\n",
    "    \"EPSILON_DECAY\": [5000],\n",
    "\n",
    "    \"EPISODE_TRAIN\": [5000],                # training truncation criteria\n",
    "    \"BATCH_SIZE\": [32]\n",
    "}\n",
    "\n",
    "success_criteria = 450                      #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c535ea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Simplified param grid to test functionality\n",
    "# param_grid = {\n",
    "#     # 'MODEL': [model for model in model_registry],\n",
    "#     'MODEL': ['DQN_MLP_v0'],\n",
    "#     'LR': [5e-4],\n",
    "#     \"BUFFER_SIZE\": [5000],\n",
    "#     \"MIN_REPLAY_SIZE\": [1000],\n",
    "#     \"TARGET_UPDATE_FREQ\": [1000],\n",
    "\n",
    "#     \"GAMMA\": [0.95],\n",
    "#     \"EPSILON_START\": [1.0],\n",
    "#     \"EPSILON_END\": [0.1],\n",
    "#     \"EPSILON_DECAY\": [5000],\n",
    "\n",
    "#     \"EPISODE_TRAIN\": [5000],                # training truncation criteria\n",
    "#     \"BATCH_SIZE\": [32]\n",
    "# }\n",
    "\n",
    "# success_criteria = 450                      #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bb6584",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da69956",
   "metadata": {},
   "source": [
    "Using itertools to loop through each combination of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a343ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time\n",
    "keys, values = zip(*param_grid.items())\n",
    "# keys, values = param_grid.keys(), param_grid.values()\n",
    "num_config = len(list(itertools.product(*values)))\n",
    "\n",
    "# Set fixed seed\n",
    "seed = 42\n",
    "np.random.seed(seed); random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "for idx, v in enumerate(itertools.product(*values)):\n",
    "\n",
    "    # Unpacking the hyperparameter configurations\n",
    "    config = dict(zip(keys,v))\n",
    "    MODEL_NAME = config['MODEL']\n",
    "    MODEL_CLASS = model_registry[MODEL_NAME]['class']\n",
    "    MODEL_CONFIG = model_registry[MODEL_NAME]['config']\n",
    "    match = re.search(r'v\\d+',MODEL_NAME)\n",
    "    MODEL_ID = match.group(0) if match else 404\n",
    "\n",
    "    LR = config['LR']\n",
    "    BUFFER_SIZE = config['BUFFER_SIZE']\n",
    "    MIN_REPLAY_SIZE = config['MIN_REPLAY_SIZE']\n",
    "    TARGET_UPDATE_FREQ = config['TARGET_UPDATE_FREQ']\n",
    "    GAMMA = config['GAMMA']\n",
    "    EPS_START = config['EPSILON_START']\n",
    "    EPS_END = config['EPSILON_END']\n",
    "    EPS_DECAY = config['EPSILON_DECAY']\n",
    "    EPISODE_TRAIN = config['EPISODE_TRAIN']\n",
    "    BATCH_SIZE = config['BATCH_SIZE']\n",
    "\n",
    "\n",
    "    # Re-initialize the environment\n",
    "    env = gym.make('CartPole-v1')\n",
    "    env_test = gym.make('CartPole-v1')\n",
    "    obs,_ = env.reset(seed=seed)\n",
    "\n",
    "    obs_space = env.observation_space.shape[0]\n",
    "    action_space = env.action_space.n\n",
    "\n",
    "\n",
    "    # Re-initialize the NN models\n",
    "    Q_net = MODEL_CLASS(obs_space,action_space,MODEL_CONFIG)\n",
    "    target_net = MODEL_CLASS(obs_space,action_space,MODEL_CONFIG)\n",
    "    target_net.load_state_dict(Q_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.SGD(Q_net.parameters(), lr = LR)\n",
    "    # optimizer = optim.Adam(Q_net.parameters(), lr = LR)\n",
    "\n",
    "    # Re-initialize and pre-fill the replay buffer\n",
    "    replay_buffer = deque(maxlen = BUFFER_SIZE)\n",
    "    \n",
    "    obs, _ = env.reset()\n",
    "    for _ in range(MIN_REPLAY_SIZE):\n",
    "        action = env.action_space.sample()\n",
    "        next_obs, reward, term, trunc, _ = env.step(action)\n",
    "        done = term or trunc\n",
    "\n",
    "        replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "        obs = next_obs if not done else env.reset()[0]\n",
    "\n",
    "    # Create the directory to store results\n",
    "    _, hyperparam_config, save_path = create_directory(MODEL_ID,LR,GAMMA,EPS_DECAY,BATCH_SIZE,BUFFER_SIZE,TARGET_UPDATE_FREQ)\n",
    "\n",
    "    # Training information\n",
    "    print(f'Trial {idx+1}/{num_config} - model {MODEL_NAME}, lr={LR}, buffer={BUFFER_SIZE}, target_freq={TARGET_UPDATE_FREQ}, gamma={GAMMA}, eps_decay={EPS_DECAY}, batch_size={BATCH_SIZE}')\n",
    "    \n",
    "    # Train the DQN with the given hyperparameter configuration\n",
    "    start_time = time.time()\n",
    "    reward_history, val_history, msg = DQN_train(env, env_test,\n",
    "                                    Q_net, target_net,optimizer,\n",
    "                                    replay_buffer,\n",
    "                                    TARGET_UPDATE_FREQ, GAMMA, \n",
    "                                    EPS_START, EPS_END, EPS_DECAY,\n",
    "                                    EPISODE_TRAIN,\n",
    "                                    BATCH_SIZE, \n",
    "                                    save_path,\n",
    "                                    seed=seed)\n",
    "    end_time = time.time()\n",
    "    print(f\"Runtime - {end_time - start_time:.3f}\")\n",
    "    print(msg)\n",
    "\n",
    "    # Load the best Q net parameter from the experiment\n",
    "    model_path = os.path.join(save_path,'q _network_checkpoint.pth')\n",
    "    load_model(Q_net, model_path)\n",
    "\n",
    "    # Average test reward of the resulting policy\n",
    "    env_test = gym.make('CartPole-v1')\n",
    "    average_reward = policy_eval(env_test, Q_net, n_episode_test=500)\n",
    "    print(f\"Validation average reward {average_reward:4.2f}\")\n",
    "\n",
    "    # Store the validation hisotry and average test reward in the param_config JSON file\n",
    "    param_path = os.path.join(save_path,'param_config.json')\n",
    "    if os.path.exists(param_path):\n",
    "        with open(param_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        data = {}\n",
    "    data['val_history'] = val_history\n",
    "    data['test_result'] = average_reward\n",
    "    \n",
    "    \n",
    "\n",
    "    with open(param_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    \n",
    "    # Display and save the reward history in current trial folderr \n",
    "    plot_reward_hist(reward_history, hyperparam_config, save_path)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def4e72",
   "metadata": {},
   "source": [
    "# Load and Simulate Saved Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c848fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model after training\n",
    "\n",
    "# Manually select a folder/run to load\n",
    "run_number = 'run_00012'\n",
    "\n",
    "\n",
    "# Find the paths to the param_config and model checkpoint\n",
    "RESULT_DIR = os.path.dirname(save_path)\n",
    "RUN_DIR = os.path.join(RESULT_DIR, run_number)\n",
    "MODEL_PATH = os.path.join(RUN_DIR,'q_network_checkpoint.pth')\n",
    "\n",
    "# Find the model configuration\n",
    "with open(os.path.join(RUN_DIR,\"param_config.json\"),'r') as f:\n",
    "    data = json.load(f)\n",
    "model_id = 'DQN_MLP_' + data['parameters']['model_id']\n",
    "model_config = model_registry[model_id]['config']\n",
    "\n",
    "# Create the model according to the model version in the model registry\n",
    "q_network_loaded = QNet_MLP(obs_space, action_space, model_config)\n",
    "load_model(q_network_loaded, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968924ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual simulation of the network\n",
    "env_test_visual = gym.make(\"CartPole-v1\")\n",
    "num_test = 100\n",
    "\n",
    "for episode in range(num_test):\n",
    "    obs, _ = env_test_visual.reset()\n",
    "    done = False\n",
    "    eps_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = eps_greedy_policy(env_test_visual, obs, epsilon = 0, q_network=q_network_loaded)\n",
    "        next_obs, reward, term, trunc, _ = env_test_visual.step(action)\n",
    "\n",
    "        done = term or trunc\n",
    "        eps_reward += reward\n",
    "        obs = next_obs\n",
    "    \n",
    "    print(f\"Reward from episode {episode:3d} is {eps_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
