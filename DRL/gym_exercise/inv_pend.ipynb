{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e0cac5",
   "metadata": {},
   "source": [
    "# Inverted Pendulum using DQN\n",
    "\n",
    "When moving on from discrete environments, such as grid world or black jack, the amount of state in a continous environment renders traditional tabular methods (Monte Carlo and TD learning) intractable (at least without any modification to the problem formulation). DQN method addresses this problem by using a value function approximator, essentially replacing the Q-table of explicit state-action value with a parameterized function $q^\\pi_{\\vec w}(s,a,\\vec w)$. The vector $\\vec w$ stores the weights or parameters of this approximating function.\n",
    "\n",
    "The motivation of using value function approximator is clear, but where does it fit into the RL picture?\n",
    "- Value function approximator is a direct mapping between the states (features) to the state-action value $Q(s,\\cdot)$ \n",
    "- As such, the approximator function fits in the policy evaluation step of the RL training process. We train this function by minimizing the loss between the approximated value function and the actual value function $ s.t. \\enspace q^\\pi_{\\vec w}(s,a,\\vec w) \\approx q^*$\n",
    "- However, the true action value function $q^*$ is unknown and must be learnt from the interaction between the agent and its environment. We must employ Q-learning to approximate the true action value function, toward which we will update the parameterized value function $q^\\pi_{\\vec w}(s,a,\\vec w)$. The update is as follow\n",
    "\n",
    "**The DQN algorithm**\n",
    "1. From a state $s$, take action $a$ using $\\epsilon$-greedy policy, observe the reward $r$ and next state $s'$\n",
    "2. Store the transition tuple $(s, a, r, s')$ in a replay memory $\\mathcal{D}$. This replay memory is a buffer that stores the most recent transition tuples\n",
    "3. From this replay memory, sample batches of $(s_t, a_t, r_{t+1}, s_{t+1})$ randomly for training the Q-function approximator\n",
    "4. Adjust the parameters $\\vec w$ by optimizing the mean squared error (MSE) between the Q network and the Q-learning target using Q-learning\n",
    "$$ \\mathcal{L}_i (\\vec w_i) = \\mathbb{E}_{(s,a,r,s') \\sim \\mathcal{D}} \\bigg[ \\Big(r + \\gamma \\max_{a'}Q(s',a',\\vec w^-_i) - Q(s,a,\\vec w_i) \\Big)^2\\bigg]$$ \n",
    "\n",
    "When applying stochastic gradient descent, the update at every step is reduced as follows (the math found in the GoodNote notebook):\n",
    "\n",
    "$$\\Delta \\vec w = \\alpha \\Big(r + \\gamma \\max_{a'} \\underbrace{Q(s', a', \\vec{w}_i^-)}_{\\text{target}} - \\underbrace{Q(s,a,\\vec w_i)}_{\\text{behaviour}} \\Big) \\vec x(s,a)$$\n",
    "\n",
    "\n",
    "**Notes:** \n",
    "\n",
    "There are two mechanisms introduced in this paper that allow for stable training, one is experience replay, and the other is training two separate methods.\n",
    "- **Experience replay** - this decorrelates the experience tuples (s,a,r,s') thus follow the i.i.d assumption for training a neural network\n",
    "- **Two Q-networks**, a target network and a behaviour network. The parameters $\\vec w'$ of the target network $Q(s,a,\\vec w')$ are only updated infrequently to maintain a stable target. The parameters $\\vec w$ of the behaviour network $Q(s,a,\\vec w)$ are updated every policy improvement iteration to move toward this target network. The frequency of updating the target network is annother hyperparameter to tune. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03179d1e",
   "metadata": {},
   "source": [
    "## Question to self\n",
    "1. What does it mean to apply stochastic gradient descent to simplify the loss function? What are the implications?\n",
    "- Every policy evaluation iteration, a random sample, or batch of samples, is used for updating the state-action value instead of every single experience stored in the replay buffer. Recall that the random sampling from the replay buffer de-correlates the experience and enforces the i.i.d. assumption\n",
    "- SGD is used on a small batch of sample to reduce the variance inherent in the different experience in the replay buffer. Despite the noise and bias of this method, the performance of the Q networks improves.\n",
    "\n",
    "\n",
    "</br>\n",
    "\n",
    "2. Would it be beneficial to formulate the Q network such that the input is the state vector and the outputs are the Q-values associated with each possible action from the state. As such, one can just take the max value from the target Q network and index into the behaviour network for the corresponding action a?\n",
    "- That is the common approach for approximating the action value function, outputting Q(s) for all a, then grabbing the maximum value\n",
    " \n",
    "</br>\n",
    "\n",
    "3. At first do I have to run the agent several times using the initial $\\epsilon$-greedy policy to fill the memory buffer with training samples?\n",
    "- Yes, and then pop in new experience at the end, push the oldest experience in the replay buffer out.\n",
    "\n",
    "</br>\n",
    "\n",
    "4. How often does the behaviour target network updated?\n",
    "\n",
    "- At every step in an episode of the training process, the behaviour network is used for two different purposes:\n",
    "    - Firstly, the behaviour network governs the trajectory of the agent at every step. \n",
    "    - Secondly, the behaviour Q network is used to predict (or infer) the action value for the <s,a> pairs drawn from a batch of experience (s,a,r,s'). These value is compared to the discounted maximum action value at state s', $\\max Q(s',\\cdot)$, according to the **target network**. \n",
    "\n",
    "- The behaviour network prediction of $Q(s,a)$ and the target network's label output of $r+\\gamma \\max Q(s',a')$ are used to compute a loss function. The gradient of this loss function is used for adjusting the parameters of the behaviour network.\n",
    "- After the agent has taken a number of step, the parameters (or weights) or the behaviour network is loaded onto the target network.\n",
    "\n",
    "</br>\n",
    "\n",
    "5. How can I scale this environment up to have continuous action instead of just left/right?\n",
    "- To my understanding, actuating the cart to the left or right at similar output level (voltage) at instantaneous moments throughout the control process is good enough for modulating the speed of the cart. Think of PWMs where the voltage is just switch at a certain duty cycle to simulate different voltage level.\n",
    "- It is possible to have continuous action space. Since the Q network output is the state-action value Q(s,a) at each state s and action a, we know how good each action is in a state. The Q values can then be transformed into a proportional constant to control how much to move left or right (instead of just a binary left/right). This essentially means that the Q-value output of this network is not simply used for selecting the maximizing action at each state but also for governing the magnitude of cart actuation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d20579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random                       # To draw a random batch of samples form the replay buffer\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque       # replay buffer is a double ended queue that allows elements to be added either to the end or the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dafb2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate several episodes of the cart pole environment (random policy on a human render mode)\n",
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "\n",
    "state, _ = env.reset(); t = 1\n",
    "for i in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, term, trunc, info = env.step(action)\n",
    "    print(f\"Time {t:2d}  |  s_t {state}  |  a_t {action:2d}  |  s_t+1 {next_state}  |  reward {reward:.2f}  |  terminated {term:2}  |  {info}\")\n",
    "    state = next_state; t += 1\n",
    "    if term or trunc:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2ec82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant definitions\n",
    "LR = 1e-3\n",
    "BUFFER_SIZE = 5000                  # size of the experience replay buffer\n",
    "MIN_REPLAY_SIZE = 1000              # The number of experience to sample before starting training\n",
    "TARGET_UPDATE_FREQ = 1000           # the number of steps before updating the target network\n",
    "\n",
    "GAMMA = 0.95                        # Discount factor for return calculation\n",
    "EPSILON_START = 1.0                 # exploration rate in stochastic policy\n",
    "EPSILON_END = 0.1                   # minimum exploration rate at the end of decay\n",
    "EPSILON_DECAY = 5000                # epsilon decay rate\n",
    "\n",
    "EPISODE_TRAIN = 5000              # number of episodes to train the Q-network\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ce285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Q-network architecture\n",
    "class QNetwork(nn.Module):\n",
    "    ''' A QNetwork class that initialize an MLP neural Q network with two hidden layers, each with 128 nodes '''\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)                       # The regression output are the state values of Q(s,a=left) and Q(s,a=right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874662e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically define Q-network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8115dd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment and the Q network\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_space = env.observation_space.shape[0]        # State space of cart pos & vel, and pendulum angular pos and vel. Each component is a continuous value\n",
    "action_space =  env.action_space.n                   # Action space of left or right\n",
    "\n",
    "q_network = QNetwork(state_space,action_space)\n",
    "target_network = QNetwork(state_space,action_space)\n",
    "target_network.load_state_dict(q_network.state_dict())\n",
    "target_network.eval()                               # target network is only used for evaluation/inference and not trained\n",
    "\n",
    "optimizer = optim.SGD(q_network.parameters(), lr = LR)\n",
    "replay_buffer = deque(maxlen=BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be9e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill replay buffer with some initial samples for training (random policy)\n",
    "obs, _ = env.reset()\n",
    "for _ in range(MIN_REPLAY_SIZE):\n",
    "    action = env.action_space.sample()\n",
    "    next_obs, reward, term, trunc, _ = env.step(action)\n",
    "    done = term or trunc\n",
    "\n",
    "    replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "    obs = next_obs if not done else env.reset()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af1a257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Model Training loop\n",
    "reward_history = np.zeros(EPISODE_TRAIN)\n",
    "epsilon = EPSILON_START\n",
    "step_count = 0\n",
    "target_network_update_count = 0\n",
    "\n",
    "for episode in range(EPISODE_TRAIN):\n",
    "    obs, _ = env.reset()\n",
    "    eps_rewards = 0\n",
    "    \n",
    "\n",
    "    while True:\n",
    "        # Epsilon-greedy policy to select action\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():       # Doing inference so no need to track operations\n",
    "                state_tensor = torch.FloatTensor(obs).unsqueeze(0)  # unsqueeze to fake the sample (batch) dimension\n",
    "                q_values = q_network(state_tensor)\n",
    "                action = q_values.argmax().item()                   # index of the maximum output (action)\n",
    "        \n",
    "        # Interact with the environment\n",
    "        next_obs, reward, term, trunc, _ = env.step(action)\n",
    "        done = term or trunc\n",
    "        replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "        obs = next_obs\n",
    "        eps_rewards += reward\n",
    "        step_count += 1\n",
    "\n",
    "        # Sample a batch and update the network\n",
    "        if len(replay_buffer) >= BATCH_SIZE:\n",
    "            batch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.FloatTensor(states)                      # Shape of (BATCH_SIZE, 4). Data structure of [[p1 v1 theta1 dtheta1], [p2 v2 theta2 dtheta2], ...]\n",
    "            actions = torch.LongTensor(actions).unsqueeze(1)        # Must unsqueeze to have shape (BATCH_SIZE, 1). From [a1, a2, ...] to [[a1], [a2], ... [a_BATCH_SIZE]]\n",
    "            rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "            next_states = torch.FloatTensor(next_states)\n",
    "            dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "\n",
    "            # Compute targets using target network Q(s',a',w_i^-)\n",
    "            with torch.no_grad():\n",
    "                target_q_values = target_network(next_states)       # Find a batch of Q(s',a',w_i^-) from the batch of next_states\n",
    "                max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]\n",
    "                targets = rewards + GAMMA * max_target_q_values * (1 - dones)       # (1-dones) to avoid over estimating the value of terminal states. The target should only be the reward in the terminal states\n",
    "\n",
    "            # Compute the current Q values for the actions taken Q(s,a,w_i)\n",
    "            q_values = q_network(states).gather(1, actions)         # obtain the q_values associated to the actual action taken in each sample\n",
    "\n",
    "            # Update the parameters of the behaviour q_network\n",
    "            ## Strategy 1 - Compute the loss and update the q_network toward the target greedy network\n",
    "            loss = nn.MSELoss()(q_values, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "            ## Strategy 2 - Use Q-learning to update the weights manually\n",
    "\n",
    "             # Periodically update the target network by loading the weights from the behavior network\n",
    "            if step_count % TARGET_UPDATE_FREQ == 0:\n",
    "                target_network_update_count += 1\n",
    "                target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "   \n",
    "    # Decay epsilon\n",
    "    epsilon = max(EPSILON_END, epsilon - (EPSILON_START - EPSILON_END)/EPSILON_DECAY)\n",
    "\n",
    "    reward_history[episode] = eps_rewards\n",
    "    print(f\"Episode {episode:5d}: Total reward = {eps_rewards:5.1f}, Epsilon = {epsilon:.3f}, Step count = {step_count:5d}, Target update count = {target_network_update_count:3d}\")\n",
    "    print(f\"Average reward {sum(reward_history[:episode])/(episode+1):.2f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8145ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EMA_filter(reward: list, alpha):\n",
    "    output = np.zeros(len(reward)+1)\n",
    "    output[0] = reward[0]\n",
    "    for idx, item in enumerate(reward):\n",
    "        output[idx+1] = (1 - alpha) * output[idx] + alpha * item\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464bca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the history of success rate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "episode = range(EPISODE_TRAIN)\n",
    "filtered_reward_history = EMA_filter(reward_history, 0.1)\n",
    "\n",
    "# Set figure size (width, height) in inches\n",
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "slice_idx = len(reward_history)\n",
    "# Plot y1 vs x on the first subplot\n",
    "plt.plot(episode[:slice_idx], reward_history[:slice_idx], color = \"blue\")\n",
    "plt.plot(episode[:slice_idx], filtered_reward_history[:slice_idx], color = \"red\")\n",
    "plt.title('Success Rate vs Traing Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend(['Total reward', 'Filtered reward'])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c30e48e",
   "metadata": {},
   "source": [
    "## Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8d7de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                                               # For saving models and training results\n",
    "from datetime import datetime                           # For creating the directory of each training run\n",
    "import json                                             # For storing training parameters during each run\n",
    "\n",
    "# Generate a timestamped directory for the training run\n",
    "timestamp = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "BASE_DIR = os.getcwd()\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR,f\"inv_pend_results/{timestamp}_{EPISODE_TRAIN}_{BATCH_SIZE}_{LR}_{GAMMA}_{TARGET_UPDATE_FREQ}_{EPSILON_DECAY}\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': q_network.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    \n",
    "    'learning_rate': LR,\n",
    "    'buffer_size': BUFFER_SIZE,\n",
    "    'min_replay_size': MIN_REPLAY_SIZE,\n",
    "    'target_update_freq': TARGET_UPDATE_FREQ,\n",
    "\n",
    "    'gamma': GAMMA,\n",
    "    'epsilon_start': EPSILON_START,\n",
    "    'epsilon_end': EPSILON_END,\n",
    "    'epsilon_decay': EPSILON_DECAY,\n",
    "\n",
    "    'episode_train': EPISODE_TRAIN,\n",
    "    'batch_size': BATCH_SIZE\n",
    "}, os.path.join(OUTPUT_DIR,'q_network_checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cf419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"C:/Minh Nguyen/experiment/DRL/gym_exercise/inv_pend_results/250520_165708_5000_32_0.001_0.95_1000_5000/q_network_checkpoint.pth\")\n",
    "q_network_loaded = QNetwork(state_space,action_space)\n",
    "q_network_loaded.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b034cbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(checkpoint.keys())\n",
    "print(checkpoint['optimizer_state_dict'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42defbc",
   "metadata": {},
   "source": [
    "## Performance Visualization\n",
    "\n",
    "Visualize the performance of the trained Q network in multiple simulations and observer the reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a3a23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "\n",
    "# print(type(obs))\n",
    "for episode in range(5):\n",
    "    obs, _ = env_test.reset()\n",
    "    done = False\n",
    "    eps_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        with torch.no_grad():       # Doing inference so no need to track operations\n",
    "            state_tensor = torch.FloatTensor(obs).unsqueeze(0)  # unsqueeze to fake the sample (batch) dimension\n",
    "            q_values = q_network(state_tensor)\n",
    "            action = q_values.argmax().item()                   # index of the maximum output (action)\n",
    "        \n",
    "        next_obs, reward, term, trunc, _ = env_test.step(action)\n",
    "\n",
    "        done = term or trunc\n",
    "        eps_reward += reward\n",
    "        obs = next_obs\n",
    "    \n",
    "    print(f\"Reward from episode {episode:3d} is {eps_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6460e208",
   "metadata": {},
   "source": [
    "Visualize the state vs time plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229eb899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "obs, _ = env_test.reset()\n",
    "done = False\n",
    "obs_history = []; obs_history.append(obs)\n",
    "action_history = []\n",
    "reward_history = []\n",
    "\n",
    "while not done:\n",
    "    with torch.no_grad():       # Doing inference so no need to track operations\n",
    "        state_tensor = torch.FloatTensor(obs).unsqueeze(0)  # unsqueeze to fake the sample (batch) dimension\n",
    "        q_values = q_network(state_tensor)\n",
    "        action = q_values.argmax().item()                   # index of the maximum output (action)\n",
    "    \n",
    "    next_obs, reward, term, trunc, _ = env_test.step(action)\n",
    "\n",
    "    action_history.append(action)\n",
    "    obs_history.append(next_obs)\n",
    "    \n",
    "    if not reward_history: reward_history.append(reward)\n",
    "    else: reward_history.append(reward_history[-1] + reward)\n",
    "\n",
    "    done = term or trunc\n",
    "    obs = next_obs\n",
    "\n",
    "# Convert obs_history to a NumPy array for easier slicing\n",
    "obs_array = np.array(obs_history)\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(6, 1, figsize=(12, 24), sharex=True)\n",
    "\n",
    "# 1. Plot cumulative reward history\n",
    "axs[0].plot(reward_history, label='Cumulative Reward', color='green')\n",
    "axs[0].set_ylabel('Cumulative Reward')\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "# 2. Plot action history\n",
    "axs[1].plot(action_history, label='Actions Taken', color='blue')\n",
    "axs[1].set_ylabel('Action')\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "# 3. Plot each CartPole state variable over time\n",
    "state_labels = ['Cart Position', 'Cart Velocity', 'Pole Angle', 'Pole Angular Velocity']\n",
    "for i in range(4):\n",
    "    axs[i+2].plot(obs_array[:, i], label=state_labels[i])\n",
    "    axs[i+2].set_ylabel('State Values')\n",
    "    axs[i+2].set_xlabel('Timestep')\n",
    "    axs[i+2].legend()\n",
    "    axs[i+2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9491c631",
   "metadata": {},
   "source": [
    "# Inverted Pendulum using double DQN (DDQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b674c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random                       # To draw a random batch of samples form the replay buffer\n",
    "import gymnasium as gym             # To create the inverted pendulum environment\n",
    "import torch                        \n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from collections import deque       # replay buffer is a double ended queue that allows elements to be added either to the end or the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f46fd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet_MLP(nn.Module):\n",
    "    ''' A QNetwork class that dynamically initialize an MLP Q Net from a list specifying the number of nodes in each hidden layer (e.g. [64,32])'''\n",
    "    def __init__(self,input_dim,output_dim,hidden_layer = [64,32]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        for size in hidden_layer:\n",
    "            self.layers.append(nn.Linear(self.input_dim, size))\n",
    "            self.input_dim = size\n",
    "        \n",
    "        self.layers.append(nn.Linear(self.input_dim,self.output_dim))\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        for layer in self.layers[:-1]:\n",
    "            input_data = torch.relu(layer(input_data))\n",
    "        return self.layers[-1](input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0afaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN_experiment():\n",
    "    def __init__(self, model_name: str,      # \"DQN_MLP_v0\" or \"DQN_MLP_v1\"\n",
    "                 model_registry, \n",
    "                 lr: float, \n",
    "                 buffer_size: int, \n",
    "                 target_update_freq: int, \n",
    "                 gamma: float, \n",
    "                 eps_start: float, \n",
    "                 eps_decay: int, batch_size: int):\n",
    "        ''' Defining hyperparameters in the experiment '''\n",
    "        self.model_name = model_name                                        # Full name of the model\n",
    "        self.model_class = model_name[self.model_name]['class']             # The model class \"QNet_MLP\" or \"QNet_test\"\n",
    "        self.model_config = model_registry[self.model_name]['config']       # List of nodes in each hidden layer\n",
    "        match = re.search(r'v\\d+',self.model_name)\n",
    "        self.model_id = match.group(0) if match else 404                    # Extract the \"v0\" or \"v1\" out of model name for abbreviation\n",
    "\n",
    "        # Hyperparameters of the experiment\n",
    "        self.lr = lr\n",
    "        self.buffer_size = buffer_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps_start\n",
    "        self.eps_decay = eps_decay\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "        obs_space = self.env.observation_space.shape[0]\n",
    "        act_space = self.env.action_space.n\n",
    "        self.Q_net = QNet_MLP(obs_space, act_space, self.model_config)\n",
    "        self.target_net = \n",
    "\n",
    "\n",
    "    def create_directory(self):\n",
    "        ''' Function that creates directory to save model state_dict, architecture, training configuration, and history\n",
    "\n",
    "        Parameters: \n",
    "        ------------\n",
    "        (hyperparameters for differentiating between different directory)\n",
    "        \n",
    "        lr : float\n",
    "            the learning rate to optimize the Q network\n",
    "        gamma : float \n",
    "            the discount rate in Q learning\n",
    "        epsilon_decay : integer\n",
    "            the amount of episode over which the exploratory rate (epsilon) decays\n",
    "        batch_size : integer\n",
    "            number of experience drawn from replay buffer to train the behaviour network\n",
    "        buffer_size : integer\n",
    "            the number of samples in the replay buffer at a time\n",
    "        target_udpate_freq : integer\n",
    "            the amount of step count during the training process before updating the target Q net (loading the parameters of the behaviour net onto the target Q Net)\n",
    "        \n",
    "\n",
    "        Returns\n",
    "        ------------\n",
    "        name_codified : str\n",
    "            the shortened name for the current experiment \n",
    "        hyperparameters_codified : str\n",
    "            the shortened string of hyperparameter configuration\n",
    "        OUTPUT_DIR : path\n",
    "            the directory to which the training results and model (state_dict and architecture) will be saved\n",
    "        '''\n",
    "        timestamp = datetime.now().strftime(\"%y%m%d_%H%M\")\n",
    "\n",
    "        RESULT_FOLDER = \"inv_pend_DDQN_results\"\n",
    "        BASE_DIR = os.getcwd()\n",
    "        RESULT_DIR = os.path.join(BASE_DIR, RESULT_FOLDER)\n",
    "        os.makedirs(RESULT_DIR, exist_ok=True)      # Create the directory if one does not already exist\n",
    "\n",
    "        # Find the trial # of the latest run\n",
    "        existing_runs = [d for d in os.listdir(RESULT_DIR) if os.path.isdir(os.path.join(RESULT_DIR,d))]\n",
    "        run_numbers = [int(re.search(r'run_(\\d{5})',d).group(1)) for d in existing_runs if re.match(r'run_\\d{5}',d)]\n",
    "        trial_number = max(run_numbers,default=-1)+1\n",
    "\n",
    "        # Create a folder for the run\n",
    "        name_codified = f\"run_{trial_number:05d}\"\n",
    "        OUTPUT_DIR = os.path.join(RESULT_DIR,name_codified)\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)      # Create the directory\n",
    "\n",
    "        # Append the mapping from run # to hyperparameter configuration in a JSON file inside RESULT_DIR\n",
    "        trial_to_param_path = os.path.join(RESULT_DIR,'trial_to_param.json')\n",
    "        if os.path.exists(trial_to_param_path):\n",
    "            with open(trial_to_param_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "        else:\n",
    "            data = {name_codified: []}\n",
    "\n",
    "        hyperparam_codified = f\"{self.model_id}_{self.lr}_{self.buffer_size}_{self.target_update_freq}_{self.gamma}_{self.eps_decay}_{self.batch_size}\"\n",
    "        hyperparam_codified_time = f\"{timestamp}_{self.model_id}_{self.lr}_{self.buffer_size}_{self.target_update_freq}_{self.gamma}_{self.eps_decay}_{self.batch_size}\"\n",
    "        data[name_codified] = hyperparam_codified_time\n",
    "\n",
    "        with open(trial_to_param_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "        # Store the training configs in JSON file\n",
    "        training_params = {\n",
    "            'model_id':             self.model_id,\n",
    "            'lr':                   self.lr,\n",
    "            'gamma':                self.gamma,\n",
    "            'epsilon_decay':        self.eps_decay,\n",
    "            'batch_size':           self.batch_size,\n",
    "            'buffer_size':          self.buffer_size,\n",
    "            'target_update_freq':   self.target_update_freq\n",
    "        }\n",
    "\n",
    "        # Store training parameters in each run \n",
    "        param_path = os.path.join(OUTPUT_DIR, \"param_config.json\")\n",
    "        with open(param_path, \"w\") as f:\n",
    "            json.dump({\"parameters\": training_params}, f, indent=2)\n",
    "\n",
    "        return name_codified, hyperparam_codified, OUTPUT_DIR\n",
    "    \n",
    "    def eps_greedy_policy(self, obs):      \n",
    "        # TODO: epsilon can be replaced with self.epsilon, but must remember to update self.epsilon each episode\n",
    "        ''' Function to take an action according to an epsilon-greedy policy and a Q-network'''\n",
    "        if np.random.random() < self.eps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
    "                q_values = self.Q_net(state_tensor)\n",
    "                action = q_values.argmax().item()\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def DDQN_train(env: gym.Env, env_test: gym.Env,\n",
    "              q_network: nn.Module, target_net: nn.Module, optimizer: torch.optim.Optimizer, \n",
    "              replay_buffer,\n",
    "              target_update_freq,\n",
    "              gamma,\n",
    "              eps_start, eps_end, eps_decay,\n",
    "              episode_train,\n",
    "              batch_size, \n",
    "              save_path,\n",
    "              seed = 42):\n",
    "    ''' Function to train a policy for a set of hyperparameters '''\n",
    "\n",
    "    msg = \"Training ended, no good model found!\"\n",
    "\n",
    "    reward_history = np.zeros(episode_train)\n",
    "    epsilon = eps_start\n",
    "    step_count = 0\n",
    "    episode = 0\n",
    "    target_network_update_count = 0\n",
    "\n",
    "    # Control of early stopping\n",
    "    consecutive_pass_count = 0           # Number of consecutive episodes where performance exceeds a threshold\n",
    "    CONSECUTIVE_PASS_LIMIT = 3          # No. of consecutive episodes with higher performance than reward limit\n",
    "    EPISODE_REWARD_LIMIT = 450\n",
    "    best_reward = 0\n",
    "    performance_crit = False\n",
    "    train_terminated = False\n",
    "    val_history = {}                    # Monitor which episode had a validation run, the train reward, and the validation (test) reward \n",
    "\n",
    "    while not train_terminated:     # Experiment level - loop through episodes\n",
    "        obs, _ = env.reset(seed=seed)\n",
    "        eps_rewards = 0\n",
    "    \n",
    "        while True:                 # Episode level - loop through steps\n",
    "            action = eps_greedy_policy(env, obs, epsilon, q_network)\n",
    "\n",
    "            # Interact with the environment\n",
    "            next_obs, reward, term, trunc, _ = env.step(action)\n",
    "            done = term or trunc\n",
    "            replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "            obs = next_obs\n",
    "            eps_rewards += reward\n",
    "            step_count += 1\n",
    "\n",
    "            # Train the Q-net using a batch of samples from the experience replay\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch = random.sample(replay_buffer, batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                states = torch.FloatTensor(np.array(states))            # Shape of (BATCH_SIZE, 4). Data structure of [[p1 v1 theta1 dtheta1], [p2 v2 theta2 dtheta2], ...]\n",
    "                actions = torch.LongTensor(actions).unsqueeze(1)        # Must unsqueeze to have shape (BATCH_SIZE, 1). From [a1, a2, ...] to [[a1], [a2], ... [a_BATCH_SIZE]]\n",
    "                rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "                next_states = torch.FloatTensor(np.array(next_states))\n",
    "                dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "                \n",
    "                # Compute targets using target network Q(s',a',w_i^-)\n",
    "                with torch.no_grad():\n",
    "                    target_q_values = target_net(next_states)       # Find a batch of Q(s',a',w_i^-) from the batch of next_states\n",
    "                    max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]\n",
    "                    targets = rewards + gamma * max_target_q_values * (1 - dones)       # (1-dones) to avoid over estimating the value of terminal states. The target should only be the reward in the terminal states\n",
    "            \n",
    "                # Compute the current Q values for the actions taken Q(s,a,w_i)\n",
    "                q_values = q_network(states).gather(1, actions)         # obtain the q_values associated to the actual action taken in each sample\n",
    "\n",
    "                # Update the parameters of the behaviour q_network\n",
    "                loss = nn.MSELoss()(q_values, targets)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Periodically update the target network by loading the weights from the behavior network\n",
    "            if step_count % target_update_freq == 0:\n",
    "                target_network_update_count += 1\n",
    "                target_net.load_state_dict(q_network.state_dict())\n",
    "\n",
    "            if done:        # End of a training episode\n",
    "                break\n",
    "\n",
    "        # Decay epsilon after an episode\n",
    "        epsilon = max(eps_end, epsilon - (eps_start - eps_end)/eps_decay)\n",
    "\n",
    "        reward_history[episode] = eps_rewards\n",
    "        # print(f\"Episode {episode:5d}: Total reward = {eps_rewards:5.1f}, Epsilon = {epsilon:.3f}, Step count = {step_count:5d}, Target update count = {target_network_update_count:3d}\")\n",
    "        # print(f\"Average reward {sum(reward_history[:episode])/(episode+1):.2f}\")\n",
    "\n",
    "        \n",
    "        if episode % 10 == 0:                   # print progress periodically\n",
    "            print(f\"Episode {episode:5d}: Total reward = {eps_rewards:5.1f}, Epsilon = {epsilon:.3f}\", end = \"\\r\")\n",
    "\n",
    "        # Early stopping condition\n",
    "        if eps_rewards >= EPISODE_REWARD_LIMIT:\n",
    "            test_reward = policy_eval(env_test, q_network, 100)\n",
    "            val_history[episode] = [eps_rewards, test_reward]\n",
    "\n",
    "            if test_reward > best_reward:           # Set the new best reward\n",
    "                best_reward = test_reward\n",
    "                save_model(q_network, optimizer, save_path)\n",
    "                msg = f\"Training terminated due to episode limit, best model saved at episode {episode:5d}\"\n",
    "            if test_reward > EPISODE_REWARD_LIMIT: \n",
    "                consecutive_pass_count += 1\n",
    "            else: consecutive_pass_count = 0\n",
    "        else:\n",
    "            consecutive_pass_count = 0\n",
    "            \n",
    "        # Performance criteria - if good results for several episodes => training performance satisfied and terminate early\n",
    "        if consecutive_pass_count >= CONSECUTIVE_PASS_LIMIT:\n",
    "            performance_crit = True \n",
    "            msg = f\"Early termination at episode {episode:5d}, desired performance reached\"\n",
    "\n",
    "\n",
    "        episode += 1\n",
    "\n",
    "        # Checking for early training termination or truncation\n",
    "        train_terminated = (episode >= episode_train) or (performance_crit)\n",
    "    print(\"\\n\")\n",
    "    return reward_history, val_history, msg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93adb7c4",
   "metadata": {},
   "source": [
    "## Blah"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
