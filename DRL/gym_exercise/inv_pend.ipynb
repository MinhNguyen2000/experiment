{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e0cac5",
   "metadata": {},
   "source": [
    "# Inverted Pendulum using DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1079ee",
   "metadata": {},
   "source": [
    "When moving on from discrete environments, such as grid world or black jack, the amount of state in a continous environment renders traditional tabular methods (Monte Carlo and TD learning) intractable (at least without any modification to the problem formulation). DQN method addresses this problem by using a value function approximator, essentially replacing the Q-table of explicit state-action value with a parameterized function $q^\\pi_{\\vec w}(s,a,\\vec w)$. The vector $\\vec w$ stores the weights or parameters of this approximating function.\n",
    "\n",
    "The motivation of using value function approximator is clear, but where does it fit into the RL picture?\n",
    "- Value function approximator is a direct mapping between the states (features) to the state-action value $Q(s,\\cdot)$ \n",
    "- As such, the approximator function fits in the policy evaluation step of the RL training process. We train this function by minimizing the loss between the approximated value function and the actual value function $ s.t. \\enspace q^\\pi_{\\vec w}(s,a,\\vec w) \\approx q^*$\n",
    "- However, the true action value function $q^*$ is unknown and must be learnt from the interaction between the agent and its environment. We must employ Q-learning to approximate the true action value function, toward which we will update the parameterized value function $q^\\pi_{\\vec w}(s,a,\\vec w)$. The update is as follow\n",
    "\n",
    "**The DQN algorithm**\n",
    "1. From a state $s$, take action $a$ using $\\epsilon$-greedy policy, observe the reward $r$ and next state $s'$\n",
    "2. Store the transition tuple $(s, a, r, s')$ in a replay memory $\\mathcal{D}$. This replay memory is a buffer that stores the most recent transition tuples\n",
    "3. From this replay memory, sample batches of $(s_t, a_t, r_{t+1}, s_{t+1})$ randomly for training the Q-function approximator\n",
    "4. Adjust the parameters $\\vec w$ by optimizing the mean squared error (MSE) between the Q network and the Q-learning target using Q-learning\n",
    "$$ \\mathcal{L}_i (\\vec w_i) = \\mathbb{E}_{(s,a,r,s') \\sim \\mathcal{D}} \\bigg[ \\Big(r + \\gamma \\max_{a'}Q(s',a',\\vec w^-_i) - Q(s,a,\\vec w_i) \\Big)^2\\bigg]$$ \n",
    "\n",
    "When applying stochastic gradient descent, the update at every step is reduced as follows (the math found in the GoodNote notebook):\n",
    "\n",
    "$$\\Delta \\vec w = \\alpha \\Big(r + \\gamma \\max_{a'} \\underbrace{Q(s', a', \\vec{w}_i^-)}_{\\text{target}} - \\underbrace{Q(s,a,\\vec w_i)}_{\\text{behaviour}} \\Big) \\vec x(s,a)$$\n",
    "\n",
    "\n",
    "**Notes:** \n",
    "\n",
    "There are two mechanisms introduced in this paper that allow for stable training, one is experience replay, and the other is training two separate methods.\n",
    "- **Experience replay** - this decorrelates the experience tuples (s,a,r,s') thus follow the i.i.d assumption for training a neural network\n",
    "- **Two Q-networks**, a target network and a behaviour network. The parameters $\\vec w'$ of the target network $Q(s,a,\\vec w')$ are only updated infrequently to maintain a stable target. The parameters $\\vec w$ of the behaviour network $Q(s,a,\\vec w)$ are updated every policy improvement iteration to move toward this target network. The frequency of updating the target network is annother hyperparameter to tune. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03179d1e",
   "metadata": {},
   "source": [
    "## Question to self\n",
    "1. What does it mean to apply stochastic gradient descent to simplify the loss function? What are the implications?\n",
    "- Every policy evaluation iteration, a random sample, or batch of samples, is used for updating the state-action value instead of every single experience stored in the replay buffer. Recall that the random sampling from the replay buffer de-correlates the experience and enforces the i.i.d. assumption\n",
    "- SGD is used on a small batch of sample to reduce the variance inherent in the different experience in the replay buffer. Despite the noise and bias of this method, the performance of the Q networks improves.\n",
    "\n",
    "\n",
    "</br>\n",
    "\n",
    "2. Would it be beneficial to formulate the Q network such that the input is the state vector and the outputs are the Q-values associated with each possible action from the state. As such, one can just take the max value from the target Q network and index into the behaviour network for the corresponding action a?\n",
    "- That is the common approach for approximating the action value function, outputting Q(s) for all a, then grabbing the maximum value\n",
    " \n",
    "</br>\n",
    "\n",
    "3. At first do I have to run the agent several times using the initial $\\epsilon$-greedy policy to fill the memory buffer with training samples?\n",
    "- Yes, and then pop in new experience at the end, push the oldest experience in the replay buffer out.\n",
    "\n",
    "</br>\n",
    "\n",
    "4. How often does the behaviour target network updated?\n",
    "\n",
    "- At every step in an episode of the training process, the behaviour network is used for two different purposes:\n",
    "    - Firstly, the behaviour network governs the trajectory of the agent at every step. \n",
    "    - Secondly, the behaviour Q network is used to predict (or infer) the action value for the <s,a> pairs drawn from a batch of experience (s,a,r,s'). These value is compared to the discounted maximum action value at state s', $\\max Q(s',\\cdot)$, according to the **target network**. \n",
    "\n",
    "- The behaviour network prediction of $Q(s,a)$ and the target network's label output of $r+\\gamma \\max Q(s',a')$ are used to compute a loss function. The gradient of this loss function is used for adjusting the parameters of the behaviour network.\n",
    "- After the agent has taken a number of step, the parameters (or weights) or the behaviour network is loaded onto the target network.\n",
    "\n",
    "</br>\n",
    "\n",
    "5. How can I scale this environment up to have continuous action instead of just left/right?\n",
    "- To my understanding, actuating the cart to the left or right at similar output level (voltage) at instantaneous moments throughout the control process is good enough for modulating the speed of the cart. Think of PWMs where the voltage is just switch at a certain duty cycle to simulate different voltage level.\n",
    "- It is possible to have continuous action space. Since the Q network output is the state-action value Q(s,a) at each state s and action a, we know how good each action is in a state. The Q values can then be transformed into a proportional constant to control how much to move left or right (instead of just a binary left/right). This essentially means that the Q-value output of this network is not simply used for selecting the maximizing action at each state but also for governing the magnitude of cart actuation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d20579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random                       # To draw a random batch of samples form the replay buffer\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque       # replay buffer is a double ended queue that allows elements to be added either to the end or the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dafb2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate several episodes of the cart pole environment (random policy on a human render mode)\n",
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "\n",
    "state, _ = env.reset(); t = 1\n",
    "for i in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, term, trunc, info = env.step(action)\n",
    "    print(f\"Time {t:2d}  |  s_t {state}  |  a_t {action:2d}  |  s_t+1 {next_state}  |  reward {reward:.2f}  |  terminated {term:2}  |  {info}\")\n",
    "    state = next_state; t += 1\n",
    "    if term or trunc:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2ec82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant definitions\n",
    "LR = 1e-3\n",
    "BUFFER_SIZE = 5000                  # size of the experience replay buffer\n",
    "MIN_REPLAY_SIZE = 1000              # The number of experience to sample before starting training\n",
    "TARGET_UPDATE_FREQ = 1000           # the number of steps before updating the target network\n",
    "\n",
    "GAMMA = 0.95                        # Discount factor for return calculation\n",
    "EPSILON_START = 1.0                 # exploration rate in stochastic policy\n",
    "EPSILON_END = 0.1                   # minimum exploration rate at the end of decay\n",
    "EPSILON_DECAY = 5000                # epsilon decay rate\n",
    "\n",
    "EPISODE_TRAIN = 5000              # number of episodes to train the Q-network\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ce285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Q-network architecture\n",
    "class QNetwork(nn.Module):\n",
    "    ''' A QNetwork class that initialize an MLP neural Q network with two hidden layers, each with 128 nodes '''\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)                       # The regression output are the state values of Q(s,a=left) and Q(s,a=right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874662e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically define Q-network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8115dd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment and the Q network\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_space = env.observation_space.shape[0]        # State space of cart pos & vel, and pendulum angular pos and vel. Each component is a continuous value\n",
    "action_space =  env.action_space.n                   # Action space of left or right\n",
    "\n",
    "q_network = QNetwork(state_space,action_space)\n",
    "target_network = QNetwork(state_space,action_space)\n",
    "target_network.load_state_dict(q_network.state_dict())\n",
    "target_network.eval()                               # target network is only used for evaluation/inference and not trained\n",
    "\n",
    "optimizer = optim.SGD(q_network.parameters(), lr = LR)\n",
    "replay_buffer = deque(maxlen=BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be9e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill replay buffer with some initial samples for training (random policy)\n",
    "obs, _ = env.reset()\n",
    "for _ in range(MIN_REPLAY_SIZE):\n",
    "    action = env.action_space.sample()\n",
    "    next_obs, reward, term, trunc, _ = env.step(action)\n",
    "    done = term or trunc\n",
    "\n",
    "    replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "    obs = next_obs if not done else env.reset()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af1a257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Model Training loop\n",
    "reward_history = np.zeros(EPISODE_TRAIN)\n",
    "epsilon = EPSILON_START\n",
    "step_count = 0\n",
    "target_network_update_count = 0\n",
    "\n",
    "for episode in range(EPISODE_TRAIN):\n",
    "    obs, _ = env.reset()\n",
    "    eps_rewards = 0\n",
    "    \n",
    "\n",
    "    while True:\n",
    "        # Epsilon-greedy policy to select action\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():       # Doing inference so no need to track operations\n",
    "                state_tensor = torch.FloatTensor(obs).unsqueeze(0)  # unsqueeze to fake the sample (batch) dimension\n",
    "                q_values = q_network(state_tensor)\n",
    "                action = q_values.argmax().item()                   # index of the maximum output (action)\n",
    "        \n",
    "        # Interact with the environment\n",
    "        next_obs, reward, term, trunc, _ = env.step(action)\n",
    "        done = term or trunc\n",
    "        replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "        obs = next_obs\n",
    "        eps_rewards += reward\n",
    "        step_count += 1\n",
    "\n",
    "        # Sample a batch and update the network\n",
    "        if len(replay_buffer) >= BATCH_SIZE:\n",
    "            batch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.FloatTensor(states)                                          # Shape of (BATCH_SIZE, 4). Data structure of [[p1 v1 theta1 dtheta1], [p2 v2 theta2 dtheta2], ...]\n",
    "            actions = torch.LongTensor(actions).unsqueeze(1)                            # Must unsqueeze to have shape (BATCH_SIZE, 1). From [a1, a2, ...] to [[a1], [a2], ... [a_BATCH_SIZE]]\n",
    "            rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "            next_states = torch.FloatTensor(next_states)\n",
    "            dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "\n",
    "            # Compute targets using target network Q(s',a',w_i^-)\n",
    "            with torch.no_grad():\n",
    "                target_q_values = target_network(next_states)                           # Find a batch of Q(s',a',w_i^-) from the batch of next_states\n",
    "                max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]\n",
    "                targets = rewards + GAMMA * max_target_q_values * (1 - dones)           # (1-dones) to avoid over estimating the value of terminal states. The target should only be the reward in the terminal states\n",
    "\n",
    "            # Compute the current Q values for the actions taken Q(s,a,w_i)\n",
    "            q_values = q_network(states).gather(1, actions)                             # obtain the q_values associated to the actual action taken in each sample\n",
    "\n",
    "            # Update the parameters of the behaviour q_network\n",
    "            loss = nn.MSELoss()(q_values, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "             # Periodically update the target network\n",
    "            if step_count % TARGET_UPDATE_FREQ == 0:\n",
    "                target_network_update_count += 1\n",
    "                target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "   \n",
    "    # Decay epsilon\n",
    "    epsilon = max(EPSILON_END, epsilon - (EPSILON_START - EPSILON_END)/EPSILON_DECAY)\n",
    "\n",
    "    reward_history[episode] = eps_rewards\n",
    "    print(f\"Episode {episode:5d}: Total reward = {eps_rewards:5.1f}, Epsilon = {epsilon:.3f}, Step count = {step_count:5d}, Target update count = {target_network_update_count:3d}\")\n",
    "    print(f\"Average reward {sum(reward_history[:episode])/(episode+1):.2f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8145ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for EMA filters and plotting data\n",
    "def EMA_filter(reward: list, alpha):\n",
    "    ''' Function that runs an exponential moving average filter along a datastream '''\n",
    "    output = np.zeros(len(reward)+1)\n",
    "    output[0] = reward[0]\n",
    "    for idx, item in enumerate(reward):\n",
    "        output[idx+1] = (1 - alpha) * output[idx] + alpha * item\n",
    "    \n",
    "    return output\n",
    "\n",
    "def plot_reward_hist(reward_history: list, hyperparam_config: str, save_path = None, alpha = 0.1):\n",
    "    ''' Function that plots the reward and filtered reward per episode, then saves the plot in a specified save directory'''\n",
    "    n_episodes= len(reward_history)\n",
    "    episodes = range(n_episodes)\n",
    "    filtered_reward_hist = EMA_filter(reward_history, alpha)\n",
    "\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.plot(episodes, reward_history[:n_episodes], color = \"blue\")\n",
    "    plt.plot(episodes, filtered_reward_hist[:n_episodes], color = \"red\")\n",
    "    plt.title(f'Total reward per episode - {hyperparam_config}')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend(['Total reward','Filtered reward'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(os.path.join(save_path,'reward_history.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464bca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the history of success rate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "episode = range(EPISODE_TRAIN)\n",
    "filtered_reward_history = EMA_filter(reward_history, 0.1)\n",
    "\n",
    "# Set figure size (width, height) in inches\n",
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "slice_idx = len(reward_history)\n",
    "# Plot y1 vs x on the first subplot\n",
    "plt.plot(episode[:slice_idx], reward_history[:slice_idx], color = \"blue\")\n",
    "plt.plot(episode[:slice_idx], filtered_reward_history[:slice_idx], color = \"red\")\n",
    "plt.title('Success Rate vs Traing Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend(['Total reward', 'Filtered reward'])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c30e48e",
   "metadata": {},
   "source": [
    "## Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8d7de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                                               # For saving models and training results\n",
    "from datetime import datetime                           # For creating the directory of each training run\n",
    "import json                                             # For storing training parameters during each run\n",
    "\n",
    "# Generate a timestamped directory for the training run\n",
    "timestamp = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "BASE_DIR = os.getcwd()\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR,f\"inv_pend_results/{timestamp}_{EPISODE_TRAIN}_{BATCH_SIZE}_{LR}_{GAMMA}_{TARGET_UPDATE_FREQ}_{EPSILON_DECAY}\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': q_network.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    \n",
    "    'learning_rate': LR,\n",
    "    'buffer_size': BUFFER_SIZE,\n",
    "    'min_replay_size': MIN_REPLAY_SIZE,\n",
    "    'target_update_freq': TARGET_UPDATE_FREQ,\n",
    "\n",
    "    'gamma': GAMMA,\n",
    "    'epsilon_start': EPSILON_START,\n",
    "    'epsilon_end': EPSILON_END,\n",
    "    'epsilon_decay': EPSILON_DECAY,\n",
    "\n",
    "    'episode_train': EPISODE_TRAIN,\n",
    "    'batch_size': BATCH_SIZE\n",
    "}, os.path.join(OUTPUT_DIR,'q_network_checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cf419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"C:/Minh Nguyen/experiment/DRL/gym_exercise/inv_pend_results/250520_165708_5000_32_0.001_0.95_1000_5000/q_network_checkpoint.pth\")\n",
    "q_network_loaded = QNetwork(state_space,action_space)\n",
    "q_network_loaded.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b034cbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(checkpoint.keys())\n",
    "print(checkpoint['optimizer_state_dict'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42defbc",
   "metadata": {},
   "source": [
    "## Performance Visualization\n",
    "\n",
    "Visualize the performance of the trained Q network in multiple simulations and observer the reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a3a23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "\n",
    "# print(type(obs))\n",
    "for episode in range(5):\n",
    "    obs, _ = env_test.reset()\n",
    "    done = False\n",
    "    eps_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        with torch.no_grad():       # Doing inference so no need to track operations\n",
    "            state_tensor = torch.FloatTensor(obs).unsqueeze(0)  # unsqueeze to fake the sample (batch) dimension\n",
    "            q_values = q_network(state_tensor)\n",
    "            action = q_values.argmax().item()                   # index of the maximum output (action)\n",
    "        \n",
    "        next_obs, reward, term, trunc, _ = env_test.step(action)\n",
    "\n",
    "        done = term or trunc\n",
    "        eps_reward += reward\n",
    "        obs = next_obs\n",
    "    \n",
    "    print(f\"Reward from episode {episode:3d} is {eps_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6460e208",
   "metadata": {},
   "source": [
    "Visualize the state vs time plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229eb899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "obs, _ = env_test.reset()\n",
    "done = False\n",
    "obs_history = []; obs_history.append(obs)\n",
    "action_history = []\n",
    "reward_history = []\n",
    "\n",
    "while not done:\n",
    "    with torch.no_grad():       # Doing inference so no need to track operations\n",
    "        state_tensor = torch.FloatTensor(obs).unsqueeze(0)  # unsqueeze to fake the sample (batch) dimension\n",
    "        q_values = q_network(state_tensor)\n",
    "        action = q_values.argmax().item()                   # index of the maximum output (action)\n",
    "    \n",
    "    next_obs, reward, term, trunc, _ = env_test.step(action)\n",
    "\n",
    "    action_history.append(action)\n",
    "    obs_history.append(next_obs)\n",
    "    \n",
    "    if not reward_history: reward_history.append(reward)\n",
    "    else: reward_history.append(reward_history[-1] + reward)\n",
    "\n",
    "    done = term or trunc\n",
    "    obs = next_obs\n",
    "\n",
    "# Convert obs_history to a NumPy array for easier slicing\n",
    "obs_array = np.array(obs_history)\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(6, 1, figsize=(12, 24), sharex=True)\n",
    "\n",
    "# 1. Plot cumulative reward history\n",
    "axs[0].plot(reward_history, label='Cumulative Reward', color='green')\n",
    "axs[0].set_ylabel('Cumulative Reward')\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "# 2. Plot action history\n",
    "axs[1].plot(action_history, label='Actions Taken', color='blue')\n",
    "axs[1].set_ylabel('Action')\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "# 3. Plot each CartPole state variable over time\n",
    "state_labels = ['Cart Position', 'Cart Velocity', 'Pole Angle', 'Pole Angular Velocity']\n",
    "for i in range(4):\n",
    "    axs[i+2].plot(obs_array[:, i], label=state_labels[i])\n",
    "    axs[i+2].set_ylabel('State Values')\n",
    "    axs[i+2].set_xlabel('Timestep')\n",
    "    axs[i+2].legend()\n",
    "    axs[i+2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9491c631",
   "metadata": {},
   "source": [
    "# Inverted Pendulum using double DQN (DDQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b674c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random                       # To draw a random batch of samples form the replay buffer\n",
    "import gymnasium as gym             # To create the inverted pendulum environment\n",
    "import torch                        \n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import deque       # replay buffer is a double ended queue that allows elements to be added either to the end or the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37a9ea40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU: 1\n",
      "Current device: 0|NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # device = torch.cuda.device()\n",
    "    device_idx = torch.cuda.current_device()\n",
    "    print(f\"Number of GPU: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {device_idx}|{torch.cuda.get_device_name(device_idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0adada0",
   "metadata": {},
   "source": [
    "## Hyperparameter definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c9cc9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant definitions\n",
    "MODEL_NAME = 'DQN_MLP_v0'\n",
    "LR = 5e-4\n",
    "BUFFER_SIZE = 5000                  # size of the experience replay buffer\n",
    "MIN_REPLAY_SIZE = 1000              # The number of experience to sample before starting training\n",
    "TARGET_UPDATE_FREQ = 1000           # the number of steps before updating the target network\n",
    "\n",
    "GAMMA = 0.95                        # Discount factor for return calculation\n",
    "EPS_START = 1.0                 # exploration rate in stochastic policy\n",
    "EPS_END = 0.1                   # minimum exploration rate at the end of decay\n",
    "EPS_DECAY = 5000                # epsilon decay rate\n",
    "\n",
    "EPISODE_TRAIN = 5000              # number of episodes to train the Q-network\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "CUDA_ENABLED = False\n",
    "SUCCESS_CRITERIA = 450\n",
    "VERBOSE = False                 # Set to false to skip information prints and save computation time\n",
    "NUM_RUN = 100                   # Number of experiment to run collect training time and runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e8b2a9",
   "metadata": {},
   "source": [
    "## Setup - Model and DDQN Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41de9054",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet_MLP(nn.Module):\n",
    "    ''' A QNetwork class that dynamically initialize an MLP Q Net from a list specifying the number of nodes in each hidden layer (e.g. [64,32])'''\n",
    "    def __init__(self,input_dim,output_dim,hidden_layer = [64,32]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        for size in hidden_layer:\n",
    "            self.layers.append(nn.Linear(self.input_dim, size))\n",
    "            self.input_dim = size\n",
    "        \n",
    "        self.layers.append(nn.Linear(self.input_dim,self.output_dim))\n",
    "        if CUDA_ENABLED: self.cuda()\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        for layer in self.layers[:-1]:\n",
    "            input_data = torch.relu(layer(input_data))\n",
    "        return self.layers[-1](input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a0d2559",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_registry = {\n",
    "    'DQN_MLP_v0': {\n",
    "        'class': QNet_MLP,\n",
    "        'config': [64,32]\n",
    "    },\n",
    "    'DQN_MLP_v1': {\n",
    "        'class': QNet_MLP,\n",
    "        'config': [32,32]\n",
    "    },\n",
    "    'DQN_MLP_v2': {\n",
    "        \n",
    "        'class': QNet_MLP,\n",
    "        'config': [32,16]\n",
    "    },\n",
    "    'DQN_MLP_v3': {\n",
    "        'class': QNet_MLP,\n",
    "        'config': [16,16]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f0afaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN_experiment():\n",
    "    def __init__(self, model_name: str,      # \"DQN_MLP_v0\" or \"DQN_MLP_v1\"\n",
    "                 model_registry, \n",
    "                 lr: float, \n",
    "                 buffer_size: int, \n",
    "                 target_update_freq: int, \n",
    "                 gamma: float, \n",
    "                 eps_start: float, \n",
    "                 eps_decay: int,\n",
    "                 eps_end: float, \n",
    "                 batch_size: int,\n",
    "                 seed = 42,\n",
    "                 cuda_enabled = False,\n",
    "                 verbose = True):\n",
    "        \n",
    "        self.seed = seed                    # TODO - incorporate seeds to control the repeatability of the experiment \n",
    "        self.cuda_enabled = cuda_enabled    # if set to True, models and computations are done on GPU\n",
    "        self.verbose = verbose              # if set to False, skip prining the information throughout training to save time\n",
    "        ''' Defining hyperparameters in the experiment '''\n",
    "        self.model_name = model_name                                        # Full name of the model\n",
    "        self.model_class = model_registry[self.model_name]['class']             # The model class \"QNet_MLP\" or \"QNet_test\"\n",
    "        self.model_config = model_registry[self.model_name]['config']       # List of nodes in each hidden layer\n",
    "        match = re.search(r'v\\d+',self.model_name)\n",
    "        self.model_id = match.group(0) if match else 404                    # Extract the \"v0\" or \"v1\" out of model name for abbreviation\n",
    "\n",
    "        # Hyperparameters of the experiment\n",
    "        self.lr = lr\n",
    "        self.buffer_size = buffer_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.gamma = gamma\n",
    "        self.eps_start, self.eps, self.eps_decay, self.eps_end = eps_start, eps_start, eps_decay, eps_end\n",
    "        self.batch_size = batch_size\n",
    "        self.episode_train = 5000\n",
    "        self.min_replay_size = 1000\n",
    "\n",
    "        # Initialize the train and validation environments\n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "        self.env_val = gym.make(\"CartPole-v1\")\n",
    "        self.obs_space = self.env.observation_space.shape[0]\n",
    "        self.act_space = self.env.action_space.n\n",
    "\n",
    "        # Initialize the 2 Q networks and the optimizer for the behavior Q_net\n",
    "        self.Q_net = self.model_class(self.obs_space, self.act_space, self.model_config)\n",
    "        self.target_net = self.model_class(self.obs_space, self.act_space, self.model_config)\n",
    "        self.target_net.load_state_dict(self.Q_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.SGD(self.Q_net.parameters(), lr = self.lr)\n",
    "\n",
    "        self.save_path = \"\"                                             # Directory of the current run\n",
    "        self.model_path = \"\"                                            # Path to a model \n",
    "        self.hyperparam_config = \"\"                                     # Shortened list of importatnt hyperparameters\n",
    "        self.reward_history = np.zeros(self.episode_train)\n",
    "        self.val_history = {}                                           # Monitor which episode had a validation run, the train reward, and the validation (test) reward \n",
    "\n",
    "    def create_directory(self):\n",
    "        ''' Function that creates directory to save model state_dict, architecture, training configuration, and history\n",
    "\n",
    "        Parameters: \n",
    "        ------------\n",
    "        (hyperparameters for differentiating between different directory)\n",
    "        \n",
    "        lr : float\n",
    "            the learning rate to optimize the Q network\n",
    "        gamma : float \n",
    "            the discount rate in Q learning\n",
    "        epsilon_decay : integer\n",
    "            the amount of episode over which the exploratory rate (epsilon) decays\n",
    "        batch_size : integer\n",
    "            number of experience drawn from replay buffer to train the behaviour network\n",
    "        buffer_size : integer\n",
    "            the number of samples in the replay buffer at a time\n",
    "        target_udpate_freq : integer\n",
    "            the amount of step count during the training process before updating the target Q net (loading the parameters of the behaviour net onto the target Q Net)\n",
    "        \n",
    "\n",
    "        Returns\n",
    "        ------------\n",
    "        name_codified : str\n",
    "            the shortened name for the current experiment \n",
    "        hyperparameters_codified : str\n",
    "            the shortened string of hyperparameter configuration\n",
    "        OUTPUT_DIR : path\n",
    "            the directory to which the training results and model (state_dict and architecture) will be saved\n",
    "        '''\n",
    "        timestamp = datetime.now().strftime(\"%y%m%d_%H%M\")\n",
    "\n",
    "        RESULT_FOLDER = \"comp_time_results\"\n",
    "        BASE_DIR = os.getcwd()\n",
    "        RESULT_DIR = os.path.join(BASE_DIR, RESULT_FOLDER)\n",
    "        os.makedirs(RESULT_DIR, exist_ok=True)      # Create the directory if one does not already exist\n",
    "\n",
    "        # Find the trial # of the latest run\n",
    "        existing_runs = [d for d in os.listdir(RESULT_DIR) if os.path.isdir(os.path.join(RESULT_DIR,d))]\n",
    "        run_numbers = [int(re.search(r'run_(\\d{5})',d).group(1)) for d in existing_runs if re.match(r'run_\\d{5}',d)]\n",
    "        trial_number = max(run_numbers,default=-1)+1\n",
    "\n",
    "        # Create a folder for the run\n",
    "        name_codified = f\"run_{trial_number:05d}\"\n",
    "        OUTPUT_DIR = os.path.join(RESULT_DIR,name_codified)\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)      # Create the directory\n",
    "\n",
    "        # Append the mapping from run # to hyperparameter configuration in a JSON file inside RESULT_DIR\n",
    "        trial_to_param_path = os.path.join(RESULT_DIR,'trial_to_param.json')\n",
    "        if os.path.exists(trial_to_param_path):\n",
    "            with open(trial_to_param_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "        else:\n",
    "            data = {name_codified: []}\n",
    "\n",
    "        if self.cuda_enabled:\n",
    "            hyperparam_codified = f\"DDQN_OOP_CUDA_{self.model_id}_{self.lr}_{self.buffer_size}_{self.target_update_freq}_{self.gamma}_{self.eps_decay}_{self.batch_size}\"\n",
    "            hyperparam_codified_time = f\"{timestamp}_DDQN_OOP_CUDA_{self.model_id}_{self.lr}_{self.buffer_size}_{self.target_update_freq}_{self.gamma}_{self.eps_decay}_{self.batch_size}\"\n",
    "        else:   \n",
    "            hyperparam_codified = f\"DDQN_OOP_nCUDA_{self.model_id}_{self.lr}_{self.buffer_size}_{self.target_update_freq}_{self.gamma}_{self.eps_decay}_{self.batch_size}\"\n",
    "            hyperparam_codified_time = f\"{timestamp}_DDQN_OOP_nCUDA_{self.model_id}_{self.lr}_{self.buffer_size}_{self.target_update_freq}_{self.gamma}_{self.eps_decay}_{self.batch_size}\"\n",
    "        data[name_codified] = hyperparam_codified_time\n",
    "\n",
    "        with open(trial_to_param_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "        # Store the training configs in JSON file\n",
    "        training_params = {\n",
    "            'OOP':                  True,\n",
    "            'CUDA':                 self.cuda_enabled,\n",
    "            'device':               torch.cuda.get_device_name(torch.cuda.current_device()),\n",
    "            'model_id':             self.model_id,\n",
    "            'lr':                   self.lr,\n",
    "            'gamma':                self.gamma,\n",
    "            'epsilon_decay':        self.eps_decay,\n",
    "            'batch_size':           self.batch_size,\n",
    "            'buffer_size':          self.buffer_size,\n",
    "            'target_update_freq':   self.target_update_freq\n",
    "        }\n",
    "\n",
    "        # Store training parameters in each run \n",
    "        param_path = os.path.join(OUTPUT_DIR, \"param_config.json\")\n",
    "        with open(param_path, \"w\") as f:\n",
    "            json.dump({\"parameters\": training_params}, f, indent=2)\n",
    "\n",
    "        return name_codified, hyperparam_codified, OUTPUT_DIR\n",
    "    \n",
    "    def eps_greedy_policy(self, env, obs, epsilon):      \n",
    "        ''' Function to take an action according to an epsilon-greedy policy and a Q-network'''\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                if self.cuda_enabled:\n",
    "                    state_tensor = torch.tensor(obs, dtype=torch.float32, device='cuda').unsqueeze(0)\n",
    "                else:\n",
    "                    state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
    "                q_values = self.Q_net(state_tensor)\n",
    "                action = q_values.argmax().item()\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def prefill_replay(self):\n",
    "        obs,_ = self.env.reset()\n",
    "        for _ in range(self.min_replay_size):\n",
    "            action = self.env.action_space.sample()\n",
    "            next_obs, reward, term, trunc, _ = self.env.step(action)\n",
    "            done = term or trunc\n",
    "\n",
    "            self.replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "            obs = next_obs if not done else self.env.reset()[0]\n",
    "\n",
    "    def policy_eval(self, env: gym.Env, n_episode_test = 500, verbose = 0):\n",
    "        ''' Assess the average reward when following a q_network in a test environment with random state initialization\n",
    "        \n",
    "        Parameters:\n",
    "        env : gymnasium environment\n",
    "            - Can be either the self.env_test or self.env_val environment\n",
    "\n",
    "        '''\n",
    "\n",
    "        total_reward = 0\n",
    "        for i in range(n_episode_test):\n",
    "            obs,_ = env.reset()\n",
    "            done = False\n",
    "            eps_reward = 0\n",
    "\n",
    "            while not done:                 # Step thorugh the episode deterministically (no exploration)\n",
    "                action = self.eps_greedy_policy(env, obs, epsilon=0)\n",
    "                next_obs, reward, term, trunc, _ = env.step(action)\n",
    "\n",
    "                eps_reward += reward\n",
    "\n",
    "                obs = next_obs\n",
    "                done = term or trunc\n",
    "        \n",
    "            total_reward += eps_reward\n",
    "            if verbose:\n",
    "                print(f\"Validation episode {i+1:3d}/{n_episode_test}  |   Reward = {eps_reward:4.0f}\",end=\"\\r\")\n",
    "        average_reward = total_reward / n_episode_test\n",
    "        return average_reward\n",
    "    \n",
    "    # Helper function for EMA filters and plotting data\n",
    "    def EMA_filter(self, reward: list, alpha):\n",
    "        ''' Function that runs an exponential moving average filter along a datastream '''\n",
    "        output = np.zeros(len(reward)+1)\n",
    "        output[0] = reward[0]\n",
    "        for idx, item in enumerate(reward):\n",
    "            output[idx+1] = (1 - alpha) * output[idx] + alpha * item\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def plot_reward_hist(self, alpha = 0.1):\n",
    "        ''' Function that plots the reward and filtered reward per episode, then saves the plot in a specified save directory'''\n",
    "        n_episodes= len(self.reward_history)\n",
    "        episodes = range(n_episodes)\n",
    "        filtered_reward_hist = self.EMA_filter(self.reward_history, alpha)\n",
    "\n",
    "        plt.figure(figsize=(20,6))\n",
    "        plt.plot(episodes, self.reward_history[:n_episodes], color = \"blue\")\n",
    "        plt.plot(episodes, filtered_reward_hist[:n_episodes], color = \"red\")\n",
    "        plt.title(f'Total reward per episode - {self.hyperparam_config}')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.legend(['Total reward','Filtered reward'])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if self.save_path:\n",
    "            plt.savefig(os.path.join(self.save_path,'reward_history.png'))\n",
    "        plt.show()\n",
    "    \n",
    "    def save_model(self):\n",
    "        ''' Function to save the model and optimizer state_dict for inference or continued training '''\n",
    "        self.model_path = os.path.join(self.save_path, 'q_network_checkpoint.pth')\n",
    "        torch.save({\n",
    "            'model_state_dict': self.Q_net.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, self.model_path)\n",
    "\n",
    "    def load_model(self):\n",
    "        ''' This code overwrite the Q_net with the parameters store in the instance's save_path '''\n",
    "        checkpoint = torch.load(self.model_path)\n",
    "        self.Q_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    def DDQN_train(self):\n",
    "        ''' Function to train a policy for a set of hyperparameters '''\n",
    "\n",
    "        msg = \"Training ended, no good model found!\"        # Default msg, changes during training as models are saved\n",
    "        \n",
    "        self.replay_buffer = deque(maxlen = self.buffer_size)\n",
    "        self.prefill_replay()\n",
    "\n",
    "        # Create the directory to store results\n",
    "        self.run_number, self.hyperparam_config, self.save_path = self.create_directory()\n",
    "\n",
    "        # Training information\n",
    "        if self.verbose: print(f'model {self.model_name}, lr={self.lr}, buffer={self.buffer_size}, target_freq={self.target_update_freq}, gamma={self.gamma}, eps_decay={self.eps_decay}, batch_size={self.batch_size}')\n",
    "\n",
    "        self.reward_history = np.zeros(self.episode_train)\n",
    "        self.eps = self.eps_start\n",
    "        step_count = 0\n",
    "        episode = 0\n",
    "        target_network_update_count = 0\n",
    "\n",
    "        # Control of early stopping\n",
    "        consecutive_pass_count = 0           # Number of consecutive episodes where performance exceeds a threshold\n",
    "        CONSECUTIVE_PASS_LIMIT = 3          # No. of consecutive episodes with higher performance than reward limit\n",
    "        EPISODE_REWARD_LIMIT = SUCCESS_CRITERIA\n",
    "        best_reward = 0\n",
    "        performance_crit = False\n",
    "        train_terminated = False\n",
    "        self.val_history = {}               # Clear the validationi history\n",
    "        self.val_time = 0                   # Reset the total validation time\n",
    "\n",
    "        \n",
    "        self.train_time_start = time.time()\n",
    "        while not train_terminated:     # Experiment level - loop through episodes\n",
    "            obs, _ = self.env.reset(seed=self.seed)\n",
    "            eps_rewards = 0\n",
    "        \n",
    "            while True:                 # Episode level - loop through steps\n",
    "                action = self.eps_greedy_policy(self.env, obs, epsilon = self.eps)\n",
    "\n",
    "                # Interact with the environment\n",
    "                next_obs, reward, term, trunc, _ = self.env.step(action)\n",
    "                done = term or trunc\n",
    "                self.replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "                obs = next_obs\n",
    "                eps_rewards += reward\n",
    "                step_count += 1\n",
    "\n",
    "                # Train the Q-net using a batch of samples from the experience replay\n",
    "                if len(self.replay_buffer) >= self.batch_size:\n",
    "                    batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "                    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "                    if self.cuda_enabled:\n",
    "                        states =        torch.tensor(np.array(states),      dtype = torch.float32,  device='cuda')           # Shape of (BATCH_SIZE, 4). Data structure of [[p1 v1 theta1 dtheta1], [p2 v2 theta2 dtheta2], ...]\n",
    "                        actions =       torch.tensor(actions,               dtype = torch.long,     device='cuda').unsqueeze(1)        # Must unsqueeze to have shape (BATCH_SIZE, 1). From [a1, a2, ...] to [[a1], [a2], ... [a_BATCH_SIZE]]\n",
    "                        rewards =       torch.tensor(rewards,               dtype = torch.float32,  device='cuda').unsqueeze(1)\n",
    "                        next_states =   torch.tensor(np.array(next_states), dtype = torch.float32,  device = 'cuda')\n",
    "                        dones =         torch.tensor(dones,                 dtype = torch.float32,  device='cuda').unsqueeze(1)\n",
    "                    else:\n",
    "                        states = torch.FloatTensor(np.array(states))                                          # Shape of (BATCH_SIZE, 4). Data structure of [[p1 v1 theta1 dtheta1], [p2 v2 theta2 dtheta2], ...]\n",
    "                        actions = torch.LongTensor(actions).unsqueeze(1)                            # Must unsqueeze to have shape (BATCH_SIZE, 1). From [a1, a2, ...] to [[a1], [a2], ... [a_BATCH_SIZE]]\n",
    "                        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "                        next_states = torch.FloatTensor(np.array(next_states))\n",
    "                        dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "\n",
    "                    # Compute targets using target network Q(s',a',w_i^-)\n",
    "                    # TODO - Change this from DQN to DDQN code\n",
    "                    with torch.no_grad():\n",
    "                        # Select the maximizing action according to the online behaviour net\n",
    "                        optimal_next_actions_online = self.Q_net(next_states).argmax(dim=1,keepdim=True)\n",
    "                        \n",
    "                        # Find the target Q value of the maximizing action according to the target net\n",
    "                        target_q_values = self.target_net(next_states).gather(1,optimal_next_actions_online)\n",
    "                        \n",
    "                        targets = rewards + self.gamma * target_q_values * (1 - dones)       # (1-dones) to avoid over estimating the value of terminal states. The target should only be the reward in the terminal states   # (1-dones) to avoid over estimating the value of terminal states. The target should only be the reward in the terminal states\n",
    "                \n",
    "                    # Compute the current Q values for the actions taken Q(s,a,w_i)\n",
    "                    q_values = self.Q_net(states).gather(1, actions)         # obtain the q_values associated to the actual action taken in each sample\n",
    "\n",
    "                    # Update the parameters of the behaviour q_network\n",
    "                    loss = nn.MSELoss()(q_values, targets)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                # Periodically update the target network by loading the weights from the behavior network\n",
    "                if step_count % self.target_update_freq == 0:\n",
    "                    target_network_update_count += 1\n",
    "                    self.target_net.load_state_dict(self.Q_net.state_dict())\n",
    "\n",
    "                if done:        # End of a training episode\n",
    "                    break\n",
    "\n",
    "            # Decay epsilon after an episode\n",
    "            self.eps = max(self.eps_end, self.eps - (self.eps_start - self.eps_end)/self.eps_decay)\n",
    "\n",
    "            self.reward_history[episode] = eps_rewards\n",
    "            # print(f\"Episode {episode:5d}: Total reward = {eps_rewards:5.1f}, Epsilon = {epsilon:.3f}, Step count = {step_count:5d}, Target update count = {target_network_update_count:3d}\")\n",
    "            # print(f\"Average reward {sum(reward_history[:episode])/(episode+1):.2f}\")\n",
    "\n",
    "            \n",
    "            if episode % 10 == 0 and self.verbose:                   # print progress periodically\n",
    "                print(f\"Episode {episode:5d}/{self.episode_train}: Total reward = {eps_rewards:5.1f}, Epsilon = {self.eps:.3f}\", end = \"\\r\")\n",
    "\n",
    "            # Early stopping condition\n",
    "            if eps_rewards >= EPISODE_REWARD_LIMIT:\n",
    "                # Evaluate the current good policy and time the validation time\n",
    "                self.val_time_start = time.time()\n",
    "                test_reward = self.policy_eval(self.env_val, 100,verbose=self.verbose)\n",
    "                self.val_time += time.time() - self.val_time_start\n",
    "\n",
    "                self.val_history[episode] = [eps_rewards, test_reward]\n",
    "\n",
    "                if test_reward >= best_reward:           # Set the new best reward\n",
    "                    best_reward = test_reward\n",
    "                    self.save_model()\n",
    "                    self.best_model_episode = episode\n",
    "                    msg = f\"Training terminated due to episode limit, best model saved at episode {self.best_model_episode:5d}\"\n",
    "                if test_reward > EPISODE_REWARD_LIMIT: \n",
    "                    consecutive_pass_count += 1\n",
    "                else: consecutive_pass_count = 0\n",
    "            else:\n",
    "                consecutive_pass_count = 0\n",
    "                \n",
    "            # Performance criteria - if good results for several episodes => training performance satisfied and terminate early\n",
    "            if consecutive_pass_count >= CONSECUTIVE_PASS_LIMIT:\n",
    "                self.save_model()\n",
    "                self.best_model_episode = episode\n",
    "                performance_crit = True \n",
    "                msg = f\"Early termination at episode {self.best_model_episode:5d}, desired performance reached\"\n",
    "\n",
    "\n",
    "            episode += 1\n",
    "\n",
    "            # Checking for early training termination or truncation\n",
    "            train_terminated = (episode >= self.episode_train) or (performance_crit)\n",
    "        self.train_time = time.time() - self.train_time_start\n",
    "\n",
    "        print(f\"\\nRuntime - {self.train_time:5.2f}\")\n",
    "        print(msg)\n",
    "        return\n",
    "\n",
    "    def DDQN_record(self):\n",
    "        ''' Method to plot the reward history and store the data in the current run folder '''\n",
    "\n",
    "        # Load the best Q net parameter from the experiment\n",
    "        self.load_model()\n",
    "\n",
    "        # Average test reward of the resulting policy\n",
    "        self.env_test = gym.make('CartPole-v1')\n",
    "        print('Testing the Q_net')\n",
    "        average_reward = self.policy_eval(self.env_test, n_episode_test=500, verbose=1)\n",
    "        print(f\"\\nValidation average reward {average_reward:4.2f}\")\n",
    "\n",
    "        # Store the validation hisotry and average test reward in the param_config JSON file\n",
    "        param_path = os.path.join(self.save_path,'param_config.json')\n",
    "        if os.path.exists(param_path):\n",
    "            with open(param_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "        else:\n",
    "            data = {}\n",
    "        data['runtime'] = self.train_time\n",
    "        data['valtime'] = self.val_time\n",
    "        data['best_model_at'] = self.best_model_episode\n",
    "        data['val_history'] = self.val_history\n",
    "        data['test_result'] = average_reward\n",
    "\n",
    "        with open(param_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "        # Plot\n",
    "        self.plot_reward_hist()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8263cb1c",
   "metadata": {},
   "source": [
    "## DDQN Training (OOP, non CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694b238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "# DQN Training\n",
    "experiment = DDQN_experiment('DQN_MLP_v0', model_registry, \n",
    "                             LR, \n",
    "                             BUFFER_SIZE, \n",
    "                             TARGET_UPDATE_FREQ, \n",
    "                             GAMMA, \n",
    "                             EPS_START, EPS_DECAY, EPS_END, \n",
    "                             BATCH_SIZE, 42, cuda_enabled=CUDA_ENABLED, verbose = VERBOSE)\n",
    "\n",
    "# # Check whether the model is using CUDA\n",
    "# next(experiment.Q_net.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7233089",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NUM_RUN):\n",
    "    print(f\"Run {i+1:4d}/{NUM_RUN} ------------------\")\n",
    "    experiment.DDQN_train()\n",
    "    experiment.DDQN_record()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cfeec5",
   "metadata": {},
   "source": [
    "Miscellaneous code to gather the training results from all OOP, non-CUDA runs to calculate the average runtime and training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3cfc58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_times(experiment: str, \n",
    "              results_dir):\n",
    "    ''' Function to extract the run/val/train times of a computation time experiment \n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    \n",
    "    experiment : str\n",
    "        the indicator of the experiment and can be one of [\"nOOP_nCUDA\", \"nOOP_CUDA\", \"OOP_nCUDA\", \"OOP_CUDA\"]\n",
    "\n",
    "    results_dir\n",
    "        path to the directory that contains all the run directories (run_00001, run_00002) and the trial_to_param.json file\n",
    "    \n",
    "    Return:\n",
    "    ----------\n",
    "    result \n",
    "        A numpy array of size (3 x number of run) containing the run time (1st row), validation time (2nd row), and train time (3rd row)\n",
    "    '''\n",
    "\n",
    "    if experiment not in [\"nOOP_nCUDA\", \"nOOP_CUDA\", \"OOP_nCUDA\", \"OOP_CUDA\"]:\n",
    "        print(\"Please recheck the experiment name\")\n",
    "        print(\"experiment can only be one of can be one of [\\\"nOOP_nCUDA\\\", \\\"nOOP_CUDA\\\", \\\"OOP_nCUDA\\\", \\\"OOP_CUDA\\\"]\")\n",
    "        return\n",
    "\n",
    "    # Load the data from trial_to_param.json file\n",
    "    with open(os.path.join(results_dir,'trial_to_param.json'),'r') as f:\n",
    "        run_data = json.load(f)\n",
    "\n",
    "    # Filter the runs that has the string 'OOP_nCUDA'\n",
    "    oop_ncuda_runs = [run for run, desc in run_data.items() if 'OOP_nCUDA' in desc]\n",
    "\n",
    "\n",
    "    # Extract the data from each run\n",
    "    oop_ncuda_run_times = np.zeros(len(oop_ncuda_runs))\n",
    "    oop_ncuda_val_times = np.zeros(len(oop_ncuda_runs))\n",
    "    oop_ncuda_train_times = np.zeros(len(oop_ncuda_runs))\n",
    "\n",
    "    for idx, run in enumerate(oop_ncuda_runs):\n",
    "        param_path = os.path.join(results_dir, run, 'param_config.json')\n",
    "\n",
    "        try:\n",
    "            with open(param_path, 'r') as f:\n",
    "                config = json.load(f)\n",
    "            \n",
    "            runtime = config.get('runtime'); valtime = config.get('valtime')\n",
    "            oop_ncuda_run_times[idx] = runtime\n",
    "            oop_ncuda_val_times[idx] = valtime\n",
    "            if runtime and valtime:\n",
    "                oop_ncuda_train_times[idx] = runtime - valtime\n",
    "            else:   # In case of missing runtime and valtime\n",
    "                oop_ncuda_train_times[idx] = None\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"{run}: param_connfig.json not found\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"{run}: Error reading param_config.json\")\n",
    "        \n",
    "    result = np.vstack((oop_ncuda_run_times,oop_ncuda_val_times,oop_ncuda_train_times))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9439c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULTS_DIR = os.path.join(os.getcwd(),'comp_time_results')\n",
    "OOP_nCUDA_times = get_times('OOP_nCUDA', RESULTS_DIR)\n",
    "OOP_nCUDA_train_times = OOP_nCUDA_times[2]\n",
    "OOP_nCUDA_train_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e084f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb8c1f67",
   "metadata": {},
   "source": [
    "## DDQN Training (non-OOP, non cuda)\n",
    "\n",
    "This portion of DDQN training does not use the DDQN_experiment class but simply explicit code. The model and computation all runs on cpu and not cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "051928dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(model_id: int,\n",
    "                     lr: float, \n",
    "                     gamma: float,\n",
    "                     epsilon_decay: int,\n",
    "                     batch_size: int, \n",
    "                     buffer_size: int,\n",
    "                     target_update_freq: int):\n",
    "    ''' Function that creates directory to save model state_dict, architecture, training configuration, and history\n",
    "\n",
    "    Parameters: \n",
    "    ------------\n",
    "    (hyperparameters for differentiating between different directory)\n",
    "    \n",
    "    lr : float\n",
    "        the learning rate to optimize the Q network\n",
    "    gamma : float \n",
    "        the discount rate in Q learning\n",
    "    epsilon_decay : integer\n",
    "        the amount of episode over which the exploratory rate (epsilon) decays\n",
    "    batch_size : integer\n",
    "        number of experience drawn from replay buffer to train the behaviour network\n",
    "    buffer_size : integer\n",
    "        the number of samples in the replay buffer at a time\n",
    "    target_udpate_freq : integer\n",
    "        the amount of step count during the training process before updating the target Q net (loading the parameters of the behaviour net onto the target Q Net)\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    ------------\n",
    "    name_codified : str\n",
    "        the shortened name for the current experiment \n",
    "    hyperparameters_codified : str\n",
    "        the shortened string of hyperparameter configuration\n",
    "    OUTPUT_DIR : path\n",
    "        the directory to which the training results and model (state_dict and architecture) will be saved\n",
    "    '''\n",
    "    timestamp = datetime.now().strftime(\"%y%m%d_%H%M\")\n",
    "\n",
    "    BASE_DIR = os.getcwd()\n",
    "    RESULT_DIR = os.path.join(BASE_DIR, \"comp_time_results\")\n",
    "    os.makedirs(RESULT_DIR, exist_ok=True)      # Create the directory if one does not already exist\n",
    "\n",
    "    # Find the trial # of the latest run\n",
    "    existing_runs = [d for d in os.listdir(RESULT_DIR) if os.path.isdir(os.path.join(RESULT_DIR,d))]\n",
    "    run_numbers = [int(re.search(r'run_(\\d{5})',d).group(1)) for d in existing_runs if re.match(r'run_\\d{5}',d)]\n",
    "    trial_number = max(run_numbers,default=-1)+1\n",
    "\n",
    "    # Create a folder for the run\n",
    "    name_codified = f\"run_{trial_number:05d}\"\n",
    "    OUTPUT_DIR = os.path.join(RESULT_DIR,name_codified)\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)      # Create the directory\n",
    "\n",
    "    # Append the mapping from run # to hyperparameter configuration in a JSON file inside RESULT_DIR\n",
    "    trial_to_param_path = os.path.join(RESULT_DIR,'trial_to_param.json')\n",
    "    if os.path.exists(trial_to_param_path):\n",
    "        with open(trial_to_param_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        data = {name_codified: []}\n",
    "    if CUDA_ENABLED:\n",
    "        hyperparam_codified = f\"DDQN_nOOP_CUDA_{model_id}_{lr}_{buffer_size}_{target_update_freq}_{gamma}_{epsilon_decay}_{batch_size}\"\n",
    "        hyperparam_codified_time = f\"{timestamp}_DDQN_nOOP_CUDA_{model_id}_{lr}_{buffer_size}_{target_update_freq}_{gamma}_{epsilon_decay}_{batch_size}\"\n",
    "    else:\n",
    "        hyperparam_codified = f\"DDQN_nOOP_nCUDA_{model_id}_{lr}_{buffer_size}_{target_update_freq}_{gamma}_{epsilon_decay}_{batch_size}\"\n",
    "        hyperparam_codified_time = f\"{timestamp}_DDQN_nOOP_nCUDA_{model_id}_{lr}_{buffer_size}_{target_update_freq}_{gamma}_{epsilon_decay}_{batch_size}\"\n",
    "    data[name_codified] = hyperparam_codified_time\n",
    "\n",
    "    with open(trial_to_param_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    \n",
    "    # Store the training configs in JSON file\n",
    "    training_params = {\n",
    "        'OOP':                  False,\n",
    "        'CUDA':                 CUDA_ENABLED,\n",
    "        'device':               torch.cuda.get_device_name(torch.cuda.current_device()),          \n",
    "        'model_id':             model_id,\n",
    "        'lr':                   lr,\n",
    "        'gamma':                gamma,\n",
    "        'epsilon_decay':        epsilon_decay,\n",
    "        'batch_size':           batch_size,\n",
    "        'buffer_size':          buffer_size,\n",
    "        'target_update_freq':   target_update_freq\n",
    "    }\n",
    "\n",
    "    # Store training parameters in each run \n",
    "    param_path = os.path.join(OUTPUT_DIR, \"param_config.json\")\n",
    "    with open(param_path, \"w\") as f:\n",
    "        json.dump({\"parameters\": training_params}, f, indent=2)\n",
    "\n",
    "    return name_codified, hyperparam_codified, OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26b088db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy_policy(env, obs, epsilon: float, q_network: QNet_MLP):\n",
    "    ''' Function to take an action according to an epsilon-greedy policy and a Q-network'''\n",
    "    if np.random.random() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            if CUDA_ENABLED:\n",
    "                state_tensor = torch.tensor(obs,dtype=torch.float32,device='cuda').unsqueeze(0)\n",
    "            else:\n",
    "                state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
    "            q_values = q_network(state_tensor)\n",
    "            action = q_values.argmax().item()\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "482b9206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(env_test, q_network, n_episode_test = 500, verbose = False):\n",
    "    ''' Assess the average reward when following a q_network in a test environment with random state initialization '''\n",
    "    \n",
    "    total_reward = 0\n",
    "    for i in range(n_episode_test):\n",
    "        obs,_ = env_test.reset()\n",
    "        done = False\n",
    "        eps_reward = 0\n",
    "\n",
    "        while not done:                 # Step thorugh the episode\n",
    "            action = eps_greedy_policy(env_test, obs, epsilon=0, q_network=q_network)\n",
    "            next_obs, reward, term, trunc, _ = env_test.step(action)\n",
    "\n",
    "            eps_reward += reward\n",
    "\n",
    "            obs = next_obs\n",
    "            done = term or trunc\n",
    "    \n",
    "        total_reward += eps_reward\n",
    "        if verbose:\n",
    "                    print(f\"Validation episode {i+1:3d}/{n_episode_test}  |   Reward = {eps_reward:4.0f}\",end=\"\\r\")\n",
    "    average_reward = total_reward / n_episode_test\n",
    "\n",
    "    return average_reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df1ad87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(q_network: nn.Module, optimizer: torch.optim.Optimizer, save_path):\n",
    "    ''' Function to save the model and optimizer state_dict for inference or continued training '''\n",
    "    torch.save({\n",
    "        'model_state_dict': q_network.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, os.path.join(save_path, 'q_network_checkpoint.pth'))\n",
    "\n",
    "def load_model(q_network: nn.Module, model_path):\n",
    "    checkpoint = torch.load(model_path)\n",
    "    q_network.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e2b525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EMA_filter(reward: list, alpha):\n",
    "    ''' Function that runs an exponential moving average filter along a datastream '''\n",
    "    output = np.zeros(len(reward)+1)\n",
    "    output[0] = reward[0]\n",
    "    for idx, item in enumerate(reward):\n",
    "        output[idx+1] = (1 - alpha) * output[idx] + alpha * item\n",
    "    \n",
    "    return output\n",
    "\n",
    "def plot_reward_hist(reward_history: list, hyperparam_config: str, save_path = None, alpha = 0.1):\n",
    "    ''' Function that plots the reward and filtered reward per episode, then saves the plot in a specified save directory'''\n",
    "    n_episodes= len(reward_history)\n",
    "    episodes = range(n_episodes)\n",
    "    filtered_reward_hist = EMA_filter(reward_history, alpha)\n",
    "\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.plot(episodes, reward_history[:n_episodes], color = \"blue\")\n",
    "    plt.plot(episodes, filtered_reward_hist[:n_episodes], color = \"red\")\n",
    "    plt.title(f'Total reward per episode - {hyperparam_config}')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend(['Total reward','Filtered reward'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(os.path.join(save_path,'reward_history.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b980d648",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASS = model_registry[MODEL_NAME]['class']\n",
    "MODEL_CONFIG = model_registry[MODEL_NAME]['config']\n",
    "match = re.search(r'v\\d+',MODEL_NAME)\n",
    "MODEL_ID = match.group(0) if match else 404\n",
    "\n",
    "# Initialize the environment and the Q network\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env_val = gym.make(\"CartPole-v1\")\n",
    "obs_space = env.observation_space.shape[0]        # State space of cart pos & vel, and pendulum angular pos and vel. Each component is a continuous value\n",
    "action_space =  env.action_space.n                   # Action space of left or right\n",
    "\n",
    "Q_net = MODEL_CLASS(obs_space,action_space, MODEL_CONFIG)\n",
    "target_net = MODEL_CLASS(obs_space,action_space, MODEL_CONFIG)\n",
    "target_net.load_state_dict(Q_net.state_dict())\n",
    "target_net.eval()                               # target network is only used for evaluation/inference and not trained\n",
    "\n",
    "optimizer = optim.SGD(Q_net.parameters(), lr = LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a467025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run    1/100 | run_00002 ------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 102\u001b[0m\n\u001b[0;32m    100\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()(q_values, targets)\n\u001b[0;32m    101\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 102\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Periodically update the target network\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\caomi\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\caomi\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\caomi\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_RUN = NUM_RUN\n",
    "for i in range(NUM_RUN):\n",
    "\n",
    "    msg = \"Training ended, no good model found!\"        # Default msg, changes during training as models are saved\n",
    "\n",
    "    #Initialize and pre-fill the replay buffer\n",
    "    replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "    obs, _ = env.reset()\n",
    "    for _ in range(MIN_REPLAY_SIZE):\n",
    "        action = env.action_space.sample()\n",
    "        next_obs, reward, term, trunc, _ = env.step(action)\n",
    "        done = term or trunc\n",
    "\n",
    "        replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "        obs = next_obs if not done else env.reset()[0]\n",
    "\n",
    "    # Create the directory to store results\n",
    "    run_number, hyperparam_config, save_path = create_directory(MODEL_ID,LR,GAMMA,EPS_DECAY,BATCH_SIZE,BUFFER_SIZE,TARGET_UPDATE_FREQ)\n",
    "\n",
    "    # Training information\n",
    "    print(f\"Run {i+1:4d}/{NUM_RUN} | {run_number} ------------------\")\n",
    "    if VERBOSE:\n",
    "        print(f'model {MODEL_NAME}, lr={LR}, buffer={BUFFER_SIZE}, target_freq={TARGET_UPDATE_FREQ}, gamma={GAMMA}, eps_decay={EPS_DECAY}, batch_size={BATCH_SIZE}')\n",
    "\n",
    "    # DDQN Model Training loop\n",
    "    reward_history = np.zeros(EPISODE_TRAIN)\n",
    "    epsilon = EPS_START\n",
    "    step_count = 0\n",
    "    episode = 0\n",
    "    target_network_update_count = 0\n",
    "\n",
    "    # Control of early stopping\n",
    "    consecutive_pass_count = 0           # Number of consecutive episodes where performance exceeds a threshold\n",
    "    CONSECUTIVE_PASS_LIMIT = 3          # No. of consecutive episodes with higher performance than reward limit\n",
    "    EPISODE_REWARD_LIMIT = SUCCESS_CRITERIA\n",
    "    best_reward = 0\n",
    "    performance_crit = False\n",
    "    train_terminated = False\n",
    "    val_history = {}                    # Monitor which episode had a validation run, the train reward, and the validation (test) reward \n",
    "    val_time = 0\n",
    "\n",
    "    train_time_start = time.time()\n",
    "    while not train_terminated:     # Experiment level - loop through episodes\n",
    "        obs, _ = env.reset()\n",
    "        eps_rewards = 0\n",
    "        \n",
    "\n",
    "        while True:\n",
    "            # Epsilon-greedy policy to select action\n",
    "            if np.random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():       # Doing inference so no need to track operations\n",
    "                    if CUDA_ENABLED:\n",
    "                        state_tensor = torch.tensor(obs, dtype=torch.float32, device='cuda').unsqueeze(0)  # unsqueeze to fake the sample (batch) dimension\n",
    "                    else:\n",
    "                        state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
    "                    q_values = Q_net(state_tensor)\n",
    "                    action = q_values.argmax().item()                   # index of the maximum output (action)\n",
    "            \n",
    "            # Interact with the environment\n",
    "            next_obs, reward, term, trunc, _ = env.step(action)\n",
    "            done = term or trunc\n",
    "            replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "            obs = next_obs\n",
    "            eps_rewards += reward\n",
    "            step_count += 1\n",
    "\n",
    "            # Sample a batch and update the network\n",
    "            if len(replay_buffer) >= BATCH_SIZE:\n",
    "                batch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "                if CUDA_ENABLED:\n",
    "                    states =        torch.tensor(np.array(states),      dtype = torch.float32,  device='cuda')           # Shape of (BATCH_SIZE, 4). Data structure of [[p1 v1 theta1 dtheta1], [p2 v2 theta2 dtheta2], ...]\n",
    "                    actions =       torch.tensor(actions,               dtype = torch.long,     device='cuda').unsqueeze(1)        # Must unsqueeze to have shape (BATCH_SIZE, 1). From [a1, a2, ...] to [[a1], [a2], ... [a_BATCH_SIZE]]\n",
    "                    rewards =       torch.tensor(rewards,               dtype = torch.float32,  device='cuda').unsqueeze(1)\n",
    "                    next_states =   torch.tensor(np.array(next_states), dtype = torch.float32,  device = 'cuda')\n",
    "                    dones =         torch.tensor(dones,                 dtype = torch.float32,  device='cuda').unsqueeze(1)   \n",
    "                else:\n",
    "                    states =        torch.FloatTensor(np.array(states))           # Shape of (BATCH_SIZE, 4). Data structure of [[p1 v1 theta1 dtheta1], [p2 v2 theta2 dtheta2], ...]\n",
    "                    actions =       torch.LongTensor (actions).unsqueeze(1)        # Must unsqueeze to have shape (BATCH_SIZE, 1). From [a1, a2, ...] to [[a1], [a2], ... [a_BATCH_SIZE]]\n",
    "                    rewards =       torch.FloatTensor(rewards).unsqueeze(1)\n",
    "                    next_states =   torch.FloatTensor(np.array(next_states))\n",
    "                    dones =         torch.FloatTensor(dones).unsqueeze(1)\n",
    "\n",
    "                # Compute targets using target network Q(s',a',w_i^-)\n",
    "                with torch.no_grad():\n",
    "                    # Select the maximizing action according to the online behaviour net\n",
    "                            optimal_next_actions_online = Q_net(next_states).argmax(dim=1,keepdim=True)\n",
    "                            \n",
    "                            # Find the target Q value of the maximizing action according to the target net\n",
    "                            target_q_values = target_net(next_states).gather(1,optimal_next_actions_online)\n",
    "                            \n",
    "                            targets = rewards + GAMMA * target_q_values * (1 - dones)       # (1-dones) to avoid over estimating the value of terminal states. The target should only be the reward in the terminal states   # (1-dones) to avoid over estimating the value of terminal states. The target should only be the reward in the terminal states\n",
    "\n",
    "                # Compute the current Q values for the actions taken Q(s,a,w_i)\n",
    "                q_values = Q_net(states).gather(1, actions)                             # obtain the q_values associated to the actual action taken in each sample\n",
    "\n",
    "                # Update the parameters of the behaviour q_network\n",
    "                loss = nn.MSELoss()(q_values, targets)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Periodically update the target network\n",
    "                if step_count % TARGET_UPDATE_FREQ == 0:\n",
    "                    target_network_update_count += 1\n",
    "                    target_net.load_state_dict(Q_net.state_dict())\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "    \n",
    "        # Decay epsilon after an episode\n",
    "        epsilon = max(EPS_END, epsilon - (EPS_START - EPS_END)/EPS_DECAY)\n",
    "\n",
    "        reward_history[episode] = eps_rewards\n",
    "        # print(f\"Episode {episode:5d}: Total reward = {eps_rewards:5.1f}, Epsilon = {epsilon:.3f}, Step count = {step_count:5d}, Target update count = {target_network_update_count:3d}\")\n",
    "        # print(f\"Average reward {sum(reward_history[:episode])/(episode+1):.2f}\")\n",
    "\n",
    "        \n",
    "        if episode % 10 == 0 and VERBOSE:                   # print progress periodically\n",
    "            print(f\"Episode {episode:5d}: Total reward = {eps_rewards:5.1f}, Epsilon = {epsilon:.3f}\", end = \"\\r\")\n",
    "\n",
    "        # Early stopping condition\n",
    "        if eps_rewards >= EPISODE_REWARD_LIMIT:\n",
    "            val_time_start = time.time()\n",
    "            test_reward = policy_eval(env_val, Q_net, 100)\n",
    "            val_time += time.time() - val_time_start\n",
    "\n",
    "            val_history[episode] = [eps_rewards, test_reward]\n",
    "\n",
    "            if test_reward >= best_reward:           # Set the new best reward\n",
    "                best_reward = test_reward\n",
    "                save_model(Q_net, optimizer, save_path)\n",
    "                best_model_episode = episode\n",
    "                msg = f\"Training terminated due to episode limit, best model saved at episode {best_model_episode:5d}\"\n",
    "            if test_reward > EPISODE_REWARD_LIMIT: \n",
    "                consecutive_pass_count += 1\n",
    "            else: consecutive_pass_count = 0\n",
    "        else:\n",
    "            consecutive_pass_count = 0\n",
    "            \n",
    "        # Performance criteria - if good results for several episodes => training performance satisfied and terminate early\n",
    "        if consecutive_pass_count >= CONSECUTIVE_PASS_LIMIT:\n",
    "            save_model(Q_net, optimizer, save_path)\n",
    "            best_model_episode = episode\n",
    "            performance_crit = True \n",
    "            msg = f\"Early termination at episode {best_model_episode:5d}, desired performance reached\"\n",
    "\n",
    "\n",
    "        episode += 1\n",
    "\n",
    "        # Checking for early training termination or truncation\n",
    "        train_terminated = (episode >= EPISODE_TRAIN) or (performance_crit)\n",
    "    \n",
    "    train_time = time.time()-train_time_start\n",
    "    print(f\"\\nRuntime - {train_time:5.2f}\")\n",
    "    print(msg)\n",
    "    env.close()\n",
    "\n",
    "    ## Post training stuff\n",
    "    # Load the best model\n",
    "    model_path = os.path.join(save_path,'q_network_checkpoint.pth')\n",
    "    load_model(Q_net,model_path)\n",
    "\n",
    "    # Average test reward of the resulting policy\n",
    "    env_test = gym.make('CartPole-v1')\n",
    "    print('Testing the Q_net')\n",
    "    average_reward = policy_eval(env_test, Q_net, n_episode_test=500,verbose = True)\n",
    "    print(f\"\\nValidation average reward {average_reward:4.2f}\")\n",
    "\n",
    "    # Store the validation hisotry and average test reward in the param_config JSON file\n",
    "    param_path = os.path.join(save_path,'param_config.json')\n",
    "    if os.path.exists(param_path):\n",
    "        with open(param_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        data = {}\n",
    "    data['runtime'] = train_time\n",
    "    data['valtime'] = val_time\n",
    "    data['best_model_at'] = best_model_episode\n",
    "    data['val_history'] = val_history\n",
    "    data['test_result'] = average_reward\n",
    "\n",
    "    with open(param_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "    # Plot\n",
    "    plot_reward_hist(reward_history, hyperparam_config, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b30528",
   "metadata": {},
   "source": [
    "# Load an Simulate Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7a1672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(q_network: nn.Module, model_path):\n",
    "    checkpoint = torch.load(model_path)\n",
    "    q_network.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207111cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy_policy(env, obs, epsilon: float, q_network: QNet_MLP):\n",
    "    ''' Function to take an action according to an epsilon-greedy policy and a Q-network'''\n",
    "    if np.random.random() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
    "            q_values = q_network(state_tensor)\n",
    "            action = q_values.argmax().item()\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06effa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model after training\n",
    "\n",
    "# Manually select a folder/run to load\n",
    "run_number = 'run_00008'\n",
    "\n",
    "\n",
    "# Find the paths to the param_config and model checkpoint\n",
    "RESULT_DIR = os.path.dirname(experiment.save_path)\n",
    "RUN_DIR = os.path.join(RESULT_DIR, run_number)\n",
    "MODEL_PATH = os.path.join(RUN_DIR,'q_network_checkpoint.pth')\n",
    "\n",
    "# Find the model configuration\n",
    "with open(os.path.join(RUN_DIR,\"param_config.json\"),'r') as f:\n",
    "    data = json.load(f)\n",
    "model_id = 'DQN_MLP_' + data['parameters']['model_id']\n",
    "model_config = model_registry[model_id]['config']\n",
    "\n",
    "# Create the model according to the model version in the model registry\n",
    "q_network_loaded = QNet_MLP(experiment.obs_space, experiment.act_space, model_config)\n",
    "load_model(q_network_loaded, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0f9e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual simulation of the network\n",
    "env_test_visual = gym.make(\"CartPole-v1\",render_mode=\"human\")\n",
    "num_test = 5\n",
    "\n",
    "for episode in range(num_test):\n",
    "    obs, _ = env_test_visual.reset()\n",
    "    done = False\n",
    "    eps_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = eps_greedy_policy(env_test_visual, obs, epsilon = 0, q_network=q_network_loaded)\n",
    "        next_obs, reward, term, trunc, _ = env_test_visual.step(action)\n",
    "\n",
    "        done = term or trunc\n",
    "        eps_reward += reward\n",
    "        obs = next_obs\n",
    "    \n",
    "    print(f\"Reward from episode {episode:3d} is {eps_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93adb7c4",
   "metadata": {},
   "source": [
    "## Blah"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
