{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e0cac5",
   "metadata": {},
   "source": [
    "# Inverted Pendulum using DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1079ee",
   "metadata": {},
   "source": [
    "When moving on from discrete environments, such as grid world or black jack, the amount of state in a continous environment renders traditional tabular methods (Monte Carlo and TD learning) intractable (at least without any modification to the problem formulation). DQN method addresses this problem by using a value function approximator, essentially replacing the Q-table of explicit state-action value with a parameterized function $q^\\pi_{\\vec w}(s,a,\\vec w)$. The vector $\\vec w$ stores the weights or parameters of this approximating function.\n",
    "\n",
    "The motivation of using value function approximator is clear, but where does it fit into the RL picture?\n",
    "- Value function approximator is a direct mapping between the states (features) to the state-action value $Q(s,\\cdot)$ \n",
    "- As such, the approximator function fits in the policy evaluation step of the RL training process. We train this function by minimizing the loss between the approximated value function and the actual value function $ s.t. \\enspace q^\\pi_{\\vec w}(s,a,\\vec w) \\approx q^*$\n",
    "- However, the true action value function $q^*$ is unknown and must be learnt from the interaction between the agent and its environment. We must employ Q-learning to approximate the true action value function, toward which we will update the parameterized value function $q^\\pi_{\\vec w}(s,a,\\vec w)$. The update is as follow\n",
    "\n",
    "**The DQN algorithm**\n",
    "1. From a state $s$, take action $a$ using $\\epsilon$-greedy policy, observe the reward $r$ and next state $s'$\n",
    "2. Store the transition tuple $(s, a, r, s')$ in a replay memory $\\mathcal{D}$. This replay memory is a buffer that stores the most recent transition tuples\n",
    "3. From this replay memory, sample batches of $(s_t, a_t, r_{t+1}, s_{t+1})$ randomly for training the Q-function approximator\n",
    "4. Adjust the parameters $\\vec w$ by optimizing the mean squared error (MSE) between the Q network and the Q-learning target using Q-learning\n",
    "$$ \\mathcal{L}_i (\\vec w_i) = \\mathbb{E}_{(s,a,r,s') \\sim \\mathcal{D}} \\bigg[ \\Big(r + \\gamma \\max_{a'}Q(s',a',\\vec w^-_i) - Q(s,a,\\vec w_i) \\Big)^2\\bigg]$$ \n",
    "\n",
    "When applying stochastic gradient descent, the update at every step is reduced as follows (the math found in the GoodNote notebook):\n",
    "\n",
    "$$\\Delta \\vec w = \\alpha \\Big(r + \\gamma \\max_{a'} \\underbrace{Q(s', a', \\vec{w}_i^-)}_{\\text{target}} - \\underbrace{Q(s,a,\\vec w_i)}_{\\text{behaviour}} \\Big) \\vec x(s,a)$$\n",
    "\n",
    "\n",
    "**Notes:** \n",
    "\n",
    "There are two mechanisms introduced in this paper that allow for stable training, one is experience replay, and the other is training two separate methods.\n",
    "- **Experience replay** - this decorrelates the experience tuples (s,a,r,s') thus follow the i.i.d assumption for training a neural network\n",
    "- **Two Q-networks**, a target network and a behaviour network. The parameters $\\vec w'$ of the target network $Q(s,a,\\vec w')$ are only updated infrequently to maintain a stable target. The parameters $\\vec w$ of the behaviour network $Q(s,a,\\vec w)$ are updated every policy improvement iteration to move toward this target network. The frequency of updating the target network is annother hyperparameter to tune. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03179d1e",
   "metadata": {},
   "source": [
    "## Question to self\n",
    "1. What does it mean to apply stochastic gradient descent to simplify the loss function? What are the implications?\n",
    "- Every policy evaluation iteration, a random sample, or batch of samples, is used for updating the state-action value instead of every single experience stored in the replay buffer. Recall that the random sampling from the replay buffer de-correlates the experience and enforces the i.i.d. assumption\n",
    "- SGD is used on a small batch of sample to reduce the variance inherent in the different experience in the replay buffer. Despite the noise and bias of this method, the performance of the Q networks improves.\n",
    "\n",
    "\n",
    "</br>\n",
    "\n",
    "2. Would it be beneficial to formulate the Q network such that the input is the state vector and the outputs are the Q-values associated with each possible action from the state. As such, one can just take the max value from the target Q network and index into the behaviour network for the corresponding action a?\n",
    "- That is the common approach for approximating the action value function, outputting Q(s) for all a, then grabbing the maximum value\n",
    " \n",
    "</br>\n",
    "\n",
    "3. At first do I have to run the agent several times using the initial $\\epsilon$-greedy policy to fill the memory buffer with training samples?\n",
    "- Yes, and then pop in new experience at the end, push the oldest experience in the replay buffer out.\n",
    "\n",
    "</br>\n",
    "\n",
    "4. How often does the behaviour target network updated?\n",
    "\n",
    "- At every step in an episode of the training process, the behaviour network is used for two different purposes:\n",
    "    - Firstly, the behaviour network governs the trajectory of the agent at every step. \n",
    "    - Secondly, the behaviour Q network is used to predict (or infer) the action value for the <s,a> pairs drawn from a batch of experience (s,a,r,s'). These value is compared to the discounted maximum action value at state s', $\\max Q(s',\\cdot)$, according to the **target network**. \n",
    "\n",
    "- The behaviour network prediction of $Q(s,a)$ and the target network's label output of $r+\\gamma \\max Q(s',a')$ are used to compute a loss function. The gradient of this loss function is used for adjusting the parameters of the behaviour network.\n",
    "- After the agent has taken a number of step, the parameters (or weights) or the behaviour network is loaded onto the target network.\n",
    "\n",
    "</br>\n",
    "\n",
    "5. How can I scale this environment up to have continuous action instead of just left/right?\n",
    "- To my understanding, actuating the cart to the left or right at similar output level (voltage) at instantaneous moments throughout the control process is good enough for modulating the speed of the cart. Think of PWMs where the voltage is just switch at a certain duty cycle to simulate different voltage level.\n",
    "- It is possible to have continuous action space. Since the Q network output is the state-action value Q(s,a) at each state s and action a, we know how good each action is in a state. The Q values can then be transformed into a proportional constant to control how much to move left or right (instead of just a binary left/right). This essentially means that the Q-value output of this network is not simply used for selecting the maximizing action at each state but also for governing the magnitude of cart actuation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d20579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random                       # To draw a random batch of samples form the replay buffer\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque       # replay buffer is a double ended queue that allows elements to be added either to the end or the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dafb2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate several episodes of the cart pole environment (random policy on a human render mode)\n",
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "\n",
    "state, _ = env.reset(); t = 1\n",
    "for i in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, term, trunc, info = env.step(action)\n",
    "    print(f\"Time {t:2d}  |  s_t {state}  |  a_t {action:2d}  |  s_t+1 {next_state}  |  reward {reward:.2f}  |  terminated {term:2}  |  {info}\")\n",
    "    state = next_state; t += 1\n",
    "    if term or trunc:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2ec82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant definitions\n",
    "LR = 1e-3\n",
    "BUFFER_SIZE = 5000                  # size of the experience replay buffer\n",
    "MIN_REPLAY_SIZE = 1000              # The number of experience to sample before starting training\n",
    "TARGET_UPDATE_FREQ = 1000           # the number of steps before updating the target network\n",
    "\n",
    "GAMMA = 0.95                        # Discount factor for return calculation\n",
    "EPSILON_START = 1.0                 # exploration rate in stochastic policy\n",
    "EPSILON_END = 0.1                   # minimum exploration rate at the end of decay\n",
    "EPSILON_DECAY = 5000                # epsilon decay rate\n",
    "\n",
    "EPISODE_TRAIN = 5000              # number of episodes to train the Q-network\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ce285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Q-network architecture\n",
    "class QNetwork(nn.Module):\n",
    "    ''' A QNetwork class that initialize an MLP neural Q network with two hidden layers, each with 128 nodes '''\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)                       # The regression output are the state values of Q(s,a=left) and Q(s,a=right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874662e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically define Q-network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8115dd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment and the Q network\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_space = env.observation_space.shape[0]        # State space of cart pos & vel, and pendulum angular pos and vel. Each component is a continuous value\n",
    "action_space =  env.action_space.n                   # Action space of left or right\n",
    "\n",
    "q_network = QNetwork(state_space,action_space)\n",
    "target_network = QNetwork(state_space,action_space)\n",
    "target_network.load_state_dict(q_network.state_dict())\n",
    "target_network.eval()                               # target network is only used for evaluation/inference and not trained\n",
    "\n",
    "optimizer = optim.SGD(q_network.parameters(), lr = LR)\n",
    "replay_buffer = deque(maxlen=BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be9e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill replay buffer with some initial samples for training (random policy)\n",
    "obs, _ = env.reset()\n",
    "for _ in range(MIN_REPLAY_SIZE):\n",
    "    action = env.action_space.sample()\n",
    "    next_obs, reward, term, trunc, _ = env.step(action)\n",
    "    done = term or trunc\n",
    "\n",
    "    replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "    obs = next_obs if not done else env.reset()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af1a257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Model Training loop\n",
    "reward_history = np.zeros(EPISODE_TRAIN)\n",
    "epsilon = EPSILON_START\n",
    "step_count = 0\n",
    "target_network_update_count = 0\n",
    "\n",
    "for episode in range(EPISODE_TRAIN):\n",
    "    obs, _ = env.reset()\n",
    "    eps_rewards = 0\n",
    "    \n",
    "\n",
    "    while True:\n",
    "        # Epsilon-greedy policy to select action\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():       # Doing inference so no need to track operations\n",
    "                state_tensor = torch.FloatTensor(obs).unsqueeze(0)  # unsqueeze to fake the sample (batch) dimension\n",
    "                q_values = q_network(state_tensor)\n",
    "                action = q_values.argmax().item()                   # index of the maximum output (action)\n",
    "        \n",
    "        # Interact with the environment\n",
    "        next_obs, reward, term, trunc, _ = env.step(action)\n",
    "        done = term or trunc\n",
    "        replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "        obs = next_obs\n",
    "        eps_rewards += reward\n",
    "        step_count += 1\n",
    "\n",
    "        # Sample a batch and update the network\n",
    "        if len(replay_buffer) >= BATCH_SIZE:\n",
    "            batch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.FloatTensor(states)                                          # Shape of (BATCH_SIZE, 4). Data structure of [[p1 v1 theta1 dtheta1], [p2 v2 theta2 dtheta2], ...]\n",
    "            actions = torch.LongTensor(actions).unsqueeze(1)                            # Must unsqueeze to have shape (BATCH_SIZE, 1). From [a1, a2, ...] to [[a1], [a2], ... [a_BATCH_SIZE]]\n",
    "            rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "            next_states = torch.FloatTensor(next_states)\n",
    "            dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "\n",
    "            # Compute targets using target network Q(s',a',w_i^-)\n",
    "            with torch.no_grad():\n",
    "                target_q_values = target_network(next_states)                           # Find a batch of Q(s',a',w_i^-) from the batch of next_states\n",
    "                max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]\n",
    "                targets = rewards + GAMMA * max_target_q_values * (1 - dones)           # (1-dones) to avoid over estimating the value of terminal states. The target should only be the reward in the terminal states\n",
    "\n",
    "            # Compute the current Q values for the actions taken Q(s,a,w_i)\n",
    "            q_values = q_network(states).gather(1, actions)                             # obtain the q_values associated to the actual action taken in each sample\n",
    "\n",
    "            # Update the parameters of the behaviour q_network\n",
    "            loss = nn.MSELoss()(q_values, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "             # Periodically update the target network\n",
    "            if step_count % TARGET_UPDATE_FREQ == 0:\n",
    "                target_network_update_count += 1\n",
    "                target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "   \n",
    "    # Decay epsilon\n",
    "    epsilon = max(EPSILON_END, epsilon - (EPSILON_START - EPSILON_END)/EPSILON_DECAY)\n",
    "\n",
    "    reward_history[episode] = eps_rewards\n",
    "    print(f\"Episode {episode:5d}: Total reward = {eps_rewards:5.1f}, Epsilon = {epsilon:.3f}, Step count = {step_count:5d}, Target update count = {target_network_update_count:3d}\")\n",
    "    print(f\"Average reward {sum(reward_history[:episode])/(episode+1):.2f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8145ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for EMA filters and plotting data\n",
    "def EMA_filter(reward: list, alpha):\n",
    "    ''' Function that runs an exponential moving average filter along a datastream '''\n",
    "    output = np.zeros(len(reward)+1)\n",
    "    output[0] = reward[0]\n",
    "    for idx, item in enumerate(reward):\n",
    "        output[idx+1] = (1 - alpha) * output[idx] + alpha * item\n",
    "    \n",
    "    return output\n",
    "\n",
    "def plot_reward_hist(reward_history: list, hyperparam_config: str, save_path = None, alpha = 0.1):\n",
    "    ''' Function that plots the reward and filtered reward per episode, then saves the plot in a specified save directory'''\n",
    "    n_episodes= len(reward_history)\n",
    "    episodes = range(n_episodes)\n",
    "    filtered_reward_hist = EMA_filter(reward_history, alpha)\n",
    "\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.plot(episodes, reward_history[:n_episodes], color = \"blue\")\n",
    "    plt.plot(episodes, filtered_reward_hist[:n_episodes], color = \"red\")\n",
    "    plt.title(f'Total reward per episode - {hyperparam_config}')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend(['Total reward','Filtered reward'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(os.path.join(save_path,'reward_history.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464bca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the history of success rate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "episode = range(EPISODE_TRAIN)\n",
    "filtered_reward_history = EMA_filter(reward_history, 0.1)\n",
    "\n",
    "# Set figure size (width, height) in inches\n",
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "slice_idx = len(reward_history)\n",
    "# Plot y1 vs x on the first subplot\n",
    "plt.plot(episode[:slice_idx], reward_history[:slice_idx], color = \"blue\")\n",
    "plt.plot(episode[:slice_idx], filtered_reward_history[:slice_idx], color = \"red\")\n",
    "plt.title('Success Rate vs Traing Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend(['Total reward', 'Filtered reward'])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c30e48e",
   "metadata": {},
   "source": [
    "## Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8d7de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                                               # For saving models and training results\n",
    "from datetime import datetime                           # For creating the directory of each training run\n",
    "import json                                             # For storing training parameters during each run\n",
    "\n",
    "# Generate a timestamped directory for the training run\n",
    "timestamp = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "BASE_DIR = os.getcwd()\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR,f\"inv_pend_results/{timestamp}_{EPISODE_TRAIN}_{BATCH_SIZE}_{LR}_{GAMMA}_{TARGET_UPDATE_FREQ}_{EPSILON_DECAY}\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': q_network.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    \n",
    "    'learning_rate': LR,\n",
    "    'buffer_size': BUFFER_SIZE,\n",
    "    'min_replay_size': MIN_REPLAY_SIZE,\n",
    "    'target_update_freq': TARGET_UPDATE_FREQ,\n",
    "\n",
    "    'gamma': GAMMA,\n",
    "    'epsilon_start': EPSILON_START,\n",
    "    'epsilon_end': EPSILON_END,\n",
    "    'epsilon_decay': EPSILON_DECAY,\n",
    "\n",
    "    'episode_train': EPISODE_TRAIN,\n",
    "    'batch_size': BATCH_SIZE\n",
    "}, os.path.join(OUTPUT_DIR,'q_network_checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cf419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"C:/Minh Nguyen/experiment/DRL/gym_exercise/inv_pend_results/250520_165708_5000_32_0.001_0.95_1000_5000/q_network_checkpoint.pth\")\n",
    "q_network_loaded = QNetwork(state_space,action_space)\n",
    "q_network_loaded.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b034cbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(checkpoint.keys())\n",
    "print(checkpoint['optimizer_state_dict'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42defbc",
   "metadata": {},
   "source": [
    "## Performance Visualization\n",
    "\n",
    "Visualize the performance of the trained Q network in multiple simulations and observer the reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a3a23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "\n",
    "# print(type(obs))\n",
    "for episode in range(5):\n",
    "    obs, _ = env_test.reset()\n",
    "    done = False\n",
    "    eps_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        with torch.no_grad():       # Doing inference so no need to track operations\n",
    "            state_tensor = torch.FloatTensor(obs).unsqueeze(0)  # unsqueeze to fake the sample (batch) dimension\n",
    "            q_values = q_network(state_tensor)\n",
    "            action = q_values.argmax().item()                   # index of the maximum output (action)\n",
    "        \n",
    "        next_obs, reward, term, trunc, _ = env_test.step(action)\n",
    "\n",
    "        done = term or trunc\n",
    "        eps_reward += reward\n",
    "        obs = next_obs\n",
    "    \n",
    "    print(f\"Reward from episode {episode:3d} is {eps_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6460e208",
   "metadata": {},
   "source": [
    "Visualize the state vs time plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229eb899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "obs, _ = env_test.reset()\n",
    "done = False\n",
    "obs_history = []; obs_history.append(obs)\n",
    "action_history = []\n",
    "reward_history = []\n",
    "\n",
    "while not done:\n",
    "    with torch.no_grad():       # Doing inference so no need to track operations\n",
    "        state_tensor = torch.FloatTensor(obs).unsqueeze(0)  # unsqueeze to fake the sample (batch) dimension\n",
    "        q_values = q_network(state_tensor)\n",
    "        action = q_values.argmax().item()                   # index of the maximum output (action)\n",
    "    \n",
    "    next_obs, reward, term, trunc, _ = env_test.step(action)\n",
    "\n",
    "    action_history.append(action)\n",
    "    obs_history.append(next_obs)\n",
    "    \n",
    "    if not reward_history: reward_history.append(reward)\n",
    "    else: reward_history.append(reward_history[-1] + reward)\n",
    "\n",
    "    done = term or trunc\n",
    "    obs = next_obs\n",
    "\n",
    "# Convert obs_history to a NumPy array for easier slicing\n",
    "obs_array = np.array(obs_history)\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(6, 1, figsize=(12, 24), sharex=True)\n",
    "\n",
    "# 1. Plot cumulative reward history\n",
    "axs[0].plot(reward_history, label='Cumulative Reward', color='green')\n",
    "axs[0].set_ylabel('Cumulative Reward')\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "# 2. Plot action history\n",
    "axs[1].plot(action_history, label='Actions Taken', color='blue')\n",
    "axs[1].set_ylabel('Action')\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "# 3. Plot each CartPole state variable over time\n",
    "state_labels = ['Cart Position', 'Cart Velocity', 'Pole Angle', 'Pole Angular Velocity']\n",
    "for i in range(4):\n",
    "    axs[i+2].plot(obs_array[:, i], label=state_labels[i])\n",
    "    axs[i+2].set_ylabel('State Values')\n",
    "    axs[i+2].set_xlabel('Timestep')\n",
    "    axs[i+2].legend()\n",
    "    axs[i+2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9491c631",
   "metadata": {},
   "source": [
    "# Inverted Pendulum using double DQN (DDQN)\n",
    "\n",
    "This experiment is an extension of the previous experiment with DQN. Firstly, this experiment investigates the double DQN algorithm in an inverted pendulum experiment. Secondly, this experiment compares the computation time of various training setups\n",
    "1. **OOP implementation** - one set of experiments using OOP with the DDQN_experiment class containing all training methods, and one set of experiment using explicit function calls (non OOP)\n",
    "2. **CUDA computation** - one set of experiments with CUDA enabled, while CUDA is disabled in the other\n",
    "\n",
    "<br>\n",
    "With this, there are four experiments, namely\n",
    "1. No OOP and no CUDA (nOOP_nCUDA) - simplest and most straightforward implementation of DDQN\n",
    "2. No OOP and CUDA (nOOP_CUDA)\n",
    "3. OOP and no CUDA (OOP_nCUDA)\n",
    "4. OOP and CUDA (OOP_CUDA)\n",
    "\n",
    "<br> <br>\n",
    "**Experiment Metrics**\n",
    "\n",
    "Each experiment set will be run for n times (n=100) to measure the amount of time to run (train + validate) each Q_net experiment. The average amount of time taken is used for deciding the optimal training setup for future research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b674c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random                       # To draw a random batch of samples form the replay buffer\n",
    "import gymnasium as gym             # To create the inverted pendulum environment\n",
    "import torch                        \n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import deque       # replay buffer is a double ended queue that allows elements to be added either to the end or the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a9ea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # device = torch.cuda.device()\n",
    "    device_idx = torch.cuda.current_device()\n",
    "    print(f\"Number of GPU: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {device_idx}|{torch.cuda.get_device_name(device_idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0adada0",
   "metadata": {},
   "source": [
    "## Parameter definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb40d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta parameters of the experiment\n",
    "RESULTS_DIR = \"comp_time_results\"       # which experiment folder to save the training - \"inv_pend_results\" (DQN), \"inv_pend_DDQN_result\", or \"comp_time_results\"\n",
    "VERBOSE = False                         # set to false to skip information prints and save computation time\n",
    "NUM_RUN = 100                           # number of experiment to run collect training time and runtime\n",
    "CUDA_ENABLED = False                    # whether to use CUDA computation acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9cc9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter definitions\n",
    "MODEL_NAME = 'DQN_MLP_v0'\n",
    "LR = 5e-4\n",
    "BUFFER_SIZE = 5000              # size of the experience replay buffer\n",
    "MIN_REPLAY_SIZE = 1000          # the number of experience to sample before starting training\n",
    "TARGET_UPDATE_FREQ = 1000       # the number of steps before updating the target network\n",
    "\n",
    "GAMMA = 0.95                    # discount factor for return calculation\n",
    "EPS_START = 1.0                 # exploration rate in stochastic policy\n",
    "EPS_END = 0.1                   # minimum exploration rate at the end of decay\n",
    "EPS_DECAY = 5000                # epsilon decay rate\n",
    "\n",
    "EPISODE_TRAIN = 5000            # number of episodes to train the Q-network\n",
    "BATCH_SIZE = 32                 # how many samples drawn from the experience replay buffer to train behaviour Q-net \n",
    "\n",
    "SUCCESS_CRITERIA = 450\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e8b2a9",
   "metadata": {},
   "source": [
    "## Setup - Model and DDQN Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44334f75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41de9054",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet_MLP(nn.Module):\n",
    "    ''' A QNetwork class that dynamically initialize an MLP Q Net from a list specifying the number of nodes in each hidden layer (e.g. [64,32])'''\n",
    "    def __init__(self,input_dim,output_dim,hidden_layer = [64,32]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        for size in hidden_layer:\n",
    "            self.layers.append(nn.Linear(self.input_dim, size))\n",
    "            self.input_dim = size\n",
    "        \n",
    "        self.layers.append(nn.Linear(self.input_dim,self.output_dim))\n",
    "        if CUDA_ENABLED: self.cuda()\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        for layer in self.layers[:-1]:\n",
    "            input_data = torch.relu(layer(input_data))\n",
    "        return self.layers[-1](input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0d2559",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_registry = {\n",
    "    'DQN_MLP_v0': {\n",
    "        'class': QNet_MLP,\n",
    "        'config': [64,32]\n",
    "    },\n",
    "    'DQN_MLP_v1': {\n",
    "        'class': QNet_MLP,\n",
    "        'config': [32,32]\n",
    "    },\n",
    "    'DQN_MLP_v2': {\n",
    "        \n",
    "        'class': QNet_MLP,\n",
    "        'config': [32,16]\n",
    "    },\n",
    "    'DQN_MLP_v3': {\n",
    "        'class': QNet_MLP,\n",
    "        'config': [16,16]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0afaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN_experiment():\n",
    "    def __init__(self, model_name: str,      # \"DQN_MLP_v0\" or \"DQN_MLP_v1\"\n",
    "                 model_registry, \n",
    "                 lr: float, \n",
    "                 buffer_size: int, \n",
    "                 target_update_freq: int, \n",
    "                 gamma: float, \n",
    "                 eps_start: float, \n",
    "                 eps_decay: int,\n",
    "                 eps_end: float, \n",
    "                 batch_size: int,\n",
    "                 result_folder,\n",
    "                 seed = 42,\n",
    "                 cuda_enabled = False,\n",
    "                 verbose = True):\n",
    "        \n",
    "        self.seed = seed                    # TODO - incorporate seeds to control the repeatability of the experiment \n",
    "        self.cuda_enabled = cuda_enabled    # if set to True, models and computations are done on GPU\n",
    "        self.verbose = verbose              # if set to False, skip prining the information throughout training to save time\n",
    "        self.result_folder = result_folder\n",
    "        \n",
    "        ''' Defining hyperparameters in the experiment '''\n",
    "        self.model_name = model_name                                        # Full name of the model\n",
    "        self.model_class = model_registry[self.model_name]['class']             # The model class \"QNet_MLP\" or \"QNet_test\"\n",
    "        self.model_config = model_registry[self.model_name]['config']       # List of nodes in each hidden layer\n",
    "        match = re.search(r'v\\d+',self.model_name)\n",
    "        self.model_id = match.group(0) if match else 404                    # Extract the \"v0\" or \"v1\" out of model name for abbreviation\n",
    "\n",
    "        # Hyperparameters of the experiment\n",
    "        self.lr = lr\n",
    "        self.buffer_size = buffer_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.gamma = gamma\n",
    "        self.eps_start, self.eps, self.eps_decay, self.eps_end = eps_start, eps_start, eps_decay, eps_end\n",
    "        self.batch_size = batch_size\n",
    "        self.episode_train = 5000\n",
    "        self.min_replay_size = 1000\n",
    "\n",
    "        # Initialize the train and validation environments\n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "        self.env_val = gym.make(\"CartPole-v1\")\n",
    "        self.obs_space = self.env.observation_space.shape[0]\n",
    "        self.act_space = self.env.action_space.n\n",
    "\n",
    "        # Initialize the 2 Q networks and the optimizer for the behavior Q_net\n",
    "        self.Q_net = self.model_class(self.obs_space, self.act_space, self.model_config)\n",
    "        self.target_net = self.model_class(self.obs_space, self.act_space, self.model_config)\n",
    "        self.target_net.load_state_dict(self.Q_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.SGD(self.Q_net.parameters(), lr = self.lr)\n",
    "\n",
    "        self.save_path = \"\"                                             # Directory of the current run\n",
    "        self.model_path = \"\"                                            # Path to a model \n",
    "        self.hyperparam_config = \"\"                                     # Shortened list of importatnt hyperparameters\n",
    "        self.reward_history = np.zeros(self.episode_train)\n",
    "        self.val_history = {}                                           # Monitor which episode had a validation run, the train reward, and the validation (test) reward \n",
    "\n",
    "    def create_directory(self):\n",
    "        ''' Function that creates directory to save model state_dict, architecture, training configuration, and history\n",
    "\n",
    "        Parameters: \n",
    "        ------------\n",
    "        (hyperparameters for differentiating between different directory)\n",
    "        \n",
    "        lr : float\n",
    "            the learning rate to optimize the Q network\n",
    "        gamma : float \n",
    "            the discount rate in Q learning\n",
    "        epsilon_decay : integer\n",
    "            the amount of episode over which the exploratory rate (epsilon) decays\n",
    "        batch_size : integer\n",
    "            number of experience drawn from replay buffer to train the behaviour network\n",
    "        buffer_size : integer\n",
    "            the number of samples in the replay buffer at a time\n",
    "        target_udpate_freq : integer\n",
    "            the amount of step count during the training process before updating the target Q net (loading the parameters of the behaviour net onto the target Q Net)\n",
    "        \n",
    "\n",
    "        Returns\n",
    "        ------------\n",
    "        name_codified : str\n",
    "            the shortened name for the current experiment \n",
    "        hyperparameters_codified : str\n",
    "            the shortened string of hyperparameter configuration\n",
    "        OUTPUT_DIR : path\n",
    "            the directory to which the training results and model (state_dict and architecture) will be saved\n",
    "        '''\n",
    "        timestamp = datetime.now().strftime(\"%y%m%d_%H%M\")\n",
    "\n",
    "        RESULT_FOLDER = self.result_folder\n",
    "        BASE_DIR = os.getcwd()\n",
    "        RESULT_DIR = os.path.join(BASE_DIR, RESULT_FOLDER)\n",
    "        os.makedirs(RESULT_DIR, exist_ok=True)      # Create the directory if one does not already exist\n",
    "\n",
    "        # Find the trial # of the latest run\n",
    "        existing_runs = [d for d in os.listdir(RESULT_DIR) if os.path.isdir(os.path.join(RESULT_DIR,d))]\n",
    "        run_numbers = [int(re.search(r'run_(\\d{5})',d).group(1)) for d in existing_runs if re.match(r'run_\\d{5}',d)]\n",
    "        trial_number = max(run_numbers,default=-1)+1\n",
    "\n",
    "        # Create a folder for the run\n",
    "        name_codified = f\"run_{trial_number:05d}\"\n",
    "        OUTPUT_DIR = os.path.join(RESULT_DIR,name_codified)\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)      # Create the directory\n",
    "\n",
    "        # Append the mapping from run # to hyperparameter configuration in a JSON file inside RESULT_DIR\n",
    "        trial_to_param_path = os.path.join(RESULT_DIR,'trial_to_param.json')\n",
    "        if os.path.exists(trial_to_param_path):\n",
    "            with open(trial_to_param_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "        else:\n",
    "            data = {name_codified: []}\n",
    "\n",
    "        if self.cuda_enabled:\n",
    "            hyperparam_codified = f\"DDQN_OOP_CUDA_{self.model_id}_{self.lr}_{self.buffer_size}_{self.target_update_freq}_{self.gamma}_{self.eps_decay}_{self.batch_size}\"\n",
    "            hyperparam_codified_time = f\"{timestamp}_DDQN_OOP_CUDA_{self.model_id}_{self.lr}_{self.buffer_size}_{self.target_update_freq}_{self.gamma}_{self.eps_decay}_{self.batch_size}\"\n",
    "        else:   \n",
    "            hyperparam_codified = f\"DDQN_OOP_nCUDA_{self.model_id}_{self.lr}_{self.buffer_size}_{self.target_update_freq}_{self.gamma}_{self.eps_decay}_{self.batch_size}\"\n",
    "            hyperparam_codified_time = f\"{timestamp}_DDQN_OOP_nCUDA_{self.model_id}_{self.lr}_{self.buffer_size}_{self.target_update_freq}_{self.gamma}_{self.eps_decay}_{self.batch_size}\"\n",
    "        data[name_codified] = hyperparam_codified_time\n",
    "\n",
    "        with open(trial_to_param_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "        # Store the training configs in JSON file\n",
    "        training_params = {\n",
    "            'OOP':                  True,\n",
    "            'CUDA':                 self.cuda_enabled,\n",
    "            'device':               torch.cuda.get_device_name(torch.cuda.current_device()),\n",
    "            'model_id':             self.model_id,\n",
    "            'lr':                   self.lr,\n",
    "            'gamma':                self.gamma,\n",
    "            'epsilon_decay':        self.eps_decay,\n",
    "            'batch_size':           self.batch_size,\n",
    "            'buffer_size':          self.buffer_size,\n",
    "            'target_update_freq':   self.target_update_freq\n",
    "        }\n",
    "\n",
    "        # Store training parameters in each run \n",
    "        param_path = os.path.join(OUTPUT_DIR, \"param_config.json\")\n",
    "        with open(param_path, \"w\") as f:\n",
    "            json.dump({\"parameters\": training_params}, f, indent=2)\n",
    "\n",
    "        return name_codified, hyperparam_codified, OUTPUT_DIR\n",
    "    \n",
    "    def eps_greedy_policy(self, env, obs, epsilon):      \n",
    "        ''' Function to take an action according to an epsilon-greedy policy and a Q-network'''\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                if self.cuda_enabled:\n",
    "                    state_tensor = torch.tensor(obs, dtype=torch.float32, device='cuda').unsqueeze(0)\n",
    "                else:\n",
    "                    state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
    "                q_values = self.Q_net(state_tensor)\n",
    "                action = q_values.argmax().item()\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def prefill_replay(self):\n",
    "        obs,_ = self.env.reset()\n",
    "        for _ in range(self.min_replay_size):\n",
    "            action = self.env.action_space.sample()\n",
    "            next_obs, reward, term, trunc, _ = self.env.step(action)\n",
    "            done = term or trunc\n",
    "\n",
    "            self.replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "            obs = next_obs if not done else self.env.reset()[0]\n",
    "\n",
    "    def policy_eval(self, env: gym.Env, n_episode_test = 500, verbose = 0):\n",
    "        ''' Assess the average reward when following a q_network in a test environment with random state initialization\n",
    "        \n",
    "        Parameters:\n",
    "        env : gymnasium environment\n",
    "            - Can be either the self.env_test or self.env_val environment\n",
    "\n",
    "        '''\n",
    "\n",
    "        total_reward = 0\n",
    "        for i in range(n_episode_test):\n",
    "            obs,_ = env.reset()\n",
    "            done = False\n",
    "            eps_reward = 0\n",
    "\n",
    "            while not done:                 # Step thorugh the episode deterministically (no exploration)\n",
    "                action = self.eps_greedy_policy(env, obs, epsilon=0)\n",
    "                next_obs, reward, term, trunc, _ = env.step(action)\n",
    "\n",
    "                eps_reward += reward\n",
    "\n",
    "                obs = next_obs\n",
    "                done = term or trunc\n",
    "        \n",
    "            total_reward += eps_reward\n",
    "            if verbose:\n",
    "                print(f\"Validation episode {i+1:3d}/{n_episode_test}  |   Reward = {eps_reward:4.0f}\",end=\"\\r\")\n",
    "        average_reward = total_reward / n_episode_test\n",
    "        return average_reward\n",
    "    \n",
    "    # Helper function for EMA filters and plotting data\n",
    "    def EMA_filter(self, reward: list, alpha):\n",
    "        ''' Function that runs an exponential moving average filter along a datastream '''\n",
    "        output = np.zeros(len(reward)+1)\n",
    "        output[0] = reward[0]\n",
    "        for idx, item in enumerate(reward):\n",
    "            output[idx+1] = (1 - alpha) * output[idx] + alpha * item\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def plot_reward_hist(self, alpha = 0.1):\n",
    "        ''' Function that plots the reward and filtered reward per episode, then saves the plot in a specified save directory'''\n",
    "        n_episodes= len(self.reward_history)\n",
    "        episodes = range(n_episodes)\n",
    "        filtered_reward_hist = self.EMA_filter(self.reward_history, alpha)\n",
    "\n",
    "        plt.figure(figsize=(20,6))\n",
    "        plt.plot(episodes, self.reward_history[:n_episodes], color = \"blue\")\n",
    "        plt.plot(episodes, filtered_reward_hist[:n_episodes], color = \"red\")\n",
    "        plt.title(f'Total reward per episode - {self.hyperparam_config}')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.legend(['Total reward','Filtered reward'])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if self.save_path:\n",
    "            plt.savefig(os.path.join(self.save_path,'reward_history.png'))\n",
    "        plt.show()\n",
    "    \n",
    "    def save_model(self):\n",
    "        ''' Function to save the model and optimizer state_dict for inference or continued training '''\n",
    "        self.model_path = os.path.join(self.save_path, 'q_network_checkpoint.pth')\n",
    "        torch.save({\n",
    "            'model_state_dict': self.Q_net.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, self.model_path)\n",
    "\n",
    "    def load_model(self):\n",
    "        ''' This code overwrite the Q_net with the parameters store in the instance's save_path '''\n",
    "        checkpoint = torch.load(self.model_path)\n",
    "        self.Q_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    def DDQN_train(self):\n",
    "        ''' Function to train a policy for a set of hyperparameters '''\n",
    "\n",
    "        msg = \"Training ended, no good model found!\"        # Default msg, changes during training as models are saved\n",
    "        \n",
    "        self.replay_buffer = deque(maxlen = self.buffer_size)\n",
    "        self.prefill_replay()\n",
    "\n",
    "        # Create the directory to store results\n",
    "        self.run_number, self.hyperparam_config, self.save_path = self.create_directory()\n",
    "\n",
    "        # Training information\n",
    "        if self.verbose: print(f'model {self.model_name}, lr={self.lr}, buffer={self.buffer_size}, target_freq={self.target_update_freq}, gamma={self.gamma}, eps_decay={self.eps_decay}, batch_size={self.batch_size}')\n",
    "\n",
    "        self.reward_history = np.zeros(self.episode_train)\n",
    "        self.eps = self.eps_start\n",
    "        step_count = 0\n",
    "        episode = 0\n",
    "        target_network_update_count = 0\n",
    "\n",
    "        # Control of early stopping\n",
    "        consecutive_pass_count = 0           # Number of consecutive episodes where performance exceeds a threshold\n",
    "        CONSECUTIVE_PASS_LIMIT = 3          # No. of consecutive episodes with higher performance than reward limit\n",
    "        EPISODE_REWARD_LIMIT = SUCCESS_CRITERIA\n",
    "        best_reward = 0\n",
    "        self.best_model_episode = None\n",
    "        performance_crit = False\n",
    "        train_terminated = False\n",
    "        self.val_history = {}               # Clear the validationi history\n",
    "        self.val_time = 0                   # Reset the total validation time\n",
    "\n",
    "        \n",
    "        self.train_time_start = time.time()\n",
    "        while not train_terminated:     # Experiment level - loop through episodes\n",
    "            obs, _ = self.env.reset(seed=self.seed)\n",
    "            eps_rewards = 0\n",
    "        \n",
    "            while True:                 # Episode level - loop through steps\n",
    "                action = self.eps_greedy_policy(self.env, obs, epsilon = self.eps)\n",
    "\n",
    "                # Interact with the environment\n",
    "                next_obs, reward, term, trunc, _ = self.env.step(action)\n",
    "                done = term or trunc\n",
    "                self.replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "                obs = next_obs\n",
    "                eps_rewards += reward\n",
    "                step_count += 1\n",
    "\n",
    "                # Train the Q-net using a batch of samples from the experience replay\n",
    "                if len(self.replay_buffer) >= self.batch_size:\n",
    "                    batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "                    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "                    if self.cuda_enabled:\n",
    "                        states =        torch.tensor(np.array(states),      dtype = torch.float32,  device='cuda')           # Shape of (BATCH_SIZE, 4). Data structure of [[p1 v1 theta1 dtheta1], [p2 v2 theta2 dtheta2], ...]\n",
    "                        actions =       torch.tensor(actions,               dtype = torch.long,     device='cuda').unsqueeze(1)        # Must unsqueeze to have shape (BATCH_SIZE, 1). From [a1, a2, ...] to [[a1], [a2], ... [a_BATCH_SIZE]]\n",
    "                        rewards =       torch.tensor(rewards,               dtype = torch.float32,  device='cuda').unsqueeze(1)\n",
    "                        next_states =   torch.tensor(np.array(next_states), dtype = torch.float32,  device = 'cuda')\n",
    "                        dones =         torch.tensor(dones,                 dtype = torch.float32,  device='cuda').unsqueeze(1)\n",
    "                    else:\n",
    "                        states = torch.FloatTensor(np.array(states))                                          # Shape of (BATCH_SIZE, 4). Data structure of [[p1 v1 theta1 dtheta1], [p2 v2 theta2 dtheta2], ...]\n",
    "                        actions = torch.LongTensor(actions).unsqueeze(1)                            # Must unsqueeze to have shape (BATCH_SIZE, 1). From [a1, a2, ...] to [[a1], [a2], ... [a_BATCH_SIZE]]\n",
    "                        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "                        next_states = torch.FloatTensor(np.array(next_states))\n",
    "                        dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "\n",
    "                    # Compute targets using target network Q(s',a',w_i^-)\n",
    "                    # TODO - Change this from DQN to DDQN code\n",
    "                    with torch.no_grad():\n",
    "                        # Select the maximizing action according to the online behaviour net\n",
    "                        optimal_next_actions_online = self.Q_net(next_states).argmax(dim=1,keepdim=True)\n",
    "                        \n",
    "                        # Find the target Q value of the maximizing action according to the target net\n",
    "                        target_q_values = self.target_net(next_states).gather(1,optimal_next_actions_online)\n",
    "                        \n",
    "                        targets = rewards + self.gamma * target_q_values * (1 - dones)       # (1-dones) to avoid over estimating the value of terminal states. The target should only be the reward in the terminal states   # (1-dones) to avoid over estimating the value of terminal states. The target should only be the reward in the terminal states\n",
    "                \n",
    "                    # Compute the current Q values for the actions taken Q(s,a,w_i)\n",
    "                    q_values = self.Q_net(states).gather(1, actions)         # obtain the q_values associated to the actual action taken in each sample\n",
    "\n",
    "                    # Update the parameters of the behaviour q_network\n",
    "                    loss = nn.MSELoss()(q_values, targets)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                # Periodically update the target network by loading the weights from the behavior network\n",
    "                if step_count % self.target_update_freq == 0:\n",
    "                    target_network_update_count += 1\n",
    "                    self.target_net.load_state_dict(self.Q_net.state_dict())\n",
    "\n",
    "                if done:        # End of a training episode\n",
    "                    break\n",
    "\n",
    "            # Decay epsilon after an episode\n",
    "            self.eps = max(self.eps_end, self.eps - (self.eps_start - self.eps_end)/self.eps_decay)\n",
    "\n",
    "            self.reward_history[episode] = eps_rewards\n",
    "            # print(f\"Episode {episode:5d}: Total reward = {eps_rewards:5.1f}, Epsilon = {epsilon:.3f}, Step count = {step_count:5d}, Target update count = {target_network_update_count:3d}\")\n",
    "            # print(f\"Average reward {sum(reward_history[:episode])/(episode+1):.2f}\")\n",
    "\n",
    "            \n",
    "            if episode % 10 == 0 and self.verbose:                   # print progress periodically\n",
    "                print(f\"Episode {episode:5d}/{self.episode_train}: Total reward = {eps_rewards:5.1f}, Epsilon = {self.eps:.3f}\", end = \"\\r\")\n",
    "\n",
    "            # Early stopping condition\n",
    "            if eps_rewards >= EPISODE_REWARD_LIMIT:\n",
    "                # Evaluate the current good policy and time the validation time\n",
    "                self.val_time_start = time.time()\n",
    "                test_reward = self.policy_eval(self.env_val, 100,verbose=self.verbose)\n",
    "                self.val_time += time.time() - self.val_time_start\n",
    "\n",
    "                self.val_history[episode] = [eps_rewards, test_reward]\n",
    "\n",
    "                if test_reward >= best_reward:           # Set the new best reward\n",
    "                    best_reward = test_reward\n",
    "                    self.save_model()\n",
    "                    self.best_model_episode = episode\n",
    "                    msg = f\"Training terminated due to episode limit, best model saved at episode {self.best_model_episode:5d}\"\n",
    "                if test_reward > EPISODE_REWARD_LIMIT: \n",
    "                    consecutive_pass_count += 1\n",
    "                else: consecutive_pass_count = 0\n",
    "            else:\n",
    "                consecutive_pass_count = 0\n",
    "                \n",
    "            # Performance criteria - if good results for several episodes => training performance satisfied and terminate early\n",
    "            if consecutive_pass_count >= CONSECUTIVE_PASS_LIMIT:\n",
    "                self.save_model()\n",
    "                self.best_model_episode = episode\n",
    "                performance_crit = True \n",
    "                msg = f\"Early termination at episode {self.best_model_episode:5d}, desired performance reached\"\n",
    "\n",
    "\n",
    "            episode += 1\n",
    "\n",
    "            # Checking for early training termination or truncation\n",
    "            train_terminated = (episode >= self.episode_train) or (performance_crit)\n",
    "        self.train_time = time.time() - self.train_time_start\n",
    "\n",
    "        print(f\"\\nRuntime - {self.train_time:5.2f}\")\n",
    "        print(msg)\n",
    "        return\n",
    "\n",
    "    def DDQN_record(self):\n",
    "        ''' Method to plot the reward history and store the data in the current run folder '''\n",
    "\n",
    "        # Load the best Q net parameter from the experiment (if a model is saved)\n",
    "        if self.model_path:\n",
    "            self.load_model()\n",
    "\n",
    "        # Average test reward of the resulting policy - if best policy does not exists test the final policy\n",
    "        self.env_test = gym.make('CartPole-v1')\n",
    "        print('Testing the Q_net')\n",
    "        average_reward = self.policy_eval(self.env_test, n_episode_test=500, verbose=1)\n",
    "        print(f\"\\nValidation average reward {average_reward:4.2f}\")\n",
    "\n",
    "        # Store the validation hisotry and average test reward in the param_config JSON file\n",
    "        param_path = os.path.join(self.save_path,'param_config.json')\n",
    "        if os.path.exists(param_path):\n",
    "            with open(param_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "        else:\n",
    "            data = {}\n",
    "        data['runtime'] = self.train_time\n",
    "        data['valtime'] = self.val_time\n",
    "        data['best_model_at'] = self.best_model_episode\n",
    "        data['val_history'] = self.val_history\n",
    "        data['test_result'] = average_reward\n",
    "\n",
    "        with open(param_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "        # Plot\n",
    "        self.plot_reward_hist()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8263cb1c",
   "metadata": {},
   "source": [
    "## DDQN Training (OOP, non CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694b238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "# DQN Training\n",
    "\n",
    "# # Check whether the model is using CUDA\n",
    "# next(experiment.Q_net.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7233089",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NUM_RUN):\n",
    "    \n",
    "    experiment = DDQN_experiment(MODEL_NAME, model_registry, \n",
    "                             LR, \n",
    "                             BUFFER_SIZE, \n",
    "                             TARGET_UPDATE_FREQ, \n",
    "                             GAMMA, \n",
    "                             EPS_START, EPS_DECAY, EPS_END, \n",
    "                             BATCH_SIZE, 42, cuda_enabled=CUDA_ENABLED, verbose = VERBOSE)\n",
    "    print(f\"Run {i+1:4d}/{NUM_RUN} | {experiment.run_number}------------------\")\n",
    "    experiment.DDQN_train()\n",
    "    experiment.DDQN_record()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8c1f67",
   "metadata": {},
   "source": [
    "## DDQN Training (non-OOP, non cuda)\n",
    "\n",
    "This portion of DDQN training does not use the DDQN_experiment class but simply explicit code. The model and computation all runs on cpu and not cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051928dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(model_id: int,\n",
    "                     lr: float, \n",
    "                     gamma: float,\n",
    "                     epsilon_decay: int,\n",
    "                     batch_size: int, \n",
    "                     buffer_size: int,\n",
    "                     target_update_freq: int):\n",
    "    ''' Function that creates directory to save model state_dict, architecture, training configuration, and history\n",
    "\n",
    "    Parameters: \n",
    "    ------------\n",
    "    (hyperparameters for differentiating between different directory)\n",
    "    \n",
    "    lr : float\n",
    "        the learning rate to optimize the Q network\n",
    "    gamma : float \n",
    "        the discount rate in Q learning\n",
    "    epsilon_decay : integer\n",
    "        the amount of episode over which the exploratory rate (epsilon) decays\n",
    "    batch_size : integer\n",
    "        number of experience drawn from replay buffer to train the behaviour network\n",
    "    buffer_size : integer\n",
    "        the number of samples in the replay buffer at a time\n",
    "    target_udpate_freq : integer\n",
    "        the amount of step count during the training process before updating the target Q net (loading the parameters of the behaviour net onto the target Q Net)\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    ------------\n",
    "    name_codified : str\n",
    "        the shortened name for the current experiment \n",
    "    hyperparameters_codified : str\n",
    "        the shortened string of hyperparameter configuration\n",
    "    OUTPUT_DIR : path\n",
    "        the directory to which the training results and model (state_dict and architecture) will be saved\n",
    "    '''\n",
    "    timestamp = datetime.now().strftime(\"%y%m%d_%H%M\")\n",
    "\n",
    "    BASE_DIR = os.getcwd()\n",
    "    RESULT_DIR = os.path.join(BASE_DIR, \"comp_time_results\")\n",
    "    os.makedirs(RESULT_DIR, exist_ok=True)      # Create the directory if one does not already exist\n",
    "\n",
    "    # Find the trial # of the latest run\n",
    "    existing_runs = [d for d in os.listdir(RESULT_DIR) if os.path.isdir(os.path.join(RESULT_DIR,d))]\n",
    "    run_numbers = [int(re.search(r'run_(\\d{5})',d).group(1)) for d in existing_runs if re.match(r'run_\\d{5}',d)]\n",
    "    trial_number = max(run_numbers,default=-1)+1\n",
    "\n",
    "    # Create a folder for the run\n",
    "    name_codified = f\"run_{trial_number:05d}\"\n",
    "    OUTPUT_DIR = os.path.join(RESULT_DIR,name_codified)\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)      # Create the directory\n",
    "\n",
    "    # Append the mapping from run # to hyperparameter configuration in a JSON file inside RESULT_DIR\n",
    "    trial_to_param_path = os.path.join(RESULT_DIR,'trial_to_param.json')\n",
    "    if os.path.exists(trial_to_param_path):\n",
    "        with open(trial_to_param_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        data = {name_codified: []}\n",
    "    if CUDA_ENABLED:\n",
    "        hyperparam_codified = f\"DDQN_nOOP_CUDA_{model_id}_{lr}_{buffer_size}_{target_update_freq}_{gamma}_{epsilon_decay}_{batch_size}\"\n",
    "        hyperparam_codified_time = f\"{timestamp}_DDQN_nOOP_CUDA_{model_id}_{lr}_{buffer_size}_{target_update_freq}_{gamma}_{epsilon_decay}_{batch_size}\"\n",
    "    else:\n",
    "        hyperparam_codified = f\"DDQN_nOOP_nCUDA_{model_id}_{lr}_{buffer_size}_{target_update_freq}_{gamma}_{epsilon_decay}_{batch_size}\"\n",
    "        hyperparam_codified_time = f\"{timestamp}_DDQN_nOOP_nCUDA_{model_id}_{lr}_{buffer_size}_{target_update_freq}_{gamma}_{epsilon_decay}_{batch_size}\"\n",
    "    data[name_codified] = hyperparam_codified_time\n",
    "\n",
    "    with open(trial_to_param_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    \n",
    "    # Store the training configs in JSON file\n",
    "    training_params = {\n",
    "        'OOP':                  False,\n",
    "        'CUDA':                 CUDA_ENABLED,\n",
    "        'device':               torch.cuda.get_device_name(torch.cuda.current_device()),          \n",
    "        'model_id':             model_id,\n",
    "        'lr':                   lr,\n",
    "        'gamma':                gamma,\n",
    "        'epsilon_decay':        epsilon_decay,\n",
    "        'batch_size':           batch_size,\n",
    "        'buffer_size':          buffer_size,\n",
    "        'target_update_freq':   target_update_freq\n",
    "    }\n",
    "\n",
    "    # Store training parameters in each run \n",
    "    param_path = os.path.join(OUTPUT_DIR, \"param_config.json\")\n",
    "    with open(param_path, \"w\") as f:\n",
    "        json.dump({\"parameters\": training_params}, f, indent=2)\n",
    "\n",
    "    return name_codified, hyperparam_codified, OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b088db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy_policy(env, obs, epsilon: float, q_network: QNet_MLP):\n",
    "    ''' Function to take an action according to an epsilon-greedy policy and a Q-network'''\n",
    "    if np.random.random() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            if CUDA_ENABLED:\n",
    "                state_tensor = torch.tensor(obs,dtype=torch.float32,device='cuda').unsqueeze(0)\n",
    "            else:\n",
    "                state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
    "            q_values = q_network(state_tensor)\n",
    "            action = q_values.argmax().item()\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482b9206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(env_test, q_network, n_episode_test = 500, verbose = False):\n",
    "    ''' Assess the average reward when following a q_network in a test environment with random state initialization '''\n",
    "    \n",
    "    total_reward = 0\n",
    "    for i in range(n_episode_test):\n",
    "        obs,_ = env_test.reset()\n",
    "        done = False\n",
    "        eps_reward = 0\n",
    "\n",
    "        while not done:                 # Step thorugh the episode\n",
    "            action = eps_greedy_policy(env_test, obs, epsilon=0, q_network=q_network)\n",
    "            next_obs, reward, term, trunc, _ = env_test.step(action)\n",
    "\n",
    "            eps_reward += reward\n",
    "\n",
    "            obs = next_obs\n",
    "            done = term or trunc\n",
    "    \n",
    "        total_reward += eps_reward\n",
    "        if verbose:\n",
    "                    print(f\"Validation episode {i+1:3d}/{n_episode_test}  |   Reward = {eps_reward:4.0f}\",end=\"\\r\")\n",
    "    average_reward = total_reward / n_episode_test\n",
    "\n",
    "    return average_reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1ad87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(q_network: nn.Module, optimizer: torch.optim.Optimizer, save_path):\n",
    "    ''' Function to save the model and optimizer state_dict for inference or continued training '''\n",
    "    torch.save({\n",
    "        'model_state_dict': q_network.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, os.path.join(save_path, 'q_network_checkpoint.pth'))\n",
    "\n",
    "def load_model(q_network: nn.Module, model_path):\n",
    "    checkpoint = torch.load(model_path)\n",
    "    q_network.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EMA_filter(reward: list, alpha):\n",
    "    ''' Function that runs an exponential moving average filter along a datastream '''\n",
    "    output = np.zeros(len(reward)+1)\n",
    "    output[0] = reward[0]\n",
    "    for idx, item in enumerate(reward):\n",
    "        output[idx+1] = (1 - alpha) * output[idx] + alpha * item\n",
    "    \n",
    "    return output\n",
    "\n",
    "def plot_reward_hist(reward_history: list, hyperparam_config: str, save_path = None, alpha = 0.1):\n",
    "    ''' Function that plots the reward and filtered reward per episode, then saves the plot in a specified save directory'''\n",
    "    n_episodes= len(reward_history)\n",
    "    episodes = range(n_episodes)\n",
    "    filtered_reward_hist = EMA_filter(reward_history, alpha)\n",
    "\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.plot(episodes, reward_history[:n_episodes], color = \"blue\")\n",
    "    plt.plot(episodes, filtered_reward_hist[:n_episodes], color = \"red\")\n",
    "    plt.title(f'Total reward per episode - {hyperparam_config}')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend(['Total reward','Filtered reward'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(os.path.join(save_path,'reward_history.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a467025",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NUM_RUN):\n",
    "    MODEL_CLASS = model_registry[MODEL_NAME]['class']\n",
    "    MODEL_CONFIG = model_registry[MODEL_NAME]['config']\n",
    "    match = re.search(r'v\\d+',MODEL_NAME)\n",
    "    MODEL_ID = match.group(0) if match else 404\n",
    "\n",
    "    # Initialize the environment and the Q network\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    env_val = gym.make(\"CartPole-v1\")\n",
    "    obs_space = env.observation_space.shape[0]        # State space of cart pos & vel, and pendulum angular pos and vel. Each component is a continuous value\n",
    "    action_space =  env.action_space.n                   # Action space of left or right\n",
    "\n",
    "    Q_net = MODEL_CLASS(obs_space,action_space, MODEL_CONFIG)\n",
    "    target_net = MODEL_CLASS(obs_space,action_space, MODEL_CONFIG)\n",
    "    target_net.load_state_dict(Q_net.state_dict())\n",
    "    target_net.eval()                               # target network is only used for evaluation/inference and not trained\n",
    "\n",
    "    optimizer = optim.SGD(Q_net.parameters(), lr = LR)\n",
    "\n",
    "    msg = \"Training ended, no good model found!\"        # Default msg, changes during training as models are saved\n",
    "\n",
    "    #Initialize and pre-fill the replay buffer\n",
    "    replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "    obs, _ = env.reset()\n",
    "    for _ in range(MIN_REPLAY_SIZE):\n",
    "        action = env.action_space.sample()\n",
    "        next_obs, reward, term, trunc, _ = env.step(action)\n",
    "        done = term or trunc\n",
    "\n",
    "        replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "        obs = next_obs if not done else env.reset()[0]\n",
    "\n",
    "    # Create the directory to store results\n",
    "    run_number, hyperparam_config, save_path = create_directory(MODEL_ID,LR,GAMMA,EPS_DECAY,BATCH_SIZE,BUFFER_SIZE,TARGET_UPDATE_FREQ)\n",
    "\n",
    "    # Training information\n",
    "    print(f\"Run {i+1:4d}/{NUM_RUN} | {run_number} ------------------\")\n",
    "    if VERBOSE: print(f'model {MODEL_NAME}, lr={LR}, buffer={BUFFER_SIZE}, target_freq={TARGET_UPDATE_FREQ}, gamma={GAMMA}, eps_decay={EPS_DECAY}, batch_size={BATCH_SIZE}')\n",
    "\n",
    "    # DDQN Model Training loop\n",
    "    reward_history = np.zeros(EPISODE_TRAIN)\n",
    "    epsilon = EPS_START\n",
    "    step_count = 0\n",
    "    episode = 0\n",
    "    target_network_update_count = 0\n",
    "\n",
    "    # Control of early stopping\n",
    "    consecutive_pass_count = 0           # Number of consecutive episodes where performance exceeds a threshold\n",
    "    CONSECUTIVE_PASS_LIMIT = 3          # No. of consecutive episodes with higher performance than reward limit\n",
    "    EPISODE_REWARD_LIMIT = SUCCESS_CRITERIA\n",
    "    best_reward = 0\n",
    "    best_model_episode = None           # Default of None, only changes when a good model is encountered\n",
    "    performance_crit = False\n",
    "    train_terminated = False\n",
    "    val_history = {}                    # Monitor which episode had a validation run, the train reward, and the validation (test) reward \n",
    "    val_time = 0\n",
    "\n",
    "    train_time_start = time.time()\n",
    "    while not train_terminated:     # Experiment level - loop through episodes\n",
    "        obs, _ = env.reset()\n",
    "        eps_rewards = 0\n",
    "        \n",
    "\n",
    "        while True:\n",
    "            # Epsilon-greedy policy to select action\n",
    "            if np.random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():       # Doing inference so no need to track operations\n",
    "                    if CUDA_ENABLED:\n",
    "                        state_tensor = torch.tensor(obs, dtype=torch.float32, device='cuda').unsqueeze(0)  # unsqueeze to fake the sample (batch) dimension\n",
    "                    else:\n",
    "                        state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
    "                    q_values = Q_net(state_tensor)\n",
    "                    action = q_values.argmax().item()                   # index of the maximum output (action)\n",
    "            \n",
    "            # Interact with the environment\n",
    "            next_obs, reward, term, trunc, _ = env.step(action)\n",
    "            done = term or trunc\n",
    "            replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "            obs = next_obs\n",
    "            eps_rewards += reward\n",
    "            step_count += 1\n",
    "\n",
    "            # Sample a batch and update the network\n",
    "            if len(replay_buffer) >= BATCH_SIZE:\n",
    "                batch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "                if CUDA_ENABLED:\n",
    "                    states =        torch.tensor(np.array(states),      dtype = torch.float32,  device='cuda')           # Shape of (BATCH_SIZE, 4). Data structure of [[p1 v1 theta1 dtheta1], [p2 v2 theta2 dtheta2], ...]\n",
    "                    actions =       torch.tensor(actions,               dtype = torch.long,     device='cuda').unsqueeze(1)        # Must unsqueeze to have shape (BATCH_SIZE, 1). From [a1, a2, ...] to [[a1], [a2], ... [a_BATCH_SIZE]]\n",
    "                    rewards =       torch.tensor(rewards,               dtype = torch.float32,  device='cuda').unsqueeze(1)\n",
    "                    next_states =   torch.tensor(np.array(next_states), dtype = torch.float32,  device = 'cuda')\n",
    "                    dones =         torch.tensor(dones,                 dtype = torch.float32,  device='cuda').unsqueeze(1)   \n",
    "                else:\n",
    "                    states =        torch.FloatTensor(np.array(states))           # Shape of (BATCH_SIZE, 4). Data structure of [[p1 v1 theta1 dtheta1], [p2 v2 theta2 dtheta2], ...]\n",
    "                    actions =       torch.LongTensor (actions).unsqueeze(1)        # Must unsqueeze to have shape (BATCH_SIZE, 1). From [a1, a2, ...] to [[a1], [a2], ... [a_BATCH_SIZE]]\n",
    "                    rewards =       torch.FloatTensor(rewards).unsqueeze(1)\n",
    "                    next_states =   torch.FloatTensor(np.array(next_states))\n",
    "                    dones =         torch.FloatTensor(dones).unsqueeze(1)\n",
    "\n",
    "                # Compute targets using target network Q(s',a',w_i^-)\n",
    "                with torch.no_grad():\n",
    "                    # Select the maximizing action according to the online behaviour net\n",
    "                            optimal_next_actions_online = Q_net(next_states).argmax(dim=1,keepdim=True)\n",
    "                            \n",
    "                            # Find the target Q value of the maximizing action according to the target net\n",
    "                            target_q_values = target_net(next_states).gather(1,optimal_next_actions_online)\n",
    "                            \n",
    "                            targets = rewards + GAMMA * target_q_values * (1 - dones)       # (1-dones) to avoid over estimating the value of terminal states. The target should only be the reward in the terminal states   # (1-dones) to avoid over estimating the value of terminal states. The target should only be the reward in the terminal states\n",
    "\n",
    "                # Compute the current Q values for the actions taken Q(s,a,w_i)\n",
    "                q_values = Q_net(states).gather(1, actions)                             # obtain the q_values associated to the actual action taken in each sample\n",
    "\n",
    "                # Update the parameters of the behaviour q_network\n",
    "                loss = nn.MSELoss()(q_values, targets)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Periodically update the target network\n",
    "                if step_count % TARGET_UPDATE_FREQ == 0:\n",
    "                    target_network_update_count += 1\n",
    "                    target_net.load_state_dict(Q_net.state_dict())\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "    \n",
    "        # Decay epsilon after an episode\n",
    "        epsilon = max(EPS_END, epsilon - (EPS_START - EPS_END)/EPS_DECAY)\n",
    "\n",
    "        reward_history[episode] = eps_rewards\n",
    "        # print(f\"Episode {episode:5d}: Total reward = {eps_rewards:5.1f}, Epsilon = {epsilon:.3f}, Step count = {step_count:5d}, Target update count = {target_network_update_count:3d}\")\n",
    "        # print(f\"Average reward {sum(reward_history[:episode])/(episode+1):.2f}\")\n",
    "\n",
    "        \n",
    "        if episode % 10 == 0 and VERBOSE:                   # print progress periodically\n",
    "            print(f\"Episode {episode:5d}: Total reward = {eps_rewards:5.1f}, Epsilon = {epsilon:.3f}\", end = \"\\r\")\n",
    "\n",
    "        # Early stopping condition\n",
    "        if eps_rewards >= EPISODE_REWARD_LIMIT:\n",
    "            val_time_start = time.time()\n",
    "            test_reward = policy_eval(env_val, Q_net, 100)\n",
    "            val_time += time.time() - val_time_start\n",
    "\n",
    "            val_history[episode] = [eps_rewards, test_reward]\n",
    "\n",
    "            if test_reward >= best_reward:           # Set the new best reward\n",
    "                best_reward = test_reward\n",
    "                save_model(Q_net, optimizer, save_path)\n",
    "                best_model_episode = episode\n",
    "                msg = f\"Training terminated due to episode limit, best model saved at episode {best_model_episode:5d}\"\n",
    "            if test_reward > EPISODE_REWARD_LIMIT: \n",
    "                consecutive_pass_count += 1\n",
    "            else: consecutive_pass_count = 0\n",
    "        else:\n",
    "            consecutive_pass_count = 0\n",
    "            \n",
    "        # Performance criteria - if good results for several episodes => training performance satisfied and terminate early\n",
    "        if consecutive_pass_count >= CONSECUTIVE_PASS_LIMIT:\n",
    "            save_model(Q_net, optimizer, save_path)\n",
    "            best_model_episode = episode\n",
    "            performance_crit = True \n",
    "            msg = f\"Early termination at episode {best_model_episode:5d}, desired performance reached\"\n",
    "\n",
    "\n",
    "        episode += 1\n",
    "\n",
    "        # Checking for early training termination or truncation\n",
    "        train_terminated = (episode >= EPISODE_TRAIN) or (performance_crit)\n",
    "    \n",
    "    train_time = time.time()-train_time_start\n",
    "    print(f\"\\nRuntime - {train_time:5.2f}\")\n",
    "    print(msg)\n",
    "    env.close()\n",
    "\n",
    "    ## Post training stuff\n",
    "    # Load the best model (if one exists)\n",
    "    model_path = os.path.join(save_path,'q_network_checkpoint.pth')\n",
    "    if os.path.exists(model_path):\n",
    "        load_model(Q_net,model_path)\n",
    "    \n",
    "\n",
    "    # Average test reward of the resulting policy\n",
    "    env_test = gym.make('CartPole-v1')\n",
    "    print('Testing the Q_net')\n",
    "    average_reward = policy_eval(env_test, Q_net, n_episode_test=500,verbose = True)\n",
    "    print(f\"\\nValidation average reward {average_reward:4.2f}\")\n",
    "\n",
    "    # Store the validation hisotry and average test reward in the param_config JSON file\n",
    "    param_path = os.path.join(save_path,'param_config.json')\n",
    "    if os.path.exists(param_path):\n",
    "        with open(param_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        data = {}\n",
    "    data['runtime'] = train_time\n",
    "    data['valtime'] = val_time\n",
    "    data['best_model_at'] = best_model_episode\n",
    "    data['val_history'] = val_history\n",
    "    data['test_result'] = average_reward\n",
    "\n",
    "    with open(param_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "    # Plot\n",
    "    plot_reward_hist(reward_history, hyperparam_config, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01666e43",
   "metadata": {},
   "source": [
    "## Data Analysis of the Run/Train Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf96de86",
   "metadata": {},
   "source": [
    "Miscellaneous code to gather the training results from all OOP, non-CUDA runs to calculate the average runtime and training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88aac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_times(experiment: str, \n",
    "              results_dir):\n",
    "    ''' Function to extract the run/val/train times of a computation time experiment \n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    \n",
    "    experiment : str\n",
    "        the indicator of the experiment and can be one of [\"nOOP_nCUDA\", \"nOOP_CUDA\", \"OOP_nCUDA\", \"OOP_CUDA\"]\n",
    "\n",
    "    results_dir\n",
    "        path to the directory that contains all the run directories (run_00001, run_00002) and the trial_to_param.json file\n",
    "    \n",
    "    Return:\n",
    "    ----------\n",
    "    result \n",
    "        A numpy array of size (3 x number of run) containing the run time (1st row), validation time (2nd row), and train time (3rd row)\n",
    "    '''\n",
    "\n",
    "    if experiment not in [\"nOOP_nCUDA\", \"nOOP_CUDA\", \"OOP_nCUDA\", \"OOP_CUDA\"]:\n",
    "        print(\"Please recheck the experiment name\")\n",
    "        print(\"experiment can only be one of can be one of [\\\"nOOP_nCUDA\\\", \\\"nOOP_CUDA\\\", \\\"OOP_nCUDA\\\", \\\"OOP_CUDA\\\"]\")\n",
    "        return\n",
    "\n",
    "    # Load the data from trial_to_param.json file\n",
    "    with open(os.path.join(results_dir,'trial_to_param.json'),'r') as f:\n",
    "        run_data = json.load(f)\n",
    "\n",
    "    # Filter the runs that has the string 'OOP_nCUDA'\n",
    "    oop_ncuda_runs = [run for run, desc in run_data.items() if f\"_{experiment}\" in desc]\n",
    "\n",
    "    # Extract the data from each run\n",
    "    oop_ncuda_run_times = np.zeros(len(oop_ncuda_runs))\n",
    "    oop_ncuda_val_times = np.zeros(len(oop_ncuda_runs))\n",
    "    oop_ncuda_train_times = np.zeros(len(oop_ncuda_runs))\n",
    "\n",
    "    for idx, run in enumerate(oop_ncuda_runs):\n",
    "        param_path = os.path.join(results_dir, run, 'param_config.json')\n",
    "\n",
    "        try:\n",
    "            with open(param_path, 'r') as f:\n",
    "                config = json.load(f)\n",
    "            \n",
    "            runtime = config.get('runtime'); valtime = config.get('valtime')\n",
    "            oop_ncuda_run_times[idx] = runtime\n",
    "            oop_ncuda_val_times[idx] = valtime\n",
    "            if runtime and valtime:\n",
    "                oop_ncuda_train_times[idx] = runtime - valtime\n",
    "            else:   # In case of missing runtime and valtime\n",
    "                oop_ncuda_train_times[idx] = runtime\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"{run}: param_connfig.json not found\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"{run}: Error reading param_config.json\")\n",
    "        \n",
    "    result = np.vstack((oop_ncuda_run_times,oop_ncuda_val_times,oop_ncuda_train_times))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e107c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results from OOP_nCUDA runs\n",
    "RESULTS_DIR = os.path.join(os.getcwd(),'comp_time_results')\n",
    "OOP_nCUDA_times = get_times('OOP_nCUDA', RESULTS_DIR)\n",
    "OOP_nCUDA_run_times, OOP_nCUDA_val_times, OOP_nCUDA_train_times = OOP_nCUDA_times\n",
    "\n",
    "OOP_nCUDA_run_times_no_nan = OOP_nCUDA_run_times[~np.isnan(OOP_nCUDA_run_times)]\n",
    "OOP_nCUDA_train_times_no_nan = OOP_nCUDA_train_times[~np.isnan(OOP_nCUDA_train_times)]\n",
    "\n",
    "# Remove outlier from one extra long run ()\n",
    "OOP_nCUDA_run_times_cleaned = np.delete(OOP_nCUDA_run_times_no_nan, 15)\n",
    "OOP_nCUDA_train_times_cleaned = np.delete(OOP_nCUDA_train_times_no_nan, 15)\n",
    "\n",
    "print(f\"The average runtime of OOP + nCUDA implementation: {OOP_nCUDA_run_times_cleaned.mean()}\")\n",
    "print(f\"The average training time of OOP + nCUDA implementation: {OOP_nCUDA_train_times_cleaned.mean()}\")\n",
    "\n",
    "\n",
    "## Results from nOOP_nCUDA runs\n",
    "RESULTS_DIR = os.path.join(os.getcwd(),'comp_time_results')\n",
    "nOOP_nCUDA_times = get_times('nOOP_nCUDA', RESULTS_DIR)\n",
    "nOOP_nCUDA_run_times, nOOP_nCUDA_val_times, nOOP_nCUDA_train_times = nOOP_nCUDA_times\n",
    "\n",
    "nOOP_nCUDA_run_times_no_nan = nOOP_nCUDA_run_times[~np.isnan(nOOP_nCUDA_run_times)]\n",
    "nOOP_nCUDA_train_times_no_nan = nOOP_nCUDA_train_times[~np.isnan(nOOP_nCUDA_train_times)]\n",
    "\n",
    "# Remove outlier from one extra long run ()\n",
    "nOOP_nCUDA_run_times_cleaned = np.delete(nOOP_nCUDA_run_times_no_nan, 15)\n",
    "nOOP_nCUDA_train_times_cleaned = np.delete(nOOP_nCUDA_train_times_no_nan, 15)\n",
    "\n",
    "print(f\"The average runtime of nOOP + nCUDA implementation: {nOOP_nCUDA_run_times_cleaned.mean()}\")\n",
    "print(f\"The average training time of nOOP + nCUDA implementation: {nOOP_nCUDA_train_times_cleaned.mean()}\")\n",
    "\n",
    "plt.plot(OOP_nCUDA_train_times_cleaned)\n",
    "plt.plot(nOOP_nCUDA_train_times_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74ce574",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4,1, figsize=(20,32))\n",
    "axes = axes.flatten()\n",
    "\n",
    "axes[0].plot(OOP_nCUDA_run_times_cleaned, label = 'run_times')\n",
    "axes[0].plot(OOP_nCUDA_train_times_cleaned, label = 'train_times')\n",
    "axes[0].set_title(f'Runtime and training time of OOP_nCUDA runs')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(nOOP_nCUDA_run_times_cleaned, label = 'run_times')\n",
    "axes[1].plot(nOOP_nCUDA_train_times_cleaned, label = 'train_times')\n",
    "axes[1].set_title(f'Runtime and training time of nOOP_nCUDA runs')\n",
    "axes[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b30528",
   "metadata": {},
   "source": [
    "## Load an Simulate Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7a1672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(q_network: nn.Module, model_path):\n",
    "    checkpoint = torch.load(model_path)\n",
    "    q_network.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207111cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy_policy(env, obs, epsilon: float, q_network: QNet_MLP):\n",
    "    ''' Function to take an action according to an epsilon-greedy policy and a Q-network'''\n",
    "    if np.random.random() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
    "            q_values = q_network(state_tensor)\n",
    "            action = q_values.argmax().item()\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06effa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model after training\n",
    "\n",
    "# Manually select a folder/run to load\n",
    "run_number = 'run_00008'\n",
    "\n",
    "\n",
    "# Find the paths to the param_config and model checkpoint\n",
    "RESULT_DIR = os.path.dirname(experiment.save_path)\n",
    "RUN_DIR = os.path.join(RESULT_DIR, run_number)\n",
    "MODEL_PATH = os.path.join(RUN_DIR,'q_network_checkpoint.pth')\n",
    "\n",
    "# Find the model configuration\n",
    "with open(os.path.join(RUN_DIR,\"param_config.json\"),'r') as f:\n",
    "    data = json.load(f)\n",
    "model_id = 'DQN_MLP_' + data['parameters']['model_id']\n",
    "model_config = model_registry[model_id]['config']\n",
    "\n",
    "# Create the model according to the model version in the model registry\n",
    "q_network_loaded = QNet_MLP(experiment.obs_space, experiment.act_space, model_config)\n",
    "load_model(q_network_loaded, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0f9e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual simulation of the network\n",
    "env_test_visual = gym.make(\"CartPole-v1\",render_mode=\"human\")\n",
    "num_test = 5\n",
    "\n",
    "for episode in range(num_test):\n",
    "    obs, _ = env_test_visual.reset()\n",
    "    done = False\n",
    "    eps_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = eps_greedy_policy(env_test_visual, obs, epsilon = 0, q_network=q_network_loaded)\n",
    "        next_obs, reward, term, trunc, _ = env_test_visual.step(action)\n",
    "\n",
    "        done = term or trunc\n",
    "        eps_reward += reward\n",
    "        obs = next_obs\n",
    "    \n",
    "    print(f\"Reward from episode {episode:3d} is {eps_reward}\")"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "93adb7c4",
   "metadata": {},
   "source": [
    "# REINFORCE Policy Gradient\n",
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d5c2e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym             # To create the inverted pendulum environment\n",
    "import torch                        \n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import os\n",
    "import json                         # to save parameters and results\n",
    "import time                         # to monitor training & validation time\n",
    "from datetime import datetime       # to create a log of runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500b7048",
   "metadata": {},
   "source": [
    "## Parameter & Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fdcf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter definitions\n",
    "MODEL_NAME = 'Policy_MLP_v0'\n",
    "LR = 5e-4\n",
    "GAMMA = 0.98                    # discount factor for return calculation\n",
    "EPS_DECAY = 5000                # epsilon decay rate\n",
    "N_EPISODE_TRAIN = 5000            # number of episodes to train the Q-network\n",
    "N_EPISODE_TEST = 5\n",
    "CUDA_ENABLED = False\n",
    "SUCCESS_CRITERIA = 450\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e05432da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_layers=[64,64]):\n",
    "        ''' Initialize the model and create a list of layers '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        for layer_size in hidden_layers:\n",
    "            self.layers.append(nn.Linear(input_dim, layer_size))\n",
    "            input_dim = layer_size\n",
    "\n",
    "        self.layers.append(nn.Linear(input_dim, self.output_dim))\n",
    "        if CUDA_ENABLED: self.cuda()\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers[:-1]:\n",
    "            input = torch.relu(layer(input))\n",
    "        logits = self.layers[-1](input)\n",
    "        probs = torch.softmax(logits,dim=-1)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f35731fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_registry = {\n",
    "    \"Policy_MLP_v0\": {\n",
    "        'class': PolicyNet,\n",
    "        'config': [64,32]\n",
    "    },\n",
    "    'Policy_MLP_v1': {\n",
    "        'class': PolicyNet,\n",
    "        'config': [32,16]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f037b7b",
   "metadata": {},
   "source": [
    "## REINFORCE Training without Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73a2775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE():\n",
    "    def __init__(self, model_name: str,\n",
    "                 model_registry,\n",
    "                 lr: float,\n",
    "                 gamma: float,\n",
    "                 n_episode_train = N_EPISODE_TRAIN,\n",
    "                 result_folder = 'inv_pend_REINFORCE_results',\n",
    "                 seed = 42,\n",
    "                 cuda_enabled = False,\n",
    "                 verbose = True):\n",
    "        \n",
    "        self.N_EPISODE_TRAIN = n_episode_train\n",
    "        self.result_folder = result_folder\n",
    "        self.SEED = seed\n",
    "        self.CUDA_ENABLED = cuda_enabled\n",
    "        self.VERBOSE = verbose\n",
    "        self.LOG_PERIOD = 50\n",
    "        \n",
    "        # Experiment hyperparameters\n",
    "        self.model_name = model_name\n",
    "        self.model_class = model_registry[self.model_name]['class']\n",
    "        self.model_config = model_registry[self.model_name]['config']\n",
    "        match = re.search(r'v\\d+', self.model_name)\n",
    "        self.model_id = match.group(0) if match else 404\n",
    "\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.episode_train = 5000\n",
    "\n",
    "        # Initialize the train and validation environments\n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "        self.env_val = gym.make(\"CartPole-v1\")\n",
    "        self.obs_space = self.env.observation_space.shape[0]\n",
    "        self.act_space = self.env.action_space.n\n",
    "\n",
    "        # Initialize the policy network\n",
    "        self.policy_net = PolicyNet(self.obs_space, self.act_space, self.model_config)\n",
    "        self.policy_net.apply(self.init_weights)\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr = self.lr)\n",
    "\n",
    "        self.save_path = ''\n",
    "        self.model_path = ''\n",
    "        self.hyperparam_config = ''\n",
    "        self.reward_history = []\n",
    "        self.val_history = {}                                           # Monitor which episode had a validation run, the train reward, and the validation (test) reward \n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def create_directory(self):\n",
    "        ''' Function that creates directory to save model state_dict, architecture, training configuration, and history\n",
    "\n",
    "        Parameters: \n",
    "        ------------\n",
    "        (hyperparameters for differentiating between different directory)\n",
    "        \n",
    "        lr : float\n",
    "            the learning rate to optimize the Q network\n",
    "        gamma : float \n",
    "            the discount rate in Q learning\n",
    "        \n",
    "        Returns\n",
    "        ------------\n",
    "        name_codified : str\n",
    "            the shortened name for the current experiment \n",
    "        hyperparameters_codified : str\n",
    "            the shortened string of hyperparameter configuration\n",
    "        OUTPUT_DIR : path\n",
    "            the directory to which the training results and model (state_dict and architecture) will be saved\n",
    "        '''\n",
    "        timestamp = datetime.now().strftime(\"%y%m%d_%H%M\")\n",
    "\n",
    "        RESULT_FOLDER = self.result_folder\n",
    "        BASE_DIR = os.getcwd()\n",
    "        RESULT_DIR = os.path.join(BASE_DIR, RESULT_FOLDER)\n",
    "        os.makedirs(RESULT_DIR, exist_ok=True)      # Create the directory if one does not already exist\n",
    "\n",
    "        # Find the trial # of the latest run\n",
    "        existing_runs = [d for d in os.listdir(RESULT_DIR) if os.path.isdir(os.path.join(RESULT_DIR,d))]\n",
    "        run_numbers = [int(re.search(r'run_(\\d{5})',d).group(1)) for d in existing_runs if re.match(r'run_\\d{5}',d)]\n",
    "        trial_number = max(run_numbers,default=-1)+1\n",
    "\n",
    "        # Create a folder for the run\n",
    "        name_codified = f\"run_{trial_number:05d}\"\n",
    "        OUTPUT_DIR = os.path.join(RESULT_DIR,name_codified)\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)      # Create the directory\n",
    "\n",
    "        # Append the mapping from run # to hyperparameter configuration in a JSON file inside RESULT_DIR\n",
    "        trial_to_param_path = os.path.join(RESULT_DIR,'trial_to_param.json')\n",
    "        if os.path.exists(trial_to_param_path):\n",
    "            with open(trial_to_param_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "        else:\n",
    "            data = {name_codified: []}\n",
    "\n",
    "        if self.CUDA_ENABLED:\n",
    "            hyperparam_codified = f\"REINFORCE_OOP_CUDA_{self.model_id}_{self.lr}_{self.gamma}\"\n",
    "            hyperparam_codified_time = f\"{timestamp}_REINFORCE_OOP_CUDA_{self.model_id}_{self.lr}__{self.gamma}\"\n",
    "        else:   \n",
    "            hyperparam_codified = f\"REINFORCE_OOP_nCUDA_{self.model_id}_{self.lr}_{self.gamma}\"\n",
    "            hyperparam_codified_time = f\"{timestamp}_REINFORCE_OOP_nCUDA_{self.model_id}_{self.lr}__{self.gamma}\"\n",
    "        data[name_codified] = hyperparam_codified_time\n",
    "\n",
    "        with open(trial_to_param_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "        # Store the training configs in JSON file\n",
    "        training_params = {\n",
    "            'OOP':                  True,\n",
    "            'CUDA':                 self.CUDA_ENABLED,\n",
    "            'device':               torch.cuda.get_device_name(torch.cuda.current_device()),\n",
    "            'model_id':             self.model_id,\n",
    "            'lr':                   self.lr,\n",
    "            'gamma':                self.gamma,\n",
    "        }\n",
    "\n",
    "        # Store training parameters in each run \n",
    "        param_path = os.path.join(OUTPUT_DIR, \"param_config.json\")\n",
    "        with open(param_path, \"w\") as f:\n",
    "            json.dump({\"parameters\": training_params}, f, indent=2)\n",
    "\n",
    "        return name_codified, hyperparam_codified, OUTPUT_DIR\n",
    "    \n",
    "    def get_returns(self, eps_reward_history):\n",
    "        ''' Function to calculate the return of each time step when given a list of rewards \n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        rewards : list\n",
    "            a list of rewards achieved throughout the agent's trajectory\n",
    "        gamma   : float\n",
    "            the discount factor\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        returns : list\n",
    "            a list of returns G_t at each step of the trajectory\n",
    "            \n",
    "        For each step of th trajectory (of length T):\n",
    "        - Extract the rewards from that step onward\n",
    "        - Each step is multiplied by the corresponding gamma ^ index \n",
    "            the first reward received from leaving the state is not discounted\n",
    "            the last reward received from the trajectory is discouned by gamma ^ (T-1)\n",
    "        - Sum these values together to obtain the return at each step\n",
    "        '''\n",
    "        returns = np.zeros(len(eps_reward_history))\n",
    "        gamma = self.gamma\n",
    "        \n",
    "        for step, _ in enumerate(eps_reward_history):          # step through the \"trajectory\" or history of reward\n",
    "            step_reward = eps_reward_history[step:]            # reward from the current step onward\n",
    "\n",
    "            # List of discounted rewards at each time step\n",
    "            return_val = [gamma ** i * step_reward[i] for i in range(len(step_reward))]\n",
    "            return_val = sum(return_val)\n",
    "            \n",
    "            returns[step] = return_val\n",
    "        return returns\n",
    "\n",
    "    def policy_eval(self, env: gym.Env, n_episode_test = 500, verbose = True):\n",
    "        ''' Assess the average reward when following the policy net in a test environment with random state initialization\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        env : gymnasium environment\n",
    "            this environment can be either the self.env_test or self.env_val environment (whether they are the same)\n",
    "        n_episode_test : int \n",
    "            the number of evaluation episodes\n",
    "        verbose : bool\n",
    "            whether to print testing information \n",
    "\n",
    "        Return:\n",
    "        ----------\n",
    "        average_reward : float\n",
    "            the average reward received from running the test\n",
    "        '''\n",
    "\n",
    "        total_reward = 0\n",
    "        with torch.no_grad():\n",
    "            for i in range(n_episode_test):\n",
    "                obs,_ = env.reset()\n",
    "                done = False\n",
    "                eps_reward = 0\n",
    "\n",
    "                while not done:                 # Step thorugh the episode deterministically (no exploration)\n",
    "                    obs_tensor = torch.tensor(obs, dtype = torch.float32, device = 'cuda' if self.CUDA_ENABLED else 'cpu')\n",
    "                    action_probs = self.policy_net(obs_tensor)\n",
    "                    action_dist = Categorical(action_probs)\n",
    "                    action = action_dist.sample()\n",
    "                    next_obs, reward, term, trunc, _ = env.step(action.item())\n",
    "\n",
    "                    # Strategy 1 - Accumulate the reward from the environment\n",
    "                    eps_reward += reward\n",
    "\n",
    "                    # TODO - Strategy 2 - evaluate the strategy based on states\n",
    "\n",
    "                    obs = next_obs\n",
    "                    done = term or trunc\n",
    "            \n",
    "                total_reward += eps_reward\n",
    "                if verbose:\n",
    "                    print(f\"Validation episode {i+1:3d}/{n_episode_test}  |   Reward = {eps_reward:4.0f}\",end=\"\\r\")\n",
    "        average_reward = total_reward / n_episode_test\n",
    "        \n",
    "        return average_reward\n",
    "    \n",
    "    def EMA_filter(self, reward: list, alpha):\n",
    "        ''' Function that runs an exponential moving average filter along a datastream '''\n",
    "        output = np.zeros(len(reward)+1)\n",
    "        output[0] = reward[0]\n",
    "        for idx, item in enumerate(reward):\n",
    "            output[idx+1] = (1 - alpha) * output[idx] + alpha * item\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def plot_reward_hist(self, alpha = 0.1):\n",
    "        ''' Function that plots the reward and filtered reward per episode, then saves the plot in a specified save directory'''\n",
    "        n_episodes= len(self.reward_history)\n",
    "        episodes = range(n_episodes)\n",
    "        filtered_reward_hist = self.EMA_filter(self.reward_history, alpha)\n",
    "\n",
    "        plt.figure(figsize=(20,6))\n",
    "        plt.plot(episodes, self.reward_history[:n_episodes], color = \"blue\")\n",
    "        plt.plot(episodes, filtered_reward_hist[:n_episodes], color = \"red\")\n",
    "        plt.title(f'Total reward per episode - {self.hyperparam_config}')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.legend(['Total reward','Filtered reward'])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if self.save_path:\n",
    "            plt.savefig(os.path.join(self.save_path,'reward_history.png'))\n",
    "        plt.show()\n",
    "\n",
    "    def save_model(self):\n",
    "        ''' Function to save the model and optimizer state_dict for inference or continued training '''\n",
    "        self.model_path = os.path.join(self.save_path, 'q_network_checkpoint.pth')\n",
    "        torch.save({\n",
    "            'model_state_dict': self.policy_net.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, self.model_path)\n",
    "\n",
    "    def load_model(self):\n",
    "        ''' This code overwrite the Q_net with the parameters store in the instance's save_path '''\n",
    "        checkpoint = torch.load(self.model_path)\n",
    "        self.policy_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    def train_policy(self):\n",
    "        msg = \"Training ended with no good model found :<\"\n",
    "\n",
    "        # Create the directory to store results\n",
    "        self.run_number, self.hyperparam_config, self.save_path = self.create_directory()\n",
    "\n",
    "        # Training information\n",
    "        if self.VERBOSE:\n",
    "            print(f\"model {self.model_name}, lr={self.lr}, gamma={self.gamma}\")\n",
    "\n",
    "        self.reward_history = []                # Track the total reward per episode\n",
    "        self.val_history = {}                   # Reset the validation history\n",
    "        self.loss_history = []                  # History of loss throughout training\n",
    "        self.value_history = []                 # History of the value at state s_0 throughout training\n",
    "        self.val_time = 0                       # Time used for validation (s)\n",
    "        episode = 0\n",
    "\n",
    "        # Control of early stopping\n",
    "        consecutive_pass_count = 0\n",
    "        CONSECUTIVE_PASS_LIMIT = 3\n",
    "        EPISODE_REWARD_LIMIT = SUCCESS_CRITERIA\n",
    "        best_reward = 0\n",
    "        self.best_model_episode = None\n",
    "        performance_crit = False                # Whether desired performance is met consistently\n",
    "        train_terminated = False\n",
    "        \n",
    "\n",
    "        self.train_time_start = time.time()\n",
    "        while not train_terminated:             # Experiment level - loop through episodes\n",
    "            obs, _ = self.env.reset(seed=self.SEED)\n",
    "            done = False\n",
    "            eps_reward_history = []\n",
    "            log_prob_history = []\n",
    "\n",
    "            while not done:                     # Episode level - loop through steps in an episode\n",
    "                obs_tensor = torch.tensor(obs, dtype = torch.float32, device = 'cuda' if self.CUDA_ENABLED else 'cpu')\n",
    "                action_probs = self.policy_net(obs_tensor)\n",
    "                action_dist = Categorical(action_probs)\n",
    "                action = action_dist.sample()\n",
    "\n",
    "                next_obs, reward, term, trunc, _ = self.env.step(action.item())\n",
    "\n",
    "                eps_reward_history.append(reward)\n",
    "                log_prob_history.append(action_dist.log_prob(action))\n",
    "                \n",
    "                obs = next_obs\n",
    "                done = term or trunc\n",
    "            \n",
    "            # Post episode calculations - returns, total episode reward, and loss\n",
    "            returns = self.get_returns(eps_reward_history)\n",
    "            returns = torch.tensor(returns, dtype = torch.float32, device='cuda' if self.CUDA_ENABLED else 'cpu')\n",
    "\n",
    "            eps_reward = sum(eps_reward_history)\n",
    "            loss = - torch.sum(torch.stack(log_prob_history) * returns)\n",
    "\n",
    "            self.reward_history.append(eps_reward)     # Total reward of episode\n",
    "            self.loss_history.append(loss)\n",
    "            self.value_history.append(returns[0])\n",
    "\n",
    "            # Optimize the network parameters\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Periodic data logger\n",
    "            if episode % self.LOG_PERIOD == 0 and self.VERBOSE:\n",
    "                print(f\"Episode {episode:5d}/{self.episode_train}: Total reward = {eps_reward:5.1f}   |   Value at s_0 = {returns[0]: 5.2f}\", end='\\r')\n",
    "\n",
    "            # Early stopping condition\n",
    "            if eps_reward >= EPISODE_REWARD_LIMIT:\n",
    "                # Evaluate the current good policy and record the validation time\n",
    "                self.val_time_start = time.time()\n",
    "                test_reward = self.policy_eval(self.env_val, 20,verbose=False)\n",
    "                self.val_time += time.time() - self.val_time_start\n",
    "\n",
    "                self.val_history[episode] = [eps_reward, test_reward]\n",
    "\n",
    "                if test_reward >= best_reward:           # Set the new best reward\n",
    "                    best_reward = test_reward\n",
    "                    self.save_model()\n",
    "                    self.best_model_episode = episode\n",
    "                    msg = f\"Training terminated due to episode limit, best model saved at episode {self.best_model_episode:5d}\"\n",
    "                if test_reward > EPISODE_REWARD_LIMIT: \n",
    "                    consecutive_pass_count += 1\n",
    "                else: consecutive_pass_count = 0\n",
    "            else:\n",
    "                consecutive_pass_count = 0\n",
    "\n",
    "            # Performance criteria - if good results for several episodes => training performance satisfied and terminate early\n",
    "            if consecutive_pass_count >= CONSECUTIVE_PASS_LIMIT:\n",
    "                self.save_model()\n",
    "                self.best_model_episode = episode\n",
    "                performance_crit = True \n",
    "                msg = f\"Early termination at episode {self.best_model_episode:5d}, desired performance reached\"\n",
    "\n",
    "\n",
    "            episode += 1\n",
    "\n",
    "            # Checking for early training termination or truncation\n",
    "            train_terminated = (episode >= self.N_EPISODE_TRAIN) or (performance_crit)\n",
    "\n",
    "        self.train_time = time.time() - self.train_time_start\n",
    "\n",
    "        print(f\"\\nTotal runtime - {self.train_time:5.2f}\")\n",
    "        print(msg)\n",
    "        return\n",
    "\n",
    "    def record(self):\n",
    "        # Load the best policy net parameter from the experiment\n",
    "        if self.model_path:\n",
    "            self.load_model()\n",
    "\n",
    "        # Average test reward of the resulting policy - if best policy does not exists use the last one\n",
    "        self.env_test = gym.make('CartPole-v1')\n",
    "        print('Testing the best policy network performance')\n",
    "        average_reward = self.policy_eval(self.env_test, n_episode_test=500, verbose=True)\n",
    "        print(f\"\\nValidation average reward {average_reward:4.2f}\")\n",
    "\n",
    "        # Store the validation history and average test reward in the param_config JSON file\n",
    "        param_path = os.path.join(self.save_path, 'param_config.json')\n",
    "        if os.path.exists(param_path):\n",
    "            with open(param_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "        else:\n",
    "            data = {}\n",
    "\n",
    "        data['runtime'] = self.train_time\n",
    "        data['valtime'] = self.val_time\n",
    "        data['best_model_at'] = self.best_model_episode\n",
    "        data['val_history'] = self.val_history\n",
    "        data['test_result'] = average_reward\n",
    "\n",
    "        with open(param_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "        # Plot\n",
    "        self.plot_reward_hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec6642c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model Policy_MLP_v0, lr=0.001, gamma=0.98\n",
      "Episode   350/5000: Total reward = 500.0   |   Value at s_0 =  50.00\r"
     ]
    }
   ],
   "source": [
    "REINFORCE_experiment = REINFORCE(MODEL_NAME, model_registry,\n",
    "                                 LR, GAMMA,\n",
    "                                 seed=42,\n",
    "                                 cuda_enabled=CUDA_ENABLED,\n",
    "                                 verbose=True)\n",
    "REINFORCE_experiment.train_policy()\n",
    "REINFORCE_experiment.record()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4eba9db",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m     action_dist = Categorical(action_probs)\n\u001b[32m     12\u001b[39m     action = action_dist.sample()\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m next_obs, reward, term, trunc, _ = \u001b[43menv_test_visual\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m obs = next_obs\n\u001b[32m     16\u001b[39m done = term \u001b[38;5;129;01mor\u001b[39;00m trunc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caomi\\anaconda3\\envs\\torch\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:125\u001b[39m, in \u001b[36mTimeLimit.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[32m    114\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[32m    116\u001b[39m \n\u001b[32m    117\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m \n\u001b[32m    124\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28mself\u001b[39m._elapsed_steps += \u001b[32m1\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._elapsed_steps >= \u001b[38;5;28mself\u001b[39m._max_episode_steps:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caomi\\anaconda3\\envs\\torch\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caomi\\anaconda3\\envs\\torch\\Lib\\site-packages\\gymnasium\\core.py:327\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    325\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caomi\\anaconda3\\envs\\torch\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:285\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m.env, action)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caomi\\anaconda3\\envs\\torch\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:223\u001b[39m, in \u001b[36mCartPoleEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    220\u001b[39m     reward = -\u001b[32m1.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sutton_barto_reward \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode == \u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001b[39;00m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.array(\u001b[38;5;28mself\u001b[39m.state, dtype=np.float32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caomi\\anaconda3\\envs\\torch\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:337\u001b[39m, in \u001b[36mCartPoleEnv.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode == \u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    336\u001b[39m     pygame.event.pump()\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrender_fps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m     pygame.display.flip()\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode == \u001b[33m\"\u001b[39m\u001b[33mrgb_array\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "env_test_visual = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "\n",
    "for episode in range(N_EPISODE_TEST):\n",
    "    obs, _ = env_test_visual.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        obs_tensor = torch.tensor(obs, dtype = torch.float32, device = 'cuda' if CUDA_ENABLED else 'cpu')\n",
    "        with torch.no_grad():       # Inference only\n",
    "            action_probs = REINFORCE_experiment.policy_net(obs_tensor)\n",
    "            action_dist = Categorical(action_probs)\n",
    "            action = action_dist.sample()\n",
    "        next_obs, reward, term, trunc, _ = env_test_visual.step(action.item())\n",
    "\n",
    "        obs = next_obs\n",
    "        done = term or trunc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
